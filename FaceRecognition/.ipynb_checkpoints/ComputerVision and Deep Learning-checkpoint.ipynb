{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource for this note:\n",
    "\n",
    "[CS231n Course](http://cs231n.github.io/classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. About Image\n",
    "\n",
    "### Computer image\n",
    "An image is represented as one large 3-D array of number ranges from 0 (black) to 255 (white). The three color channels are Red, Green, and Blue (RGB for short)\n",
    "\n",
    "### Challenge of image classification:\n",
    "\n",
    "- **Viewpoint variation:** A single instance of an object can be oriented in many ways with respect to the camera.\n",
    "- **Scale variation:** Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image).\n",
    "- **Deformation:** Many objects of interest are not rigid bodies and can be deformed in extreme ways.\n",
    "- **Occlusion:** The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.\n",
    "- **Illumination conditions:** The effects of illumination are drastic on the pixel level.\n",
    "- **Background clutter:** The objects of interest may blend into their environment, making them hard to identify.\n",
    "- **Intra-class variation:** The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance.\n",
    "\n",
    "### Pipeline of image classification:\n",
    "\n",
    "- **Input:** A set of N images, each labeled with one of K different classes\n",
    "- **Learning:** \n",
    "- **Evaluation:** \n",
    "\n",
    "### Reshape image: \n",
    "To represent each image as one row, we need to flattern out all images into a one-dimensional data.\n",
    "for example a image (32 x 32 x 3) will become a row with 3072 numbers, each pixel is thought of as a feature\n",
    "\n",
    "### Image data preprocessing\n",
    "In machine learning, it is very common practice to always perform normalization of your input features (in the case of images, every pixel is thought of as a feature). In particular, it is important to **center your data** by subtracting the mean from every feature. Further common preprocessing is to **scale each input** feature so that its values range from [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image and Machine Learning \n",
    "\n",
    "### Loss function\n",
    "\n",
    "1. **[Multiclass Support Vector Machine (SVM) loss](http://cs231n.github.io/linear-classify/):** The SVM loss  is set up so that the SVM \"wants\" the correct class for each image to have a score higher than the incorrect classes by some fixed margin delta.\n",
    "\n",
    "    - max(0, -) function is oftern called hinge loss\n",
    "    - L2-SVM, use max(0, -)^2 called squared hinge loss\n",
    "    - cross-entropy loss\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "**Purpose:** For a loss function, there could be many sets of W that correctly classify every sample. We wish to encode some preference for a certain set of weights W over others to remove this ambiguity. \n",
    "\n",
    "** regularization penalty:** also called regularization loss - R(W)\n",
    "\n",
    "### Two common classifiers\n",
    "\n",
    "1. **[Binary support Vector Machine:](http://cs231n.github.io/linear-classify/)** use max-margin hinge loss\n",
    "\n",
    "2. **[Softmax:](http://cs231n.github.io/linear-classify/)** A generalization of binary Logistic Regression classifier to multiple classes, Its output is normalized class probabilities.  It takes a vector of arbitrary real-valued scores (in z) and squashes it to a vector of values between zero and one that sum to one, thus provides \"probabilities\" (regulated by regularization) for each class.\n",
    "\n",
    "### Gradient and Slope\n",
    "\n",
    "A gradient is a vector, and slope is a scalar. Gradients really become meaningful in multivarible functions, where the gradient is a vector of partial derivatives. With single variable functions, the gradient is a one dimensional vector with the slope as its single coordinate \n",
    "\n",
    "### [Derivative](http://cs231n.github.io/optimization-2/)\n",
    "\n",
    "They indicate the rate of change of a function with respect to that variable surrounding an infinitesimally small region near a particular point.\n",
    "\n",
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "A three-layer neural network: s = W3 max(0, W2 max(0, W1x)), hwere all the W3, W2, W1 are parameters to be learned. The parameters are learned with stochastic gradient descent, and their gradients are derived with chain rule (and computed with backpropagation). The size of the intermediate hidden vectors are hyperparameters of the network. \n",
    "\n",
    "Each neuron performs a dot product with the input and its weights, adds the bias and applies the non-linearity (or activation function).\n",
    "\n",
    "### Commonly used activation functions:\n",
    "\n",
    "- **Sigmoid:** $\\sigma(x) = 1 / (1 + e^{-x}) $  It takes a real-valued number and “squashes” it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1\n",
    "- **Tanh:** $\\tanh(x) = 2\\sigma(2x) - 1$ It squashes a real-valued number to the range [-1, 1]\n",
    "- **ReLU:** $f(x) = \\max(0, x)$ The activation is simply thresholded at zero\n",
    "    - It was found to greatly accelerate the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.\n",
    "    - Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.\n",
    "    - The ReLU units can irreversibly die during training since they can get knocked off the data manifold. \n",
    "- **Leaky ReLU:** $f(x) = 1(x < 0)(\\alpha x) + 1 (x >= 0)(x)$ where $\\alpha$ is a small constant (e.g. 0.01)\n",
    "- **Maxout:** $\\max(\\omega_1^Tx + b_1, \\omega_2^Tx + b_2)$ Combines all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). \n",
    "\n",
    "### What activation type should I use?\n",
    "\n",
    "Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.\n",
    "\n",
    "### Neural Network (an acyclic graph) architectures\n",
    "\n",
    "- Different layers: input layer, hidden layer, output layer. A single-layer neural network has no hidden layer (not counting the input layer)  \n",
    "- Fully-Connected layers: neurons in adjacent layers have full pair-wise connections, but neurons within a layer are not connected.\n",
    "- Sizing neural networks:\n",
    "    - by neurons: not counting the inputs\n",
    "    - by parameters: weights and biases\n",
    "    \n",
    "As an aside, in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain. \n",
    "\n",
    "### Setting number of layers and their sizes\n",
    "\n",
    "First, note that as we increase the size and number of layers in a Neural Network, the capacity of the network increases. That is, the space of representable functions grows since the neurons can collaborate to express many different functions. \n",
    "\n",
    "Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship.\n",
    "There are many other preferred ways to prevent overfitting in Neural Networks that we will discuss later (such as L2 regularization, dropout, input noise). In practice, it is always better to use these methods to control overfitting instead of the number of neurons. To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network. \n",
    "\n",
    "The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: It’s clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. \n",
    "\n",
    "In practice, what you find is that if you train a small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima. On the other hand, if you train a large network you’ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization.\n",
    "\n",
    "The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and the model\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "- **Mean subtraction:** The most common forms of preprocessing. X -= np.mean(X, axis=0) or X -= np.mean(X)\n",
    "- **Normalization:** One is to divide each dimension by its standard deviation, once it has been zero-centered (mean subtraction). so that the min and max along the dimension is -1 and 1 respectively. It only makes sense to apply this preprocessing if you have a reason to believe that different input features have different scales (or units), but they should be of approximately equal importance to the learning algorithm. In case of images, the relative scales of pixels are already approximately equal (and in range from 0 to 255), so it is not strictly necessary to perform this additional preprocessing step.\n",
    "- **PCA:** \n",
    "    - Data is first centered as described above. \n",
    "    - Then, the covariance matrix that tells us about the correlation structure in the data is computed.\n",
    "    - Compute the SVD factorization of the data covariance matrix\n",
    "    - Decorrelate the data, project the zero-centered data into the eigenbasis\n",
    "```python\n",
    "X -= np.mean(X, axis=0)\n",
    "cov = np.dot(X.T, X) / X.shpe[0]\n",
    "U,S,V = np.linalg.svd(cov)  #returned value U, the eigenvector columns are sorted by their eigenvalues.\n",
    "Xrot = np.dot(X, U)\n",
    "Xrot_reduced = np.dot(X, U[:, :100]) \n",
    "# After this operation, we would have reduced the original dataset of size [N x D] to one of size [N x 100], keeping the 100 dimensions of the data that contain the most variance.\n",
    "```\n",
    "- **Whitening:** The whitening operation takes the data in the eigenbasis and divides every dimension by the eigenvalue to normalize the scale.\n",
    "```python\n",
    "# whiten the data:\n",
    "# divide by the eigenvalues (which are square roots of the singular values)\n",
    "Xwhite = Xrot / np.sqrt(S + 1e-5)\n",
    "```\n",
    "\n",
    "**In practice**. We mention PCA/Whitening in these notes for completeness, but these transformations are not used with Convolutional Networks. However, it is very important to zero-center the data, and it is common to see normalization of every pixel as well.\n",
    "\n",
    "**Common pitfall**. An important point to make about the preprocessing is that any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation / test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "\n",
    "- **Small random numbers:** It is common to initialize the weights of the neurons to small numbers and refer to doing so as symmetry breaking. The implementation for one weight matrix might look like \n",
    "```python\n",
    "W = 0.01* np.random.randn(D,H)\n",
    "```\n",
    "\n",
    "- **Calibrating the variances with 1/sqrt(n):** One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that we can normalize the variance of each neuron’s output to 1 by scaling its weight vector by the square root of its fan-in (i.e. its number of inputs).\n",
    "```python\n",
    "w = np.random.randn(n) / sqrt(n)  ## where n is the number of its inputs\n",
    "```\n",
    "\n",
    "- **Sparse initialization:** Another way to address the uncalibrated variances problem is to set all weight matrices to zero, but to break symmetry every neuron is randomly connected (with weights sampled from a small gaussian as above) to a fixed number of neurons below it. A typical number of neurons to connect to may be as small as 10.\n",
    "\n",
    "- **Initializing the biases:** It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights. For ReLU non-linearities, some people like to use small constant value such as 0.01 for all biases because this ensures that all ReLU units fire in the beginning and therefore obtain and propagate some gradient. However, it is not clear if this provides a consistent improvement (in fact some results seem to indicate that this performs worse) and it is more common to simply use 0 bias initialization.\n",
    "\n",
    "- **[Batch Normalization](https://arxiv.org/abs/1502.03167):** In practice networks that use Batch Normalization are significantly more robust to bad initialization.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "There are several ways of controlling the capacity of Neural Networks to prevent overfitting\n",
    "\n",
    "- **L2 regularization:** Perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. That is, for every weight $\\omega$ in the network, we add the term $\\frac12 \\lambda \\omega^2$ to the objective, where $\\lambda$ is the regularization strength. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. \n",
    "\n",
    "- **L1 regularization:** Relatively common form of regularization, where for each weight ww we add the term $\\lambda |\\omega|$ to the objective. L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the \n",
    "\n",
    "- **Elastic net regularization:** It is possible to combine the L1 regularization with the L2 regularization: $\\lambda_1 |\\omega| + \\lambda_2 \\omega^2$.\n",
    "\n",
    "- **Max norm constraints:** It nforces an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. Restrict every neuron to satisfy $||\\vec{\\mathbf{\\omega}}||_2 < c$\n",
    "\n",
    "- **Dropout:** An extremely effective, simple and recently introduced regularization technique by Srivastava et al. While training, dropout is implemented by only keeping a neuron active with some probability $p$ (a hyperparameter), or setting it to zero otherwise.\n",
    "```python\n",
    "\n",
    "p = 0.5 # probability of keeping a unit active. higher = less dropout\n",
    "\n",
    "def train_step(X):\n",
    "    \"\"\" X contains the data \"\"\"\n",
    "  \n",
    "    # forward pass for example 3-layer neural network\n",
    "    H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "    U1 = np.random.rand(*H1.shape) < p # first dropout mask\n",
    "    H1 *= U1 # drop!\n",
    "    H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "    U2 = np.random.rand(*H2.shape) < p # second dropout mask\n",
    "    H2 *= U2 # drop!\n",
    "    out = np.dot(W3, H2) + b3\n",
    "```\n",
    "\n",
    "- **Bias regularization:**  It is not common to regularize the bias parameters because they do not interact with the data through multiplicative interactions, and therefore do not have the interpretation of controlling the influence of a data dimension on the final objective.\n",
    "\n",
    "**In practice:** It is most common to use a single, global L2 regularization strength that is cross-validated. It is also common to combine this with dropout applied after all layers. The value of p=0.5p=0.5 is a reasonable default, but this can be tuned on validation data.\n",
    "\n",
    "### Loss functions\n",
    "\n",
    "- **Classification:** \n",
    "    - One of two commonly seen cost functions are SVM: $L_i=\\sum \\max(0, f_j - f_{yi} + 1)$. Sometimes, squared hinge loss has better performance (using $\\max(0, f_j - f_{yi} + 1)^2$ ).\n",
    "    - The second common choice is the softmax classifier that uses the cross-entropy loss: $L_i = - \\log(\\frac{e^{f_{yi}}}{\\sum_j e^{f_j}})$. For large number of classes, (e.g. words in English dictionary, or ImageNet which contains 22,000 categories), it may be helpful to use [Hierarchical Softmax](https://arxiv.org/pdf/1310.4546.pdf).\n",
    "    - Attribute classification: oth losses above assume that there is a single correct answer $y_i$ . But what if $y_i $ is a binary vector where every example may or may not have a certain attribute, and where the attributes are not exclusive? For example, images on Instagram can be thought of as labeled with a certain subset of hashtags from a large set of all hashtags, and an image may contain multiple. $L_i \\sum_j \\max(0, 1 - y_{ij} f_j)$\n",
    "\n",
    "- **Regression:**\n",
    "    - L2 squared norm: $L_i = ||f - Y_i||_2^2$  \n",
    "    - Li norm: $L_i = ||f - y_i||_1 = \\sum_j |f_j - (y_i)_j|$  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning\n",
    "\n",
    "### Gradient checks\n",
    "\n",
    "- **Use the centered formula** $\\frac{df(x)}{dx} = \\frac{ f(x + h) - f(x - h)}{h}$ (use this) (don't use). $\\frac{df(x)}{dx} = \\frac{ f(x + h) - f(x - h)}{2h}$ (use this)  the first formula has an error on order of $O(h)$, while the second formula only has error terms on order of $O(h^2)$ \n",
    "\n",
    "- **Use relative error for comparison** When comparing the numerical gradient $f'_n$ and analytic gradient $f'_a$, don't use $|f'_a - f'_n|$, use $\\frac{|f'_a - f'_n|}{\\max(|f'_a|, |f'_n|)}$ instead. Also keep in mind that the deeper the network, the higher the relative errors will be\n",
    "\n",
    "- **Use double precision** A common pitfall is using single precision floating point to compute gradient check. It is often that case that you might get high relative errors (as high as 1e-2) even with a correct gradient implementation. In my experience I’ve sometimes seen my relative errors plummet from 1e-2 to 1e-8 by switching to double precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
