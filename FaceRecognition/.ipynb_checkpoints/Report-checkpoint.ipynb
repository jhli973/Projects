{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                                      Capstone Project\n",
    "##                        Machine Learning Engineer Nanodegree\n",
    "Jianhua Li, Mar 12st, 2017\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "As the built in cameras becomes more and more common in cell phones and personal computers, images and videos are piling up. The desire to understand what is in these images and thus to apply this information to facilitate our daily life becomes more and more strong. Computer vision is an interdisciplinary field and provides a powerful solution in high-level image as well as videos understanding. \n",
    "\n",
    "Face recognition, a form of computer vision, which uses the spatial geometry of distinguishing features of the face to identify or to authenticate a person. Software using webcam acquired images automatically log on an authorized user has been developed for many years by several companies. Computer access control system based on face recognition is also provided by three well-known computer manufacturers. Although it is convenient (hand-free), its security caused by the relatively accuracy is one of the major concerns compared with other biometric recognition systems. Nevertheless, it is still considered as a good supplement for current available solution.\n",
    "\n",
    "The conventional face recognition pipeline consists of four stages which are face detection, face alignment, feature extraction and classification. One of the most import steps is the feature extraction. Conventional features include linear functions of the raw pixel values including Eigenface, Fisherface and Laplacianface. More recently, such linear features were replaced by hand-crafted features. Although hand-crafted features and metric learning achieve promising performance in constrained environment, the performance using these features degrades dramatically in unconstrained environments where face images conver complex and large intra-personal variations such as pose, illumination, expression and occusion. \n",
    "\n",
    "In recent years, deep learning, in particular Convolutional Neural Network (CNN) has achieved very impressive results in face recognition. Unlike the conventional hand-crafted features, the CNN learning-based features are more robust to complex intra-personal variations. Indeed, the top three cases of face recognition in unconstrained environment (FRUE) reported on benchmark database LFW have been achieved by CNN. One of the advantages of CNN is that all processing layers have configurable parameters which can be learned from data, thus allievate the burden of manual feature design. However, the number of parameters in CNN can be millions if not billions. To learn such a large number of parameters, a very large training datasets are required. In many cases, collecting large datasets can be very costly or even an impossible task.\n",
    "\n",
    "To overcomes the limitd size of datasets, two methods have been successfully applied to diverse recognition problems. For example, A 3D-aided face synthesis technique has been used for facial landmark detection and face recognition. Another commonly used technique is Data augmentation which applys mirroring, cropping, rotation, and scaling without changing semantic-level image label. [here](https://arxiv.org/pdf/1603.06470.pdf)\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "For security reasons, we are required to lock our computer whenever we leave our desk. When we come back to our desk, we have to unlock it. There are multiple ways to do it, for example, one can type in the password through keyboard or use an external device to read identification information through smart ID card, but none of them are hand-free process. So it is kind of annoying to repeat these locking/unlocking steps manually. For this project, I propose to develop an auto-facial recognition system. Specifically, this system uses the built-in camera in the computer to capture the image of a person in front of it. The captured image then is processed and routed to the auto-facial recognition system where a prediction will be made based on the input image. If the image is from an authorized person, a \"password\" message will be send to the computer so that it can unlock itself automatically, Otherwise it will not do anything.\n",
    "\n",
    "In this project, a CNN, the most recent deep learning face recognition method, is applied. Our goal is to achieve more accurate face recognition capability by carefully adjust the CNN architectures.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "To identify if a image from an authorized person, a 1:N authentication method is applied. Images of authorized person to use the computer are prepared by myself. Images of unauthorized users are collected from online ([LFW](http://vis-www.cs.umass.edu/lfw/)). Images are classified into \"authorized\" and \"unauthorized\" two categories and labeled with authorized (1) and unauthorized (0). The captured image will then be processed with OpenCV. Decision will be made based on the supervised learning model.\n",
    "\n",
    "When trainning the model with CNN, the commonly used cross-entropy loss function is used.  \n",
    "\n",
    "$L_i=-\\log(\\frac{e^{f_yi}}{\\sum_j e^{f_i}})$\n",
    "\n",
    "Image data is devided into trainning, validation and testing three groups. To train the model, the keras python library is used with tensorflow as the backend. A supervised learning model is generated with the state of art machine learning algorithm convolutional neural networks (CNN). For the final classification, the softmax classifier function is used. To measure performance of the model, accuracy is used to evaluate the performance with the validation dataset and then test with testing data. The final model is further evaluate with 20 pieces of new images. \n",
    "\n",
    "$accuracy = \\frac{true \\ positives \\ + \\ true \\ negatives}{dataset \\ size}$\n",
    "\n",
    "\n",
    "## II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "To reduce the labor, I have taken the advantage of the online resource. Images used for \"unauthorized\" category are downloaded from the website of computer vision lab at the University of Massachusetts (http://vis-www.cs.umass.edu/lfw/). This image data set contains more than 13,000 images of faces collected from the web. Each face has been labeled with the name of the person pictured. The images are packed as a 173 MB gzipped tar file. The zipped file is unzipped with the 7-zip software downloaded from (http://www.7-zip.org/) and saved on a ../unauthorized folder. It has been known that there are some mismatches between the picture and the labeled name. This should not be a problem for the current project. \n",
    "\n",
    "Images used for \"authorized\" category are prepared by myself either using a cell phone or a built-in camera on the notebook. All image are taken at natural color with standard contrast. Images from these two sources have different size settings. Images manually taken are pre-trimmed with photoshop as 250 pixel x 250 pixel. Due to the time and labor limitation, total 333 images were prepared. To further enrich the \"authorized\" image pool, images were automatically derived from a set of self-recorded videos (480 pixel x 600 pixel) by using OpenCV. All videos are taken with the faces at approximate center position. Backgrounds are mostly from an indoor setting which mimicks the office environment. Images were picked every 2 to 10 frames depends on the variation of the background, illumination as well as the expression. Images were then cropped at [0:480, 60:540] and further resized as 250 x 250 x 3 format with INTER_AREA interpolation. The prepared images are saved on a ../authorized folder.\n",
    "\n",
    "For initial image data exploration, Ipython.display labrary and OpenCV (opencv.org) are used. Images are kept in good condition and can be shown with IPython.display library (see Figure 1 and ImageExplore.ipynb file). All images shape is 250 x 250 x 3 with the size 187500. There are 10,843 images belong to \"authorized\" category and 13,233 images belong to \"unauthorized\" category. \n",
    "\n",
    "### Exploratory Visualization\n",
    "\n",
    "Raw images from eith \"authorized\" or \"unauthorized\" categories are randomly picked for visualization. As we can see that images from both categories are propery displayed with normal color scheme (see Figure 1 and ImageExplore.ipynb file).\n",
    "\n",
    "Images after processing and resizing were further visualized using pyplot image. All images were shown properly with expected size. Images data saved as pickle files were further converted back as image and can be also displayed normaly with propriate color scheme (see Figure 1 and ImageRawdataProcessing.ipynb file). It is worth to note that some of the images downloaded from LFW have been shifted (see Figure 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='Auth_Raw.jpg'></td><td><img src='Auth_Procc.png'></td><td><img src='Auth_split.png'></td><td>Authorized Images<br/>Left: Raw Image<br/>Center:After Processing<br/>Right:Pickle file<br/></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='Unauth_Raw.jpg'></td><td><img src='Unauth_Procc.png'></td><td><img src='Unauth_split.png'></td><td>Unauthorized Images<br/>Left: Raw Image<br/>Center:After Processing<br/>Right:Pickle file<br/></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<table><tr><td><img src='Auth_Raw.jpg'></td><td><img src='Auth_Procc.png'></td><td><img src='Auth_split.png'></td><td>Authorized Images<br/>Left: Raw Image<br/>Center:After Processing<br/>Right:Pickle file<br/></td></tr></table>\"))\n",
    "display(HTML(\"<table><tr><td><img src='Unauth_Raw.jpg'></td><td><img src='Unauth_Procc.png'></td><td><img src='Unauth_split.png'></td><td>Unauthorized Images<br/>Left: Raw Image<br/>Center:After Processing<br/>Right:Pickle file<br/></td></tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1. Images Exploratory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "\n",
    "In this project, I have apply Convolutional Neural Networks (CNN) classifier which is a state-of-the-art algorithm for image classification, object detection and face recognition. Although CNN is particularly useful for training model from image data, the drawback associated with CNN is that it requires a large number of training data for learning a deep image representation. \n",
    "To overcome the inheriated problem, data augmentation and data synthesis are commonly used for face recognitation. For example,  using the synthesized large training datasets, [Hu]('LDFaceRepresentationsUsingSmallData.pdf') et. al., improve the face verification rate from 78.97% to 95.77% on LFW using CNNs. \n",
    "\n",
    "A CNN consists of several layers which can be classified into three groups:\n",
    "\n",
    "- Convolutional layer\n",
    "- Max-Pooling Layer\n",
    "- Fully-Connected layer\n",
    "\n",
    "The following parameters can be tuned to optimize the classifier: \n",
    "\n",
    "- Classification threshold\n",
    "- Training parameters \n",
    "    - Training length (number of epochs) \n",
    "    - Batch size (how many images to look at once during a single training step)\n",
    "    - Solver type (what algorithm to use for learning) \n",
    "    - Learning rate (how fast to learn; this can be dynamic) \n",
    "    - Weight decay (prevents the model being dominated by a few “neurons”) \n",
    "    - Momentum (takes the previous learning step into account when calculating the next            \n",
    "- Neural network architecture \n",
    "    - Number of layers \n",
    "    - Layer types \n",
    "        - convolutional\n",
    "        - fully-connected\n",
    "        - pooling\n",
    "    - Layer parameters \n",
    "- Preprocessing parameters\n",
    "    - image size \n",
    "    - dimention\n",
    "\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "Using the synthesized large training datasets, [Hu]('LDFaceRepresentationsUsingSmallData.pdf') et. al., has reported that the face verification rate to 95.77% on LFW using CNNs. \n",
    "\n",
    "## III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "To reduce the data set size, raw images were further resized into a smaller format (80 pixel x 80 pixel x 3) with OpenCV with INTER_AREA intepolation. Images data for both categories were read into seperate numpy arrays and saved as pickle files named as authorized_image_data.pickle and unauthorized_image_data.pickle, respectively. \n",
    "\n",
    "Image data then extracted from corresponding pickle files and corresponding labels were added for each image depends on which catgory it originates from. For example, a image from a \"authorized\" folder is labeled as 1 while a image from a \"unauthorized\" folder is labeled as 0. The image data is then converted into numpy arrays.  \n",
    "\n",
    "To train a surpvised model, data is randomly divided into training and testing groups with test size set as 0.25. For validation purpose, the training data set is further divided into training and validation groups with train size set as 0.7. All image data is presented as float32 data type and is centered by dividing by 255. Image label vectors were converted into binary matrics. Input shape for image data is 80 x 80 x 3 which represents 19200 features. \n",
    "\n",
    "In total, there are 24076 pieces of images with 45% are \"authorized\" images. After dividing, there are 12639 train samples, 5418 validation samples and 6019 test samples. All three groups have similar mean value (~ 0.42) and standard deviation (~ 0.27).\n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "In this project, we have selected the convolutional neural network (CNN) which has been proved particually useful for imaging classification and face recognition. The architecuture of CNN has been tuned many times. I found the architecture with 8 layers returns the best accuracy. Below is the detailed 8 layers (denoted as L1, ..., L8) which starts with input image and ends with softmax loss layer (Figure 2). \n",
    "\n",
    "![architucure of CNN](CNN.png)\n",
    "\n",
    "**Figure 2. Architecure of CNN **\n",
    "Conv: Convolutional layer, FC: Fully connected layer\n",
    "\n",
    "Following each Maxpooling, a drop-out with probability of 0.25 is applied. For the fully-connected dense layer L7, a drop-out with probability of 0.5 is applied. The final full-connected dense layer takes the outputs of L7 as its input, produce outputs equal to 2 classes through a fully connected architecture, then passes these outputs throught a softmax function. With softmax loss layer, each input image is expected to have only one label.\n",
    "\n",
    "In order to make the deep neural network training more robust to facial pose variation, real time data augmentation is used. Specifically, I have randomly rotated image in the range 0 to 20 degree and randomly flipped image vertically. Since some of the images collected from LFW were shifted, to avoid any duplication, width_shift and height_shift have been disabled during data augmentation. I have also arbitrary set the channel_shift_range to 0.2 and fill_mode as \"nearest\".In total, there are 9,422,882 trainable params.\n",
    "\n",
    "\n",
    "### Refinement\n",
    "\n",
    "Initial tuning has been focus on the epoch number, the problem associated with epoch number is that it takes longer time to test larger epoch number. so I have sticked to epoch number with no more than 40. Although as epoch number increase,we usually get better accuracy, it always over shot and miss the best epoch number. After the initial testing, I turn to early stoping method by setting the callbacks argument value equals earlyStopping with 'val_acc' as the monitor.  \n",
    "\n",
    "To find the best combination of batch size and optimizer, I have tested batch size ranging from 40 to 90 with Adam, sgd or adamax optimizer. The arguments of each optimizer were set as default as indicated in keras document. The result suggested that optimizer sgd is very sensitive to the batch size. The accuracy of train data set with sgd optimizer ranges from 84.72% to 96.65% when batch size changing from 40 to 90. On the contrary, both adam and adamax are not very sensitive to the batch size, the accuracy of train data set for both optimizers ranges from 98.66% to 99.38%, and from 98.85% to 99.21%. The difference among the accuracy of train data set for both optimizers are within 1.0%. \n",
    "\n",
    "When compiling the model, I have tested two loss functions which are categorical-cross entropy and binary_crossentropy. I have found that the former returns better accuracy. I have also tested two metrics which are accuracy and fmeasure. It turns out that accuracy returns better result.\n",
    "\n",
    "\n",
    "## IV. Results\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "\n",
    "When training, model were further cross validated with validation data set. When evaluate the model, accuracy was selected as the default metrics. From the returned verbose information, we can see that accuracy of validation data set shows similar trend as the accuracy of training data set. When using optimizer sgd, the accuracy of validation data set was sensitive to the change of batch size (from 84.03% to 97.73%), while using either adam or adamax optimizer, the accuracy of validation data set hardly changed (adam: from 98.93% to 99.59%, adamax: from 98.91% to 99.54%). Of all optimizsers I have tested, batch size 40 returns the best accuracy for validation data set, and adam optimizer returns the best accuracy for validation data set. It is worth to note that the accuracy returned by adam and adamax optimizers does not show significant difference.\n",
    "\n",
    "### Justification\n",
    "\n",
    "The optimized models were further evaluated with test data set. As I expected, the accuracy for the test data set shows the similar trend as the accuracy for both validation and training data set. When using sgd and adamax as the optimizer, batch size 40 returns the best accuracy for test data set. However, when using adam as the optimizer, batch_size 70 returns the best accuracy for test data set although batch_size 40 has the best accuracy for validation data set. With adam as the optimizer and 70 batch has the best accuracy for test data set among all test conditions.\n",
    "\n",
    "Finally, to verify the robustness of the model, I have tested the best model (optimizer: adam, batch size: 70) with 20 new images which have 10 authorized and 10 unauthorized images. It is worth to mention that the time points of authorized images were span for about 10 years with different background. Interesting, this model predict all images correctly with 100% accuracy (see PredictNewImage.ipynb and Figure 4).\n",
    "\n",
    "## V. Conclusion\n",
    "\n",
    "### Free-Form Visualization\n",
    "\n",
    "![Accuracy of Models](accuracy.png)\n",
    "\n",
    "**Figure 3. The Accuracy of test data**\n",
    "\n",
    "![Images](NewImage.png)\n",
    "\n",
    "**Figure 4. Prediction with new Image**\n",
    "NOTE: all images were collectly predicted with the model trained with batch size at 70 and a adam optimizer\n",
    "\n",
    "### Reflection\n",
    "\n",
    "In this I have established a face recognition model with the state-of-the-art CNN algorithm. The accuracy of the test data set is 99.70%. The whole process of this project can be divided into the following steps:\n",
    "\n",
    "1. Problem initialization\n",
    "2. Working Environment preparation\n",
    "3. Data collection and exploration\n",
    "4. Data processing and spliting\n",
    "5. Model training and Parameter tuning\n",
    "6. Model testing\n",
    "\n",
    "Among all these steps, the step 2, 3 and 5 are most challenging for me. \n",
    "\n",
    "For setting up the working environment, I need to install Tensorflow with GPU in my Windows 10 desktop. To accelerate the video image capturing and image processing, I have also install OpenCV and dlib. These instalation steps are trivial but very time consuming and sometimes can be very desperate. Thanks for the internet, I am happy that I am able to make them work. \n",
    "\n",
    "As for Data Collection, it is also a time-consuming step. Originally, I proposed to use images from [Faces94](http://cswww.essex.ac.uk/mv/allfaces/faces94.html). These images contain only the head part. Moreover, the background for all images is basically same with green color. I decided to give up using these images because they may not represent a real world situation. Fortunately, I was able to find the images from [LFW](http://vis-www.cs.umass.edu/lfw/) with more diverse background and variation. So I decided to use this image database as my control (unauthorized) data. To collect the authorized image data, I initially manually took my photos with camera which is a very time-consuming step. It seems a daunting task to collect 10000 images manually. After googling on the internet, I was able to find that OpenCV enable one to automatically capture image from videos. So I then turn to this automatically procedure. Finally, I have collected more than 10000 images from 20 shot videos taken with different dress styles, background and emotions. I think these images should be more close to a real world scenario.\n",
    "\n",
    "Finally, the model training and tuning step is one of the most challengling and rewarding parts. As a student starting before Oct, 2016. I did not learn any deep learning knowledge in our machine learning nanodegree program. So I have to learn it myself. I have followed the [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/) offered by stanford and [Deep Learning](https://classroom.udacity.com/courses/ud730). For parameters tuning, it was quiet complicated with so many parameters and a not so powerful computer.\n",
    "\n",
    "### Improvement\n",
    "\n",
    "Although the accuracy of the test data set has reached up to 99.70%, it might not be ture when applying this model with more diverse images or exposure to more robust testing. To achieve better accuracy and more reliable model, the following methods might be worth to give a shot.\n",
    "\n",
    "- Collecting more data, as we all know that the key point of training a good CNN model is the data size.\n",
    "- Using synthesis images, as I have already collected more than 10000 images, these image can be using as the start point to synthesize more images as has been reported by [Hu]('LDFaceRepresentationsUsingSmallData.pdf'). \n",
    "- Cropping only face landmark, this method could be very technically challenging and the face landmark captured may not be a real face. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML \n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
