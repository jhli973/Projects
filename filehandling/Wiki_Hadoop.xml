<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.28.0-wmf.4</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
      <namespace key="2600" case="first-letter">Topic</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Cloudera</title>
    <ns>0</ns>
    <id>27059655</id>
    <revision>
      <id>718846355</id>
      <parentid>718846072</parentid>
      <timestamp>2016-05-05T23:58:40Z</timestamp>
      <contributor>
        <username>Faramir1138</username>
        <id>13258862</id>
      </contributor>
      <comment>gave citation a name</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17312">{{Infobox company
| name              = Cloudera
| logo              = Cloudera logo tag rgb.png
| logo_size         = 220
| logo_caption      = Cloudera, Inc. logo with tagline
| type = Privately held
| num_locations = Palo Alto, Chicago, San Francisco, New York, North Carolina, Virginia, Austin Texas, New Hampshire, Atlanta Georgia, Kirkland WA, McLean VA

EMEA - London, Paris, Budapest.

Asia / APJ - Shanghai China, Beijing China, Tokyo Japan, Singapore, Seoul Korea,  Melbourne Australia, India Chennai
| industry = Software Development
| products = Cloudera Manager, Cloudera Navigator, Gazzang, Cloudera Navigator Optimizer, Impala
| services = [[Apache Hadoop]] distribution with support, professional services and training
| num_employees     = 1,250&lt;ref name=&quot;cloudera.com&quot;&gt;{{cite web|url=http://www.cloudera.com/about-cloudera.html|title=About Cloudera|work=Cloudera}}&lt;/ref&gt;
| homepage          = {{URL|http://www.cloudera.com/}}
| location          = [[Palo Alto, California]]
}}

'''Cloudera Inc.''' is an American-based software company that provides [[Apache Hadoop]]-based software, support and services, and training to business customers.

Cloudera's open-source Apache Hadoop distribution, CDH (Cloudera Distribution Including Apache Hadoop), targets enterprise-class deployments of that technology. Cloudera says that more than 50% of its engineering output is donated upstream to the various Apache-licensed open source projects (Apache Hive, Apache Avro, [[Apache HBase]], and so on) that combine to form the Hadoop platform. Cloudera is also a sponsor of the [[Apache Software Foundation]].&lt;ref name=&quot;Apache Software Foundation Sponsorship&quot;&gt;{{cite web|url=http://www.apache.org/foundation/sponsorship.html|title=Apache Software Foundation Sponsorship|accessdate=28 August 2012}}&lt;/ref&gt;

==History==
Three engineers from [[Google]], [[Yahoo]] and [[Facebook Inc.|Facebook]] ([[Christophe Bisciglia]], [[Amr Awadallah]] and [[Jeff Hammerbacher]], respectively) joined with a former [[Oracle Corporation|Oracle]] executive ([[Michael Olson|Mike Olson]]) to form Cloudera in 2008.&lt;ref&gt;{{cite news |url= http://bits.blogs.nytimes.com/2009/03/16/bottling-the-magic-behind-google-and-facebook/ |title= Bottling the Magic Behind Google an Facebook |last= Vance |first= Ashlee |date= 16 March 2009 |accessdate= 20 January 2014 |work=The New York Times}}&lt;/ref&gt; 
Olson was the CEO of [[Sleepycat Software]], the creator of the open-source embedded database engine [[Berkeley DB]] (acquired by Oracle in 2006). 
Awadallah was from [[Yahoo!|Yahoo]], where he ran one of the first business units using Hadoop for data analysis.&lt;ref&gt;{{cite web|url=http://articles.businessinsider.com/2012-01-31/news/31008410_1_yahoo-servers-and-storage-systems-oracle |title=This Former Yahoo-er's Startup Is So Hot, Even the CIA Invested In It |accessdate=28 August 2012 |deadurl=yes |archiveurl=https://web.archive.org/20120209182001/http://articles.businessinsider.com:80/2012-01-31/news/31008410_1_yahoo-servers-and-storage-systems-oracle |archivedate=February 9, 2012 }}&lt;/ref&gt;
At Facebook Hammerbacher used Hadoop for building analytic applications involving massive volumes of user data.&lt;ref&gt;{{cite web|url=http://www.businessweek.com/magazine/content/11_17/b4225060960537.htm|title=This Tech Bubble Is Different|accessdate=28 August 2012}}&lt;/ref&gt;

Architect [[Doug Cutting]], also a former chairman of the [[Apache Software Foundation]], authored the open-source [[Lucene]] and [[Nutch]] search technologies before he wrote the initial Hadoop software in 2004. He designed and managed a Hadoop storage and analysis cluster at Yahoo! before joining Cloudera in 2009.
Chief operating officer was Kirk Dunn.&lt;ref&gt;{{cite web
|url=http://investing.businessweek.com/research/stocks/private/person.asp?personId=1032567&amp;privcapId=5648440&amp;previousCapId=5648440&amp;previousTitle=PowerFile,%20Inc.
|title=Bloomberg Business Week, Executive Profile Kirk Dunn 
|accessdate=30 September 2012}}&lt;/ref&gt;

In March 2009, Cloudera announced the availability of Cloudera Distribution Including [[Apache Software Foundation|Apache]] [[Hadoop]] in conjunction with a $5 million investment led by [[Accel Partners]].&lt;ref&gt;{{cite web|url=http://techcrunch.com/2009/03/16/cloudera-raises-5-million-series-a-round-for-hadoop-commercialization/|title=Cloudera Raises $5 Million Series A Round For Hadoop Commercialization|last=Wauters |first=Robin|date=16 March 2009|publisher=TechCrunch|accessdate=22 April 2010}}&lt;/ref&gt; In 2011, the company raised a further $40 million from [[Ignition Partners]], [[Accel Partners]], [[Greylock Partners]], [[Meritech Capital Partners]], and [[In-Q-Tel]], a [[venture capital]] firm with open connections to the [[CIA]].&lt;ref&gt;{{cite web|url=http://venturebeat.com/2011/11/07/hadoop-cloudera-funding-ignition-accel-greylock/|title=Hadoop-based startup Cloudera raises $40M from Ignition Partners, Accel, Greylock|accessdate=28 August 2012}}&lt;/ref&gt;

In June 2013 Tom Reilly became chief executive, although Olson remained as chairman of the board and chief strategist. Reilly was chief executive at [[ArcSight]] when it was acquired by [[Hewlett-Packard]] in 2010.&lt;ref&gt;{{Cite news |title= Cloudera taps new CEO for inevitable IPO push or acquisition: Former CEO becomes chairman and chief strategist |author= Timothy Prickett Morgan |work= The Register |date= 20 June 2013 |url= http://www.theregister.co.uk/2013/06/20/cloudera_taps_new_ceo_for_inevitable_ipo_push_or_acquisition/ |accessdate= 20 January 2014 }}&lt;/ref&gt; In March 2014 Cloudera announced a $900 million funding round, led by Intel Capital ($740 million), for that Intel received 18% share in cloudera and Intel dropped its own Hadoop distribution and dedicated 70 Intel engineers to work exclusively on cloudera projects. With additional funds coming from T Rowe Price, Google Ventures and an affiliate of MSD Capital, L.P., the private investment firm for Michael S. Dell. and others.&lt;ref&gt;{{cite web|url=http://www.reuters.com/article/2014/03/31/us-intel-cloudera-idUSBREA2U0ME20140331|title=Intel invested $740 million to buy 18 percent of Cloudera|author=Noel Randewich|date=31 March 2014|work=Reuters}}&lt;/ref&gt;

In January 2012 - Oracle announced exclusive partnership with Cloudera to provide first Big data Appliance powered by Cloudera&lt;ref&gt;{{cite web|url=http://www.oracle.com/us/corporate/press/1453796|title=Oracle Selects Cloudera to Provide Apache Hadoop Distribution and Tools for Oracle Big Data Appliance|publisher=}}&lt;/ref&gt;

In January 2013 - Dell announced a strategic partnership with Cloudera&lt;ref&gt;{{cite web|url=http://www.dell.com/learn/us/en/555/solutions/hadoop-big-data-solution|title=Dell Apache Hadoop Solutions - Dell|author=Dell us|work=Dell}}&lt;/ref&gt;

In March 2013 - Intel invested $740 Million in Cloudera for an 18% investment&lt;ref&gt;{{cite web|url=http://www.reuters.com/article/us-intel-cloudera-idUSBREA2U0ME20140331|title=Intel invested $740 million to buy 18 percent of Cloudera}}&lt;/ref&gt;

In May 2013 - SAS announced a strategic partnership with Cloudera on how to driver predictive analytics with SAS software&lt;ref&gt;{{cite web|url=https://blog.cloudera.com/blog/2013/05/how-the-sas-and-cloudera-platforms-work-together/|title=How the SAS and Cloudera Platforms Work Together|work=Cloudera Engineering Blog}}&lt;/ref&gt;

In June 2014 - Accenture announced - Data as a platform offering based on Cloudera&lt;ref&gt;{{cite web|url=https://newsroom.accenture.com/industries/systems-integration-technology/accenture-forms-alliance-with-cloudera-to-empower-enterprises-with-a-data-as-a-platform-offering.htm|title=Accenture Forms Alliance with Cloudera to Empower Enterprises with Data as a Platform Offering|publisher=}}&lt;/ref&gt;

In June 2014 - Cloudera acquired Gazzang - this gives Cloudera the most secure Hadoop distribution on the marketplace. This delivers enterprise-grade data encryption and key management, addressing head on the challenges associated with securing and processing sensitive and legally protected data within the Hadoop ecosystem. Thus fulfilling a requirement in myriad compliance regulations like HIPAA-HITECH, PCI-DSS, FERPA and the EU Data Protection Directive.&lt;ref&gt;{{cite web|url=http://www.cloudera.com/about-cloudera/press-center/press-releases/2014-06-03-cloudera-strengthens-hadoop-security-with-acquisition-of-gazzang.html|title=cloudera strengthens hadoop security with acquisition of gazzang|work=Cloudera}}&lt;/ref&gt;

In October 2014 - Cloudera announces the first PCI compliant hadoop distribution with MasterCard.&lt;ref&gt;{{cite web|url=http://www.cloudera.com/about-cloudera/press-center/press-releases/2014-10-22-cloudera-enterprise-certified-for-full-pci-compliance.html|title=cloudera enterprise certified for full pci compliance|work=Cloudera}}&lt;/ref&gt;

In February 2015 - Deloitte announced a strategic alliance with Cloudera on Advanced Analytics&lt;ref&gt;{{cite web|url=http://www.cloudera.com/about-cloudera/press-center/press-releases/2015-02-19-cloudera-and-deloitte-announce-strategic-alliance.html|title=cloudera deloitte announce strategic alliance|work=Cloudera}}&lt;/ref&gt;

In May 2015 - Capgemini announced SAP HANA - Cloudera Insight driven operations - get more for less with your SAP HANA installation with Cloudera Enterprise&lt;ref&gt;{{cite web|url=https://www.capgemini.com/resources/insights-driven-operations-with-sap-hana-and-cloudera-enterprise|title=Insights-Driven Operations with SAP HANA and Cloudera Enterprise|work=Capgemini Capgemini Worldwide}}&lt;/ref&gt;

In July 9, 2015 - Cloudera announced partnership with Teradata - Integrated Enterprise ready Appliance for Hadoop&lt;ref&gt;{{cite web|url=http://www.cloudera.com/about-cloudera/press-center/press-releases/2015-07-09-cloudera-and-terdata-announce-enterprise-ready-appliance-for-hadoop.html|title=Cloudera and Teradata Announce Integrated, Enterprise-Ready Appliance for Hadoop|work=Cloudera}}&lt;/ref&gt;

In September 2015 - Cloudera announced Kudu - New Apache Hadoop Storage for fast analytics on fast data&lt;ref&gt;{{cite web|url=https://blog.cloudera.com/blog/2015/09/kudu-new-apache-hadoop-storage-for-fast-analytics-on-fast-data/|title=Kudu: New Apache Hadoop Storage for Fast Analytics on Fast Data|work=Cloudera Engineering Blog}}&lt;/ref&gt;

In September 2015 - Cloudera announced RecordService - Fine-grained security enforcement across the Hadoop ecosystem&lt;ref&gt;{{cite web|url=https://blog.cloudera.com/blog/2015/09/recordservice-for-fine-grained-security-enforcement-across-the-hadoop-ecosystem/|title=RecordService: For Fine-Grained Security Enforcement Across the Hadoop Ecosystem|work=Cloudera Engineering Blog}}&lt;/ref&gt;

In September 2015 - Microsoft Azure announced full support of Cloudera Enterprise on Azure&lt;ref&gt;{{cite web|url=https://azure.microsoft.com/en-us/blog/full-cloudera-enterprise-edh-support-on-azure/|title=Full support of Cloudera Enterprise on Azure|publisher=Microsoft}}&lt;/ref&gt;

In November 2015 - Cloudera announced that it had reached 1,000 employees.

In Jan 2016 - TCS announced an IoT framework based on Cloudera for sensor data analytics&lt;ref&gt;{{cite web|url=http://www.cloudera.com/resources/solution-brief/tcs-sensor-data-analytics-iot-framework-with-cloudera.html|title=TCS Sensor Data Analytics IoT Framework with Cloudera|work=Cloudera}}&lt;/ref&gt;

In Feb 2016 - EMC announces evolution in advanced storage with DSSD support for Cloudera&lt;ref&gt;{{cite web |url=http://www.emc.com/collateral/data-sheet/h14899-ds-d5-emc-dssd-and-cloudera-solution-brief.pdf |title=EMC DSSD and Cloudera Evolve Hadoop: Innovating to deliver high-performance enterprise analytics on Hadoop |publisher=[[EMC Corporation]] |year=2016 |accessdate=April 25, 2016}}&lt;/ref&gt;

In March 2016 Cloudera announced onDemand Training. This allows consultants, Big data practitioners to take training at a time that suits their schedule.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/training/ondemand-training.html|title=OnDemand Training|work=Cloudera}}&lt;/ref&gt;

In 2016 - Cloudera has expanded their international teams with over 200 people employed in EMEA with staff in over 12 countries in EMEA.

In March 2016 - Cloudera announced it had over 2,100 partners consisting of 450 Software and OEM, 1400 System Integrators and Resellers, 200 Platform and Cloud and 50 Data Systems. This means that your existing investment in ISVs, Hardware and platform partners is maintained with Cloudera. Cloudera has the largest partner eco-systems of all Hadoop Distributions.&lt;ref name=&quot;cloudera.com&quot;/&gt;

==Products and services==
Cloudera offers software, services and support in three different bundles:
* Cloudera Enterprise includes CDH and an annual subscription license (per node) to Cloudera Manager and technical support. It comes in three editions: Basic, Flex, and Data Hub.
* Cloudera Express includes CDH and a version of Cloudera Manager lacking enterprise features such as rolling upgrades and backup/disaster recovery, LDAP and SNMP integration.
* CDH may be downloaded from Cloudera's website at no charge, but with no technical support nor Cloudera Manager.
* Cloudera Navigator - is the only complete data governance solution for Hadoop, offering critical capabilities such as data discovery, continuous optimization, audit, lineage, metadata management, and policy enforcement. As part of Cloudera Enterprise, Cloudera Navigator is critical to enabling high-performance agile analytics, supporting continuous data architecture optimization, and meeting regulatory compliance requirements.&lt;ref&gt;{{cite web|url=https://cloudera.com/products/cloudera-navigator.html|title=Cloudera Navigator|work=Cloudera}}&lt;/ref&gt;
* Cloudera Navigator Optimizer (beta) - A SaaS based tool to provides instant insights into your workloads and recommends optimization strategies to get the best results with Hadoop.&lt;ref&gt;{{cite web|url=https://optimizer.cloudera.com/|title=Cloudera Optimizer|publisher=}}&lt;/ref&gt;

Services
* Cloudera University - World class training provided by Cloudera university&lt;ref name=&quot;cloudera.com&quot;/&gt;
* Cloudera Professional Services - World class professionals to help with customer success.&lt;ref&gt;{{cite web|url=http://www.cloudera.com/services-support/professional-services.html|title=Cloudera Professional Services|work=Cloudera}}&lt;/ref&gt;
* Cloudera Support - World class support team available 24x7 to help customers proactively and predictively ensure success&lt;ref&gt;{{cite web|url=https://cloudera.com/services-support.html|title=Get support where and when you need it.|work=Cloudera}}&lt;/ref&gt;

All versions may be downloaded from Cloudera's website.

CDH contains the main, core elements of Hadoop that provide reliable, scalable distributed data processing of large data sets (chiefly [[MapReduce]] and HDFS), as well as other enterprise-oriented components that provide security, high availability, and integration with hardware and other software.&lt;ref&gt;{{cite web|url=http://www.informationweek.com/software/information-management/cloudera-releases-next-generation-hadoop/240001574/|title=Cloudera Releases Next-Generation Hadoop Platform|last=Henschen |first=Doug|date=6 June 2012|publisher=InformationWeek|accessdate=22 April 2010}}&lt;/ref&gt;

In October 2012, Cloudera announced the [[Cloudera Impala]] project, an open-source distributed query engine for Apache Hadoop.&lt;ref&gt;{{cite web |url= http://www.zdnet.com/clouderas-impala-brings-hadoop-to-sql-and-bi-7000006413/|title=Cloudera’s Impala brings Hadoop to SQL and BI |last= Brust |first= Andrew |date= 25 October 2012 |work= ZDNet |accessdate= 20 January 2014 }}&lt;/ref&gt;

==Awards==
* In April 2010, Chief Scientist [[Jeff Hammerbacher]] was named a &quot;Best Young Tech Entrepreneur&quot; by Bloomberg BusinessWeek.&lt;ref&gt;{{cite web|url=http://www.businessweek.com/technology/special_reports/20100420best_young_tech_entrepreneurs.htm |title=Best Young Technology Entrepreneurs 2010 |accessdate=28 August 2012 |deadurl=yes |archiveurl=https://web.archive.org/20120707100946/http://www.businessweek.com:80/technology/special_reports/20100420best_young_tech_entrepreneurs.htm |archivedate=July 7, 2012 }}&lt;/ref&gt;
* In June 2012, received [[Morgan Stanley]]'s &quot;CTO Award for Innovation&quot;.&lt;ref&gt;{{cite web|url=http://www.marketwatch.com/story/cloudera-honored-by-morgan-stanley-with-prestigious-cto-award-for-innovation-2012-06-28|title=Cloudera Honored by Morgan Stanley With Prestigious 'CTO Award for Innovation'|accessdate=28 August 2012}}{{dead link|date=November 2015}}&lt;/ref&gt;
* In August 2012, CRN named Cloudera among the &quot;The 25 Coolest Emerging Vendors For 2012&quot;.&lt;ref&gt;{{cite web|url=http://www.crn.com/slide-shows/storage/240005642/the-25-coolest-emerging-vendors-for-2012.htm|title=The 25 Coolest Emerging Vendors For 2012|accessdate=28 August 2012}}&lt;/ref&gt;

==References==
{{reflist|30em}}

==External links==
* {{Official website|http://www.cloudera.com/}}
* [https://optimizer.cloudera.com/ Cloudera Navigator Optimizer]
* [http://www.cloudera.com/customers.html Cloudera Customers]
* [https://azure.microsoft.com/en-us/marketplace/partners/cloudera/clouderaedhcloudera/  Cloudera on Azure]
* [http://www.cloudera.com/partner-portal.html Cloudera partner Portal, Cloudera Connect]

[[Category:Big data companies]]
[[Category:Cloud computing providers]]
[[Category:Cloud infrastructure]]
[[Category:Distributed file systems]]
[[Category:Hadoop]]
[[Category:Software companies based in the San Francisco Bay Area]]</text>
      <sha1>seselpyrrvmp0xs0dw668nwh9k6v7je</sha1>
    </revision>
  </page>
  <page>
    <title>Cascading (software)</title>
    <ns>0</ns>
    <id>1915249</id>
    <revision>
      <id>712040096</id>
      <parentid>711915397</parentid>
      <timestamp>2016-03-26T14:57:10Z</timestamp>
      <contributor>
        <username>Nealmcb</username>
        <id>4375</id>
      </contributor>
      <comment>/* Domain-Specific Languages Built on Cascading */ clarify scalding</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7650">{{Infobox software
| name                   = Cascading
| status                 = Active
| latest release version = 3.0
| programming language   = [[Java (programming language)|Java]]
| license                = [[Apache License]]
| website                = http://www.cascading.org/
}}
'''Cascading''' is a software [[abstraction layer]] for [[Apache Hadoop]] and [[Apache Flink]].  Cascading is used to create and execute complex data processing workflows on a Hadoop cluster using any [[JVM]]-based language ([[Java (programming language)|Java]], [[JRuby]], [[Clojure]], etc.), hiding the underlying complexity of [[MapReduce]] jobs.  It is open source and available under the [[Apache License]]. Commercial support is available from Driven, Inc.&lt;ref&gt;[http://driven.io/support Cascading support page]&lt;/ref&gt; 

Cascading was originally authored by Chris Wensel, who later founded Concurrent, Inc, which has been re-branded as Driven &lt;ref&gt;[http://www.driven.io Driven, Inc.]&lt;/ref&gt; Cascading is being actively developed by the community{{fact|date=October 2014}} and a number of add-on modules are available.&lt;ref&gt;[http://www.cascading.org/modules.html Cascading modules]&lt;/ref&gt;

==Architecture==
To use Cascading, Apache Hadoop must also be installed, and the Hadoop job .jar must contain the Cascading .jars. Cascading consists of a data processing API, integration API, process planner and process scheduler.  

Cascading leverages the scalability of Hadoop but abstracts standard data processing operations away from underlying map and reduce tasks.&lt;ref&gt;[http://codeascraft.etsy.com/2010/02/24/analyzing-etsys-data-with-hadoop-and-cascading/  Blog post by Etsy describing their use of Cascading with Hadoop]&lt;/ref&gt;{{Better source|date=October 2013}}  Developers use Cascading to create a .jar file that describes the required processes. It follows a ‘source-pipe-sink’ paradigm, where data is captured from sources, follows reusable ‘pipes’ that perform data analysis processes, where the results are stored in output files or ‘sinks’. Pipes are created independent from the data they will process. Once tied to data sources and sinks, it is called a ‘flow’. These flows can be grouped into a ‘cascade’, and the process scheduler will ensure a given flow does not execute until all its dependencies are satisfied. Pipes and flows can be reused and reordered to support different business needs.&lt;ref&gt;[http://www.cascading.org/1.2/userguide/pdf/userguide.pdf Cascading User Guide]&lt;/ref&gt;  

Developers write the code in a JVM-based language and do not need to learn MapReduce. The resulting program can be regression tested and integrated with external applications like any other Java application.&lt;ref&gt;[http://www.driven.io/features/ Driven product page]&lt;/ref&gt;

Cascading is most often used for ad targeting, log file analysis, bioinformatics, machine learning, predictive analytics, web content mining, and extract, transform and load (ETL) applications.&lt;ref&gt;[http://www.driven.io/ Driven home page]&lt;/ref&gt;

==Uses of Cascading==
Cascading was cited as one of the top five most powerful Hadoop projects by SD Times in 2011,&lt;ref name=sdtimes1&gt;{{cite news
| last = Handy
| first = Alex
| date = 1 June 2011
| title = The top five most powerful Hadoop projects
| url =http://www.sdtimes.com/content/article.aspx?ArticleID=35596&amp;page=1
| newspaper = [[SD Times]]
| location = 
| publisher = 
| accessdate = 26 October 2013
}}&lt;/ref&gt;{{Verify credibility|date=October 2013}} as a major open source project relevant to bioinformatics&lt;ref name=biomedcent1&gt;{{cite news
| last = Taylor
| first = Ronald
| date = 21 December 2010
| title = An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics
| url = http://www.biomedcentral.com/1471-2105/11/S12/S1
| newspaper = [[BioMed Central]]
| location = 
| publisher = [[Springer Science+Business Media]]
| accessdate = 26 October 2013
}}&lt;/ref&gt;{{Verify credibility|date=October 2013}} and is included in Hadoop: A Definitive Guide, by Tom White.&lt;ref&gt;[http://books.google.com/books?id=Nff49D7vnJcC&amp;lpg=PA539&amp;dq=cascading%20hadoop&amp;pg=PA548#v=onepage&amp;q=cascading%20hadoop&amp;f=false White, Tom, “Hadoop: The Definitive Guide,” O’Reilly Media, Inc., 2010, pp. 539 – 549.]&lt;/ref&gt; The project has also been cited in presentations, conference proceedings and Hadoop user group meetings as a useful tool for working with Hadoop&lt;ref&gt;[http://www.slideshare.net/pacoid/getting-started-on-hadoop Nathan, Paco (Wikipedia: [[Paco Nathan]]), “Getting Started on Hadoop” presentation for the SV Cloud Computing Meetup, 7/19/2010.]&lt;/ref&gt;&lt;ref&gt;[http://www.smartfrog.org/wiki/download/attachments/6193590/hadoop_and_beyond.pdf?version=1&amp;modificationDate=1238073739000 Julio Guijarro, Steve Loughran and Paolo Castagna, “Hadoop and beyond,” HP Labs, Bristol UK, 2008.]&lt;/ref&gt;&lt;ref&gt;[http://www.slideshare.net/hadoopusergroup/flightcaster-presentation-hadoop Cross, Bradford, “Flightcaster_HUG,” Presentation at the Bay Area Hadoop Users’ Group, March 26, 2010]&lt;/ref&gt;&lt;ref&gt;[http://www.slideshare.net/chriscurtin/nosql-hadoop-cascading-june-2010?from=ss_embed Curtin, Christopher, “NoSQL, Hadoop and Cascading,” June 2010.]&lt;/ref&gt; and with [[Apache Spark]]&lt;ref&gt;{{Cite web|url=https://spark-summit.org/2014/talk/using-cascading-to-build-data-centric-applications-on-spark|title=Using Cascading to Build Data-centric Applications on Spark|date=2014-05-07|website=Spark Summit 2014|access-date=2016-03-25}}&lt;/ref&gt;

* MultiTool on [[Amazon Web Services]] was developed using Cascading.&lt;ref&gt;[http://aws.amazon.com/articles/2293?_encoding=UTF8&amp;jiveRedirect=1 Cascading{{Not a typo|.}}Multitool on AWS]&lt;/ref&gt;
* LogAnalyzer for [[Amazon CloudFront]] was developed using Cascading.&lt;ref&gt;[http://aws.amazon.com/articles/2440?_encoding=UTF8&amp;jiveRedirect=1 LogAnalyzer for Amazon CloudFront]&lt;/ref&gt; 
* BackType&lt;ref&gt;[http://tech.backtype.com/ BackType blog]&lt;/ref&gt; - social analytics platform
* Etsy&lt;ref&gt;[http://codeascraft.etsy.com/2010/02/24/analyzing-etsys-data-with-hadoop-and-cascading/  Blog post by Etsy describing their use of Cascading with Hadoop]&lt;/ref&gt; - marketplace
* FlightCaster&lt;ref&gt;[http://www.informationweek.com/news/software/infrastructure/224000240 FlightCaster]&lt;/ref&gt; - predicting flight delays
* Ion Flux&lt;ref&gt;[http://www.concurrentinc.com/casestudies/ion_flux Ion Flux]&lt;/ref&gt;  - analyzing DNA sequence data
* RapLeaf&lt;ref&gt;[http://blog.rapleaf.com/dev/2008/09/05/goodbye-mapreduce-hello-cascading/ RapLeaf Blog] {{wayback|url=http://blog.rapleaf.com/dev/2008/09/05/goodbye-mapreduce-hello-cascading/ |date=20110201023302 }}&lt;/ref&gt;  - personalization and recommendation systems
* Razorfish&lt;ref&gt;[http://aws.amazon.com/solutions/case-studies/razorfish/ Razorfish]&lt;/ref&gt;  - digital advertising

==Domain-Specific Languages Built on Cascading==
* PyCascading&lt;ref&gt;[https://github.com/twitter/pycascading]&lt;/ref&gt; - by Twitter, available on GitHub
* Cascading.jruby&lt;ref&gt;[https://github.com/gmarabout/cascading.jruby Cascading.jruby]&lt;/ref&gt; - developed by Gregoire Marabout, available on GitHub
* [[Cascalog]]&lt;ref&gt;[https://github.com/nathanmarz/cascalog Cascalog]&lt;/ref&gt; - authored by [[Nathan Marz]], available on GitHub
* Scalding&lt;ref&gt;[https://github.com/twitter/scalding Scalding]&lt;/ref&gt; - A Scala API for Cascading.  Makes it easier to transition Cascading/Scalding code to Spark. By Twitter, available on GitHub

==References==
{{Reflist|30em}}

==External links==
* [http://www.cascading.org/ Official website]

[[Category:Free software programmed in Java (programming language)]]
[[Category:Free system software]]
[[Category:Cloud infrastructure]]
[[Category:Hadoop]]</text>
      <sha1>msn5ulxnwlr7iih7ffncf9p9mpfog8y</sha1>
    </revision>
  </page>
  <page>
    <title>Apache HBase</title>
    <ns>0</ns>
    <id>16266878</id>
    <revision>
      <id>722595411</id>
      <parentid>721263434</parentid>
      <timestamp>2016-05-29T04:35:09Z</timestamp>
      <contributor>
        <ip>69.114.210.219</ip>
      </contributor>
      <comment>/* See also */Added Bigtable to the list.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7637">{{Use dmy dates|date=October 2013}}
{{Infobox software
| name                   = Apache HBase
| logo                   = [[File:HBase Logo.png]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.2.1
| latest release date    = {{release date|df=yes|2016|4|11}}
| latest preview version = 
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://hbase.apache.org/}}
}}

'''HBase''' is an [[open source]], [[Non-relational database|non-relational]], [[distributed database]] modeled after [[Google|Google's]] [[BigTable]] and written in [[Java (programming language)|Java]]. It is developed as part of [[Apache Software Foundation]]'s [[Hadoop|Apache Hadoop]] project and runs on top of [[Hadoop Distributed Filesystem|HDFS (Hadoop Distributed Filesystem)]], providing BigTable-like capabilities for Hadoop. That is, it provides a [[fault-tolerant]] way of storing large quantities of [[sparse file|sparse]] data (small amounts of information caught within a large collection of empty or unimportant data, such as finding the 50 largest items in a group of 2 billion records, or finding the non-zero items representing less than 0.1% of a huge collection). 

HBase features compression, in-memory operation, and [[Bloom filter]]s on a per-column basis as outlined in the original BigTable paper.&lt;ref&gt;[http://db.usenix.org//events/osdi06/tech/chang/chang_html/ Chang, et al. (2006). Bigtable: A Distributed Storage System for Structured Data]&lt;/ref&gt; Tables in HBase can serve as the input and output for [[Mapreduce|MapReduce]] jobs run in Hadoop, and may be accessed through the [http://hbase.apache.org/apidocs/index.html Java API] but also through [[REST]], [[Avro_(serialization_system)|Avro]] or [[Thrift (protocol)|Thrift]] gateway APIs. Hbase is a column-oriented key-value data store and has idolized widely because of its lineage with Hadoop and HDFS. HBase runs on top of HDFS and is well-suited for faster read and write operations on large datasets with high throughput and low input/output latency.&lt;ref name = hbase1&gt;[https://intellipaat.com/tutorial/hbase-tutorial/ &quot;Hbase in Nutshell&quot;]&lt;/ref&gt;


HBase is not a direct replacement for a classic [[SQL]] [[database]], however [[Apache Phoenix]] project provides an SQL layer for Hbase as well as [[JDBC]] driver that can be integrated with various [[analytics]] and [[business intelligence]] applications.  The [[Apache Trafodion]] project provides a SQL query engine with [[ODBC]] and [[JDBC]] drivers and distributed ACID transaction protection across multiple statements, tables and rows that uses HBase as a storage engine. 

Hbase is now serving several data-driven websites,&lt;ref&gt;[http://hbase.apache.org/poweredbyhbase.html Powered By HBase]&lt;/ref&gt; including [[Facebook]]'s Messaging Platform.&lt;ref name=&quot;the-underlying-technology-of-messages&quot;&gt;[http://www.facebook.com/notes/facebook-engineering/the-underlying-technology-of-messages/454991608919 The Underlying Technology of Messages]&lt;/ref&gt;&lt;ref name=&quot;theregister&quot;&gt;[http://www.theregister.co.uk/2010/12/17/facebook_messages_tech/ Facebook: Why our 'next-gen' comms ditched MySQL] Retrieved: 17 December 2010&lt;/ref&gt; Unlike relational and traditional databases, HBase does not support SQL scripting instead written in Java employing similarity with MapReduce application.&lt;ref name=hbase1 /&gt;


In the parlance of Eric Brewer’s [[CAP Theorem]], HBase is a CP type system.

==History==
Apache HBase began as a project by the company [[Powerset (company)|Powerset]] out of a need to process massive amounts of data for the purposes of [[natural language search]]. It is now a top-level Apache project.

[[Facebook]] elected to implement its new messaging platform using HBase in November 2010.&lt;ref name=&quot;the-underlying-technology-of-messages&quot;&gt;&lt;/ref&gt;

== Use cases &amp; production deployments ==

=== Enterprises that use HBase ===
The following is a list of notable enterprises that have used or are using HBase:
* [[Adobe Systems|Adobe]]
* [[Amadeus IT Group]], as its main long-term storage DB.
* [[Daumkakao]]&lt;ref&gt;{{cite web|url=http://apachebigdata2015.sched.org/event/de6abfbd8f0b9e66b1c03feb2b9e2078?iframe=yes&amp;w=i:100;&amp;sidebar=yes&amp;bg=no |title=S2Graph : A Large-Scale Graph Database with HBase |author=Doyung Yoon}}&lt;/ref&gt;
* [[Facebook]] uses HBase for its messaging platform.
* [[LinkedIn]]
* [[Netflix]]&lt;ref&gt;{{cite web|url=http://apachebigdata2015.sched.org/event/2a65daf0baa4cfbc227a8cb74a9103a2?iframe=no&amp;w=i:100;&amp;sidebar=yes&amp;bg=no |title=Netflix: Integrating Spark at Petabyte Scale |author=Cheolsoo Park and Ashwin Shankar}}&lt;/ref&gt; 
* [[Sophos]], for some of their back-end systems.
* [[Spotify]] uses HBase as base for Hadoop and machine learning jobs.&lt;ref&gt;{{cite web|url=http://apachebigdata2015.sched.org/event/2a65daf0baa4cfbc227a8cb74a9103a2?iframe=no&amp;w=i:100;&amp;sidebar=yes&amp;bg=no |title=How Apache Drives Spotify's Music Recommendations |author=Josh Baer}}&lt;/ref&gt; 
* [[Tuenti]] uses HBase for its messaging platform.&lt;ref&gt;{{cite web|url=http://corporate.tuenti.com/en/dev/blog/tuenti-group-chat-simple-yet-complex |title=Tuenti Group Chat: Simple, yet complex}}&lt;/ref&gt; &lt;ref&gt;{{cite web|url=https://github.com/tuenti/asyncthrift |title=Tuenti Asyncthrift}}&lt;/ref&gt;
* [[Sears]]

==See also==
{{Portal|Free software|Java}}
* [[NoSQL]]
* [[Wide column store]]
* [[Bigtable]]
* [[Apache Cassandra]]
* [[Cask (company)]]
* [[Oracle_NoSQL_Database | Oracle NOSQL]]
* [[Hypertable]]
* [[Apache Accumulo]]
* [[MongoDB]]
* [[Project Voldemort]]
* [[Riak]]
* [[Sqoop]]
* [[Elasticsearch]]
* [[Apache Phoenix]]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
* {{cite book
| first1       = Nick 
| last1        = Dimiduk
| first2      = Amandeep 
| last2       = Khurana 
| date        = 28 November 2012
| title       = HBase in Action
| publisher   = [[Manning Publications]]
| edition     = 1st
| page        = 350 
| isbn        = 978-1617290527
| url         = &lt;!-- http://www.manning.com/dimidukkhurana/ --&gt;
}}
* {{cite book
| first1      = Lars 
| last1       = George
| date        = 20 September 2011
| title       = HBase: The Definitive Guide
| publisher   = [[O'Reilly Media]]
| edition     = 1st
| page        = 556
| isbn        = 978-1449396107
| url         = http://shop.oreilly.com/product/0636920014348.do
}}
* {{cite book
| first       = Yifeng 
| last        = Jiang 
| date        = 16 August 2012
| title       = HBase Administration Cookbook 
| publisher   = [[Packt Publishing]]
| edition     = 1st
| page        = 332
| isbn        = 978-1849517140
| url         = http://www.packtpub.com/hbase-administration-for-optimum-database-performance-cookbook/book
}}
{{Refend}}

==External links==
*[http://hbase.apache.org/ Official Apache HBase homepage]
*[http://hadoop.apache.org/ Official Apache Hadoop homepage]
*[http://hbasecon.com/ HBaseCon: Official community conference]
*[http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable Understanding HBase]
*[http://www.networkworld.com/news/tech/2012/102212-nosql-263595.html A vendor-independent comparison of NoSQL databases: Cassandra, HBase, MongoDB, Riak] (NetworkWorld)
*[http://www.dynamicyield.com/2014/06/apache-hbase-for-the-win/ Is HBase considered harmful?]
{{Apache}}

{{DEFAULTSORT:Hbase}}
[[Category:BigTable implementations]]
[[Category:Hadoop]]
[[Category:Free database management systems]]
[[Category:Structured storage]]</text>
      <sha1>cglhqayzrc77qgjqzxmhqpe2c90c43i</sha1>
    </revision>
  </page>
  <page>
    <title>MapR</title>
    <ns>0</ns>
    <id>31958369</id>
    <revision>
      <id>720598621</id>
      <parentid>713450058</parentid>
      <timestamp>2016-05-16T21:13:47Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>/* Partners */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8351">{{Infobox company
| name             = MapR
| logo             = MapR_Company_Logo.png
| logo_caption     = MapR Company Logo
| industry         = Data Services
| location_city    = San Jose, CA
| location_country = United States of America
| locations        = 10
| area_served      = United States, United Kingdom, France, Germany, Sweden, Japan, Singapore, Korea, Australia, Singapore
| key_people       = 
| products         = Hadoop technology
| production       = 
| services         = 
| revenue          = 
| operating_income = 
| net_income       = 
| homepage         = http://www.mapr.com/
| footnotes        = 
| intl             = 
| bodystyle        = 
}}

'''MapR''' is a [[San Jose, California]]-based enterprise software company that develops and sells [[Apache Hadoop]]-derived software.  The company contributes to [[Apache Hadoop]] projects like [[HBase]], [[Pig (programming language)]], [[Apache Hive]], and [[Apache ZooKeeper]].&lt;ref name=&quot;contrib&quot;&gt;{{cite web|url=http://gigaom.com/cloud/why-mapr-is-right-to-give-back-to-apache-hadoop/|title=Why MapR is Right to Give Back to Apache Hadoop|accessdate=1 June 2011}}&lt;/ref&gt;  MapR's [[Apache Hadoop]] distribution claims to provide full data protection, no single points of failure, improved performance, and dramatic ease of use advantages. MapR entered a technology licensing agreement with [[EMC Corporation]] on 25 May 2011, supporting an EMC-specific distribution of [[Apache Hadoop]].&lt;ref name=&quot;emchadoop&quot;&gt;{{cite web|url=http://msoftnews.com/google/startup-mapr-underpins-emc%E2%80%99s-hadoop-effort/|title=Startup MapR Underpins EMC’s Hadoop Effort|accessdate=1 June 2011}}&lt;/ref&gt;  MapR was selected by Amazon to provide an upgraded version of Amazon's Elastic Map Reduce (EMR) service.&lt;ref name=&quot;emr-mapr&quot;&gt;{{cite web|url=http://aws.amazon.com/elasticmapreduce/mapr/|title=Amazon EMR with the MapR Distribution for Hadoop|accessdate=25 June 2011}}&lt;/ref&gt;&lt;ref name=&quot;emr-announce&quot;&gt;{{cite web|url=http://gigaom.com/cloud/amazon-taps-mapr-for-high-powered-elastic-mapreduce/|title=Amazon Taps MapR for High Powered Elastic Map Reduce|accessdate=25 June 2011}}&lt;/ref&gt;  MapR has also been selected by Google as a technology partner.&lt;ref&gt;{{cite web|title=MapR Technologies Joins Google Cloud Platform Partner Program|url=http://www.reuters.com/article/2012/07/24/idUS189111+24-Jul-2012+BW20120724|publisher=Reuters|accessdate=9 May 2013}}&lt;/ref&gt;  MapR was able to break the minute sort speed record on Google's compute platform.&lt;ref&gt;{{cite web|last=Metz|first=Cade|title=Google Teams With Prodigal Son to Bust Data Sort Record|url=http://www.wired.com/wiredenterprise/2013/02/google-mapr-data-sort-record/|publisher=Wired|accessdate=9 May 2013}}&lt;/ref&gt;

MapR provides three versions of their product known as M3,  M5 and M7.  M3 is a free version of the M5 product with degraded availability features.  M7 is like M5, but adds a purpose built rewrite of HBase that implements the HBase API directly in the file-system layer.

In 2016, MapR launched an enhanced and integrated product called a  &quot;Converged Data Platform&quot; which includes the MapR Streams, a pub-sub system, MapRDB, a complete no-sql document store and all the Hadoop ecosystem components integrated on the same distributed filesystem.

MapR is privately held with original funding of $9 million from [[Lightspeed Venture Partners]] and [[New Enterprise Associates]] since 2009. Key MapR executives come from [[Google]], [[Lightspeed Venture Partners]], [[Informatica]], [[EMC Corporation]] and [[Veoh]].  MapR had an additional round of funding led by [[Redpoint]] in August, 2011.&lt;ref name=&quot;redpoint&quot;&gt;{{cite web|url=http://www.cnbc.com/id/44325254/MapR_Technologies_Secures_20_Million_in_Funding_Investment_to_Fuel_Rapid_Adoption_for_Big_Data_Analytics_and_Hadoop|title=MapR Technologies Secures 20 million in Funding|accessdate=19 Sep 2011}}&lt;/ref&gt;&lt;ref name=&quot;nyt-redpoint&quot;&gt;{{cite web|url=http://www.nytimes.com/external/venturebeat/2011/08/30/30venturebeat-mapr-makes-friends-of-hadoop-and-the-enterpr-34892.html|title=MapR Makes Friends of Hadoop|accessdate=19 Sep 2011}}&lt;/ref&gt;  A C round was led by [[Mayfield Fund]] that also included Greenspring Associates as an investor.&lt;ref&gt;{{cite web|last=Hesseldahl|first=Arik|title=MapR Lands $30 Million Series C Led by Mayfield Fund.|url=http://allthingsd.com/20130318/mapr-lands-30-million-series-c-led-by-mayfield-fund/|publisher=All Things D|accessdate=9 May 2013}}&lt;/ref&gt;  In June 2014, MapR closed a $110 million financing round that was led by [[Google Capital]].  [[Qualcomm Ventures]] also participated, along with existing investors [[Lightspeed Venture Partners]], [[Mayfield Fund]], [[New Enterprise Associates]] and [[Redpoint Ventures]].&lt;ref&gt;{{cite web|last1=Hesseldahl|first1=Arik|title=MapR Raises $110 Million in Round Led by Google Capital|url=http://recode.net/2014/06/30/mapr-raises-110-million-in-round-led-by-google-capital/|website=http://recode.net|publisher=Revere Digital LLC|accessdate=2 July 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Gabrielle|first1=Karol|title=Google Capital Leads $110M Round in Big-Data Startup MapR|url=http://www.foxbusiness.com/technology/2014/06/30/google-capital-leads-110m-round-in-big-data-startup-mapr/|website=http://www.foxbusiness.com/|publisher=Fox News Network|accessdate=2 July 2014}}&lt;/ref&gt;

==MapR Converged Data Platform==
The Converged Data Platform was released in early 2016 as version 5.1.0 and includes the following key components:
* MapR Filesystem Implements the HDFS API with a native C distributed read-write file system. 
* MapR-DB Implements the HBase API but leverages all the advantages of the MapR-FS to make it more robust, faster and easier to manage.
* MapR-Streams Implements the Kafka API, but again leverages the MapR-FS to offer increased performance and reliability of streams.

==Partners==
* Amazon offers MapR's M3 and M5 editions as premium options on the Elastic MapReduce service next to a set of Apache Hadoop versions.&lt;ref name=&quot;emr-announce&quot; /&gt;
* Big Data Partnership partners with MapR as first Certified Training &amp; SI partner in EMEA&lt;ref&gt;{{cite web|title=MapR Announces Certified Training with Big Data Partnership in EMEA |url=http://www.mapr.com/press-release/mapr-announces-certified-training-with-big-data-partnership-in-emea}}&lt;/ref&gt;
* Google partnered with MapR in the launch of the [[Google Compute Engine]].&lt;ref name=&quot;google-mapr&quot;&gt;{{cite web|title=Google Compute Engine Leverages Third Party Support |url=http://www.informationweek.com/news/hardware/virtual/240003042}}&lt;/ref&gt;
* [[Cisco Systems]] announced support of MapR software on the [[Cisco Unified Computing System]] platform.&lt;ref&gt;{{cite web|title=Cisco UCS with MapR:  Delivering Advanced Performance  for Hadoop Workloads|url=http://www.cisco.com/en/US/solutions/collateral/ns340/ns517/ns224/ns944/le_38203_sb_mapr_130208.pdf|publisher=Cisco|accessdate=9 May 2013}}&lt;/ref&gt;
* [[Informatica Corporation]]
* [[Impetus Technologies]]
* [[Talend]]
* [[Teradata Corporation]]
* [[Tata Consultancy Services]]&lt;ref&gt;http://www.business-standard.com/article/companies/tcs-partners-with-mapr-technologies-to-make-new-big-data-offerings-114072201086_1.html&lt;/ref&gt;
* [[Databricks]]&lt;ref&gt;{{cite web|url=https://www.mapr.com/blog/databricks-and-mapr-partner-provide-enterprise-support-spark|title = Databricks and MapR Partner to Provide Enterprise Support for Spark|last = Tavakoli-Shiraji|first = Ashoka|date = April 1, 2014|accessdate = September 28, 2014|publisher = MapR}}&lt;/ref&gt;

==See also==
* [[Apache Accumulo|Accumulo]]
* [[Apache Software Foundation]]
* [[Big data]]
* [[BigTable]]
* [[Cloud computing]]
* [[Cloud infrastructure]]
* [[Database-centric architecture]]
* [[Datastructure]]
* [[Hadoop]]
* [[MapReduce]]
* [[HBase]]
* [[RainStor]]

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://www.mapr.com MapR Homepage]

{{DEFAULTSORT:Mapr}}
&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Cloud infrastructure]]
[[Category:Distributed file systems]]
[[Category:Hadoop]]
[[Category:Companies based in San Jose, California]]
[[Category:Big data companies]]</text>
      <sha1>td2slfkf166jm2wj3sbbn7naz3bot8k</sha1>
    </revision>
  </page>
  <page>
    <title>Hortonworks</title>
    <ns>0</ns>
    <id>35142247</id>
    <revision>
      <id>723503855</id>
      <parentid>723361791</parentid>
      <timestamp>2016-06-03T12:32:40Z</timestamp>
      <contributor>
        <username>Naltman94</username>
        <id>28472652</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10409">{{Infobox company
| name             = Hortonworks
| logo             = File:Hortonworks logo.gif
| type             = [[Public company|Public]]
| traded_as        = {{NASDAQ|HDP}}
| foundation       = {{start date and age|2011}}
| location_city    = [[Santa Clara, California]]
| location_country = United States
| num_employees    = ~850 {{small|(2015)}}&lt;ref&gt;http://hortonworks.com/about-us/quick-facts/&lt;/ref&gt;
| key_people       = &lt;!-- Eric Baldeschweiler, Founder &amp; CTO, Rob Bearden, CEO only if notable and cited --&gt;
| industry         = [[Software|Computer software]]
| products         = Hortonworks Data Platform
| services         = 
| homepage         = {{url|http://hortonworks.com/|Hortonworks.com}}
}}

'''Hortonworks''' is a business computer software company based in [[Santa Clara, California]]. The company focuses on the development and support of [[Apache Hadoop]], a framework that allows for the [[distributed processing]] of large data sets across clusters of computers.

==History==

Hortonworks was formed in June 2011 funded by $23 million from  [[Yahoo!]] and [[Benchmark Capital]] as an independent company.&lt;ref name=&quot;info&quot;&gt;{{Cite news |title= Hadoop Big Data Startup Spins Out Of Yahoo |work= Information Week |date= June 29, 2011 |author= Charles Babcock |url= http://www.informationweek.com/news/development/database/231000658 &lt;!-- pay site --&gt; |deadurl=no |archiveurl= http://web.archive.org/web/20110704055947/http://www.informationweek.com/news/development/database/231000658 |archivedate= July 4, 2011 |accessdate= October 14, 2013 }}&lt;/ref&gt;  The company employs contributors to the [[open source software]] project [[Apache Hadoop]].&lt;ref&gt;{{cite news |url= http://www.cnbc.com/id/47123495 |title= Big-data investors look for the next Splunk |work= [[CNBC]] |accessdate=24 April 2012}}&lt;/ref&gt; 

Eric Baldeschweiler was chief executive, and Rob Bearden chief operating officer, formerly from [[SpringSource]].&lt;ref name=&quot;info&quot; /&gt;
Additional investors included a $25 million round led by [[Index Ventures]] in November 2011.&lt;ref&gt;{{Cite news |title= Everything you ever wanted to know about Yahoo’s Hadoop spinoff Hortonworks |author= Derrick Harris |work= Giga Om |date= August 20, 2013 |url= http://gigaom.com/2013/08/20/everything-you-ever-wanted-to-know-about-yahoo-spinoff-hortonworks/ |accessdate= October 14, 2013 }}&lt;/ref&gt;

Hortonworks is a sponsor of the [[Apache Software Foundation]].&lt;ref&gt;[http://www.apache.org/foundation/thanks.html Sponsors], Apache Software Foundation&lt;/ref&gt;

[[Forrester Research]] named Hortonworks “a technology leader and ecosystem builder for the entire Hadoop industry.”&lt;ref&gt;[http://hortonworks.com/press-releases/hortonworks-recognized-as-a-leader-in-the-enterprise-hadoop-solutions-industry-by-independent-research-firm/ Hortonworks Recognized as a Leader in the Enterprise Hadoop Solutions Industry by Independent Research Firm], Hortonworks Website, 2014-6-30&lt;/ref&gt;

Hortonworks' product named Hortonworks Data Platform (HDP) includes Apache Hadoop and is used for storing, processing, and analyzing large volumes of data. The platform is designed to deal with data from many sources and formats.
The platform includes various [[Apache Hadoop]] projects including the Hadoop Distributed File System, [[MapReduce]], Pig, Hive, [[HBase]] and Zookeeper and additional components.&lt;ref&gt;{{Cite news |title= HortonWorks Hones a Hadoop Distribution |work= PC World |date= November 1, 2011 |author= Joab Jackson |url= http://www.pcworld.com/article/242916/hortonworks_hones_a_hadoop_distribution.html |accessdate= October 14, 2013 }}&lt;/ref&gt;

In October 2011 Hortonworks announced [[Microsoft]] would collaborate on a Hadoop distribution for [[Microsoft Azure]] and [[Windows Server]].&lt;ref&gt;{{Cite news |title= Microsoft climbs onto Hadoop bandwagon  |date= October 12, 2011 |author= Jaikumar Vijayan |work= Computer World |url= http://www.computerworld.com/s/article/9220779/Microsoft_climbs_onto_Hadoop_bandwagon |accessdate= October 14, 2013 }}&lt;/ref&gt;
On February 25, 2013, Hortonworks announced availability of a beta version of the Hortonworks Data Platform for Windows.{{Citation needed |date= October 2013}}

In November 2011 it announced HParser software from [[Informatica]] would be available for free download by its customers.&lt;ref&gt;{{Cite news |title= Informatica, Hortonworks Team Up to Release Free Data Parser for Hadoop |work= e Week |author= Chris Preimesberger  |date= November 2, 2011 |url= http://www.eweek.com/c/a/Enterprise-Applications/Informatica-Hortonworks-Team-Up-to-Release-Free-Data-Parser-for-Hadoop-788698 |accessdate= October 14, 2013 }}&lt;/ref&gt;

In February 2012 [[Teradata]] announced an alliance.&lt;ref&gt;{{Cite news |title= Teradata and Hortonworks Join Forces for a Big Data Boost |date= February 21, 2012 |author= Quentin Hardy |work= Bits blog |publisher= The New York Times |url= http://bits.blogs.nytimes.com/2012/02/21/teradata-and-hortonworks-join-forces-for-a-big-data-boost |accessdate= October 14, 2013 }}&lt;/ref&gt;
In October 2012 Teradata's [[Aster Data Systems]] division announced an appliance supporting Hortonworks' distribution,&lt;ref&gt;{{Cite news |title= It's happening: Hadoop and SQL worlds are converging |author= Tony Baer |date= October 26, 2012 |work= ZDNet |url=http://www.zdnet.com/its-happening-hadoop-and-sql-worlds-are-converging-7000006455/ |accessdate= October 14, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{Cite news |title= Teradata Big Analytics Appliance Enables New Business Insights on All Enterprise Data |date= October 17, 2012 |publisher= Teradata Aster  |work= Press release  |url= http://www.asterdata.com/news/teradata-aster-big-analytics-appliance-breaks-down-barriers.php |accessdate= October 14, 2013 }}&lt;/ref&gt;
and [[Impetus Technologies]] announced a partnership.&lt;ref&gt;{{Cite news |title= Impetus and Hortonworks Strategic Partnership |work= Press release |date=  October 2, 2012  |author= Pletus |url= http://finance.yahoo.com/news/impetus-hortonworks-strategic-partnership-103000189.html |accessdate= October 14, 2013 }}&lt;/ref&gt;

In June 2013 Hortonworks raised another $50 million in financing, from  previous investors and adding [[Tenaya Capital]] and Dragoneer Investment Group.&lt;ref&gt;{{Cite news |title= Hortonworks Raises $50M For Expansion And Development In Growing Hadoop Oriented Data Analytics Market |author= Alex Williams |date= June 25, 2013 |work= Tech Crunch |url= http://techcrunch.com/2013/06/25/hortonworks-raises-50m-for-expansion-and-development-in-growing-hadoop-oriented-data-analytics-market/ |accessdate= October 14, 2013 }}&lt;/ref&gt;

In September 2013 [[SAP AG]] announced it would resell the Hortonworks distribution (as well as one from Intel).&lt;ref&gt;{{Cite news |title= SAP looks to boost 'big data' position with Hadoop deals, new apps |quote= SAP agrees to resell Hadoop distributions from Intel and Hortonworks; unveils specialized use case apps |date= September 11, 2013 |author= Chris Kanaracus |work= Computer World |url= http://www.computerworld.com/s/article/9242337/SAP_looks_to_boost_big_data_position_with_Hadoop_deals_new_apps |accessdate= October 14, 2013 }}&lt;/ref&gt;

Hortonworks hosts the Hadoop Summit community event, along with Yahoo!.&lt;ref&gt;{{Cite web |title= Sponsors |work= Hadoop Summit 2013 web site |deadurl=no |url= http://hadoopsummit.org/san-jose/sponsors/ |archiveurl= http://web.archive.org/web/20130430202257/http://hadoopsummit.org/san-jose/sponsors/ |archivedate= April 30, 2013 |accessdate= October 14, 2013 }}&lt;/ref&gt;

In December 2014, Hortonworks went public with 6,250,000 shares listed in NASDAQ.&lt;ref&gt;{{cite web|title=Hortonworks IPO|url=http://www.nasdaq.com/markets/ipos/company/hortonworks-inc-949081-76930|publisher=NASDAQ|accessdate=16 February 2015}}&lt;/ref&gt;

==Partnerships==
Hortonworks partners with a variety of software-related companies, including [[BMC Software]] for business service management and automation,&lt;ref&gt;http://www.infoworld.com/t/hadoop/hortonworks-bmc-team-support-hadoop-automation-242857&lt;/ref&gt;  
[[Attunity]]&lt;ref&gt;http://www.complianceweek.com/attunity-hortonworks-partner-to-simplify-big-data-integration/article/230615/&lt;/ref&gt; and [[Cleo (company)|Cleo]] for data integration,&lt;ref&gt;http://hortonworks.com/blog/secure-reliable-hadoop-data-transfer-option-cleo-mft/&lt;/ref&gt; and [[SAP AG|SAP]] and [[VMWare]] for cloud, database and other virtualization infrastructure.&lt;ref&gt;http://gigaom.com/2012/06/12/hortonworks-teams-with-vmware-to-keep-hadoop-running/&lt;/ref&gt;  As part of its efforts to make [[Hadoop]] easier to use, in 2015 Hortonworks started partnering with [[ManTech Commercial Services]] and B23 to develop OpenSOC; ManTech brought cybersecurity expertise and B23 brought expertise in big data analytics.&lt;ref&gt;William Terdoslavich for InformationWeek. October 1, 2015 [http://www.informationweek.com/cloud/hortonworks-aims-to-make-hadoop-easier-to-use/d/d-id/1322438Hortonworks Aims To Make Hadoop Easier To Use]&lt;/ref&gt;

==Acquisitions==
In August, 2015, Hortonworks announced it has signed a definitive agreement to acquire Onyara, Inc., the creator of and key contributor to Apache NiFi, a top-level open source project. Apache NiFi was made available through the NSA Technology Transfer Program in the fall of 2014. Over the past eight years, Onyara’s engineers were the key contributors to the U.S. government software project that evolved into Apache NiFi. In July 2015, NiFi became a Top-Level Project, signifying that its community and technology have been successfully governed under the Apache Software Foundation.
&lt;ref&gt;http://techcrunch.com/2015/08/25/hortonworks-acquires-onyara-early-startup-with-roots-in-nsa/&lt;/ref&gt;
&lt;ref&gt;http://fortune.com/2015/08/26/hortonworks-buys-onyara/&lt;/ref&gt;
&lt;ref&gt;http://hortonworks.com/press-releases/hortonworks-to-acquire-onyara-to-turn-internet-of-anything-data-into-actionable-insights/&lt;/ref&gt;

In May 2014, Hortonworks acquired XA Secure, a data security company, to add a secure layer to its Hadoop Data Platform.&lt;ref&gt;https://www.datacenterdynamics.com/focus/archive/2014/05/hortonworks-acquires-xa-secure&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* {{Official website }}

[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Companies based in Sunnyvale, California]]
[[Category:Companies listed on NASDAQ]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation]]
[[Category:Companies established in 2011]]
[[Category:Big data companies]]</text>
      <sha1>pqf37gs0n7adv8usqsvr91kj9jjdj5s</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Accumulo</title>
    <ns>0</ns>
    <id>34571412</id>
    <revision>
      <id>721663010</id>
      <parentid>721602416</parentid>
      <timestamp>2016-05-23T09:03:36Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (12016)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5717">{{Infobox software
| name                   = Apache Accumulo
| logo                   = [[File:Accumulo logo.png|220px]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.6.5
| latest release date    = {{release date|2016|02|17}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://accumulo.apache.org}}
}}
'''Apache Accumulo''' is a computer software project that developed a sorted, distributed key/value store based on the [[BigTable]] technology from [[Google]].&lt;ref&gt;[http://accumulo.apache.org/ Apache Accumulo]. Accumulo.apache.org. Retrieved on 2013-09-18.&lt;/ref&gt;  It is a system built on top of [[Apache Hadoop]], [[Apache ZooKeeper]], and [[Apache Thrift]]. Written in [[Java (programming language)|Java]], Accumulo has cell-level [[Apache Accumulo#Cell-Level Security|access label]]s and [[server-side programming]] mechanisms. Accumulo is the third most popular [[NoSQL]] [[wide column store]] according to the DB-Engines ranking, behind [[Apache Cassandra]] and [[Hbase]] as of 2015.&lt;ref&gt;[http://db-engines.com/en/ranking/wide+column+store DB-Engines Ranking - popularity ranking of wide column stores]. Db-engines.com. Retrieved on 2013-09-18. [http://web.archive.org/web/20151206180549/http://db-engines.com/en/ranking/wide+column+store Archived 2015-12-06].&lt;/ref&gt;

==History==
Accumulo was created in 2008 by the US [[National Security Agency]] and contributed to the [[Apache Foundation]] as an incubator project in September 2011.&lt;ref name=&quot;informationweek1&quot;&gt;[http://www.informationweek.com/news/government/enterprise-apps/231600835 NSA Submits Open Source, Secure Database To Apache - Government]. Informationweek.com (2011-09-06). Retrieved on 2013-09-18.&lt;/ref&gt;

On March 21, 2012, Accumulo graduated from incubation at Apache, making it a top-level project.&lt;ref&gt;[http://incubator.apache.org/projects/accumulo.html Accumulo Incubation Status - Apache Incubator]. Incubator.apache.org. Retrieved on 2013-09-18.&lt;/ref&gt;

===Controversy===
In June 2012 the US [[Senate Armed Services Committee]] (SASC) released the Draft 2012 Department of Defense (DoD) Authorization Bill, which included references to Apache Accumulo.  In the draft bill SASC required DoD to evaluate whether Apache Accumulo could achieve commercial viability before implementing it throughout DoD.&lt;ref&gt;Metz, Cade. (2012-12-19) [http://www.wired.com/wiredenterprise/2012/07/nsa-accumulo-google-bigtable/ NSA Mimics Google, Pisses Off Senate | Wired Enterprise]. Wired.com. Retrieved on 2013-09-18.&lt;/ref&gt; Specific criteria were not included in the draft language, but the establishment of commercial entities supporting Apache Accumulo could be considered a success factor.&lt;ref&gt;[http://www.fiercegovernmentit.com/story/sasc-accumulo-language-pro-open-source-say-proponents/2012-06-14 SASC Accumulo language pro-open source, say proponents]. FierceGovernmentIT (2012-06-14). Retrieved on 2013-09-18.&lt;/ref&gt;

==Main features==

===Cell-level security===
Apache Accumulo extends the [[BigTable#Design|BigTable data model]], adding a new element to the key called [http://accumulo.apache.org/1.4/user_manual/Security.html Column Visibility]. This element stores a logical combination of security labels that must be satisfied at query time in order for the key and value to be returned as part of a user request. This allows data of varying security requirements to be stored in the same table, and allows users to see only those keys and values for which they are authorized.&lt;ref name=&quot;informationweek1&quot;/&gt;

===Server-side programming===
In addition to Cell-Level Security, Apache Accumulo provides a server-side programming mechanism called Iterators that allows users to perform additional processing at the Tablet Server. The range of operations that can be applied is equivalent to those that can be implemented within a [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/mapreduce-osdi04.pdf MapReduce Combiner function], which produces an aggregate value for several key-value pairs.

==Papers==
* 2011 [http://www.pdl.cmu.edu/PDL-FTP/Storage/socc2011.pdf YCSB++: Benchmarking and Performance Debugging Advanced Features in Scalable Table Stores] by Carnegie Mellon University and the National Security Agency.
* 2012 [http://www.mit.edu/~kepner/pubs/ByunKepner_2012_BigData_Paper.pdf Driving Big Data With Big Compute] by MIT Lincoln Laboratory.
* 2013 [http://www.mit.edu/~kepner/pubs/D4Mschema_HPEC2013_Paper.pdf  D4M 2.0 Schema:A General Purpose High Performance Schema for the Accumulo Database] by MIT Lincoln Laboratory.
* 2013 [http://geomesa.github.io/assets/outreach/SpatioTemporalIndexing_IEEEcopyright.pdf Spatio-temporal Indexing in Non-relational Distributed Databases] by CCRi

==See also==
{{Portal|Java|Free software}}
* [[BigTable]]
* [[Apache Cassandra]]
* [[Column-oriented DBMS]]
* [[Hypertable]]
* [[HBase]]
* [[Hadoop]]
* [[sqrrl]]

==References==
{{Reflist}}

==External links==
* {{Official website|https://accumulo.apache.org}}
* [http://www.reddit.com/r/accumulo Accumulo topic on reddit]
* [http://qnalist.com/g/accumulo Accumulo mailing list archives]

{{Apache}}

[[Category:Apache Software Foundation]]
[[Category:Apache Software Foundation projects|Accumulo]]
[[Category:BigTable implementations]]
[[Category:Distributed computing architecture]]
[[Category:Distributed data stores]]
[[Category:Free database management systems]]
[[Category:Hadoop]]
[[Category:NoSQL products]]</text>
      <sha1>4h31f61v1cczhn93z8lqp46hwfta5in</sha1>
    </revision>
  </page>
  <page>
    <title>Pig (programming tool)</title>
    <ns>0</ns>
    <id>29417433</id>
    <revision>
      <id>723086369</id>
      <parentid>723085287</parentid>
      <timestamp>2016-05-31T22:32:01Z</timestamp>
      <contributor>
        <username>Alanfgates</username>
        <id>8063281</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6230">'''Apache Pig'''&lt;ref name=&quot;mainpage&quot;&gt;{{cite web |url=http://pig.apache.org/|title=Hadoop: Apache Pig|accessdate=Sep 2, 2011}}&lt;/ref&gt;
is a high-level platform for creating programs that run on [[Hadoop|Apache Hadoop]]. The language for this platform is called '''Pig Latin'''.&lt;ref name=&quot;mainpage&quot;/&gt;  Pig can execute its Hadoop jobs in MapReduce, Apache Tez, or [[Apache Spark]].  Pig Latin abstracts the programming from the [[Java (programming language)|Java]] MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of [[SQL]] for [[RDBMS|RDBMSs]]. Pig Latin can be extended using [[User-defined function|User Defined Functions]] (UDFs) which the user can write in Java, [[Python (programming language)|Python]], [[JavaScript]], [[Ruby (programming language)|Ruby]] or [[Groovy (programming language)|Groovy]]&lt;ref&gt;{{cite web|url= http://pig.apache.org/docs/r0.11.1/udf.html|title=Pig user defined functions|accessdate=May 3, 2013}}&lt;/ref&gt; and then call directly from the language.

Apache Pig was originally&lt;ref&gt;{{cite web |url=https://developer.yahoo.com/blogs/hadoop/pig-road-efficient-high-level-language-hadoop-413.html|title=Yahoo Blog:Pig – The Road to an Efficient High-level language for Hadoop|accessdate=May 23, 2015}}&lt;/ref&gt; developed at [[Yahoo]] Research around 2006 for researchers to have an ad-hoc way of creating and executing MapReduce jobs on very large data sets. In 2007,&lt;ref&gt;{{cite web |url=https://developer.yahoo.com/blogs/hadoop/pig-incubation-apache-software-foundation-393.html|title=Pig into Incubation at the Apache Software Foundation|accessdate=May 23, 2015}}&lt;/ref&gt; it was moved into the [[Apache Software Foundation]].&lt;ref&gt;{{cite web |url=http://apache.org/|title=The Apache Software Foundation|accessdate=Nov 1, 2010}}&lt;/ref&gt;

==Example==
Below is an example of a &quot;[[word count|Word Count]]&quot; program in Pig Latin:

&lt;syntaxhighlight lang=pig&gt;
 input_lines = LOAD '/tmp/my-copy-of-all-pages-on-internet' AS (line:chararray);
 
 -- Extract words from each line and put them into a pig bag
 -- datatype, then flatten the bag to get one word on each row
 words = FOREACH input_lines GENERATE FLATTEN(TOKENIZE(line)) AS word;
 
 -- filter out any words that are just white spaces
 filtered_words = FILTER words BY word MATCHES '\\w+';
 
 -- create a group for each word
 word_groups = GROUP filtered_words BY word;
 
 -- count the entries in each group
 word_count = FOREACH word_groups GENERATE COUNT(filtered_words) AS count, group AS word;
 
 -- order the records by count
 ordered_word_count = ORDER word_count BY count DESC;
 STORE ordered_word_count INTO '/tmp/number-of-words-on-internet';
&lt;/syntaxhighlight&gt;

The above program will generate parallel executable tasks which can be distributed across multiple machines in a Hadoop cluster to count the number of words in a dataset such as all the webpages on the internet.

==Pig vs SQL==
In comparison to SQL, Pig
# uses [[lazy evaluation]], 
# uses [[extract, transform, load]] (ETL), 
# is able to store data at any point during a [[Pipeline (software)|pipeline]], 
# declares [[execution plan]]s, 
# supports pipeline splits, thus allowing workflows to proceed along [[directed acyclic graph|DAG]]s instead of strictly sequential pipelines. 
On the other hand, it has been argued [[DBMS]]s are substantially faster than the MapReduce system once the data is loaded, but that loading the data takes considerably longer in the database systems. It has also been argued [[Relational database management system|RDBMS]]s offer out of the box support for column-storage, working with compressed data, indexes for efficient random data access, and transaction-level fault tolerance.&lt;ref&gt;{{cite web |url=http://database.cs.brown.edu/papers/stonebraker-cacm2010.pdf|title=Communications of the ACM: MapReduce and Parallel DBMSs: Friends or Foes?|format=PDF|accessdate=May 23, 2015}}&lt;/ref&gt;

Pig Latin is [[Procedural programming|procedural]] and fits very naturally in the pipeline paradigm while SQL is instead [[Declarative programming|declarative]]. In SQL users can specify that data from two tables must be joined, but not what join implementation to use (You can specify the implementation of JOIN in SQL, thus &quot;... for many SQL applications the query writer may not have enough knowledge of the data or enough expertise to specify an appropriate join algorithm.&quot;). Pig Latin allows users to specify an implementation or aspects of an implementation to be used in executing a script in several ways.&lt;ref name = ypgd /&gt; In effect, Pig Latin programming is similar to specifying a query execution plan, making it easier for programmers to explicitly control the flow of their data processing task.&lt;ref&gt;{{cite web |url=http://infolab.stanford.edu/~olston/publications/sigmod08.pdf|title=ACM SigMod 08: Pig Latin: A Not-So-Foreign Language for Data Processing|format=PDF|accessdate=May 23, 2015}}&lt;/ref&gt;

SQL is oriented around queries that produce a single result. SQL handles trees naturally, but has no built in mechanism for splitting a data processing stream and applying different operators to each sub-stream. Pig Latin script describes a [[directed acyclic graph]] (DAG) rather than a pipeline.&lt;ref name = ypgd /&gt;

Pig Latin's ability to include user code at any point in the pipeline is useful for pipeline development. If SQL is used, data must first be imported into the database, and then the cleansing and transformation process can begin.&lt;ref name = ypgd&gt;{{cite web |url=https://developer.yahoo.com/blogs/hadoop/comparing-pig-latin-sql-constructing-data-processing-pipelines-444.html|title=Yahoo Pig Development Team: Comparing Pig Latin and SQL for Constructing Data Processing Pipelines|accessdate=May 23, 2015}}&lt;/ref&gt;

==See also==
*[[Apache Hive]]
*[[Sawzall (programming language)|Sawzall]] — similar tool from Google

==References==
{{Reflist}}

==External links==
*{{Official website|https://pig.apache.org}}

{{Apache}}

[[Category:Cloud computing]]
[[Category:Query languages]]
[[Category:Data modeling languages]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Software using the Apache license]]</text>
      <sha1>n28lveunk748q7m5c0i79r0mlk4xb1m</sha1>
    </revision>
  </page>
  <page>
    <title>Sqoop</title>
    <ns>0</ns>
    <id>36956866</id>
    <revision>
      <id>718655831</id>
      <parentid>705360976</parentid>
      <timestamp>2016-05-04T20:27:53Z</timestamp>
      <contributor>
        <ip>73.216.204.212</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6506">{{Infobox software
| name                   = Apache Sqoop
| logo                   = 
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.4.6
| latest release date    = {{release date|2015|05|11}}
| latest preview version = 
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Data management]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://sqoop.apache.org}}
}}

'''Sqoop''' is a [[command-line interface]] application for transferring data between [[relational database]]s and [[Hadoop]].&lt;ref name=&quot;mainpage&quot;&gt;{{cite web |url=https://sqoop.apache.org|title=Hadoop: Apache Sqoop|accessdate=Sep 8, 2012}}&lt;/ref&gt; It supports incremental loads of a single table or a free form [[SQL query]] as well as saved jobs which can be run multiple times to import updates made to a database since the last import. Imports can also be used to populate tables in [[Apache Hive|Hive]] or [[HBase]].&lt;ref&gt;{{cite web |url=https://blogs.apache.org/sqoop/entry/apache_sqoop_overview|title=Apache Sqoop - Overview|accessdate=Sep 8, 2012}}&lt;/ref&gt; Exports can be used to put data from Hadoop into a relational database. Sqoop got the name from sql+hadoop.
Sqoop became a top-level [[Apache Software Foundation|Apache]] project in March 2012.&lt;ref&gt;{{cite web |url=https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator|title=Apache Sqoop Graduates from Incubator|accessdate=Sep 8, 2012}}&lt;/ref&gt;

[[Pentaho]] provides [[open source]] Sqoop based connector steps, ''Sqoop Import''&lt;ref name=&quot;2015-12-10_PSI&quot; /&gt; and ''Sqoop Export'',&lt;ref name=&quot;2015-12-10_PSE&quot;/&gt; in their [[Extract, transform, load|ETL]] suite [[Pentaho Data Integration]] since version 4.5 of the software.&lt;ref name=&quot;2012-07-27_dbta&quot; /&gt; [[Microsoft]] uses a Sqoop-based connector to help transfer data from [[Microsoft SQL Server]] databases to Hadoop.&lt;ref&gt;{{cite web |url=https://www.microsoft.com/en-us/download/details.aspx?id=27584|title=Microsoft SQL Server Connector for Apache Hadoop|accessdate=Sep 8, 2012}}&lt;/ref&gt;
[[Couchbase, Inc.]] also provides a [[Couchbase Server]]-Hadoop connector by means of Sqoop.&lt;ref&gt;{{cite web |url=http://www.couchbase.com/develop/connectors/hadoop|title=Couchbase Hadoop Connector|accessdate=Sep 8, 2012}}&lt;/ref&gt;

In 2015 [[Ralph Kimball]] described '''Sqoop''' as follow under the heading ''The Future of [[Extract, transform, load|ETL]]'':&lt;ref name=&quot;Kimball_2015-12-01_KG&quot;&gt;{{cite web  | url = http://www.kimballgroup.com/2015/12/design-tip-180-the-future-is-bright/   | title = Design Tip #180 The Future Is Bright  | last = Kimball  | first = Ralph  | authorlink = Ralph Kimball  | publisher = [[Kimball Group]]  | date = 2015-12-01  | accessdate = 2015-12-03   | archiveurl = https://web.archive.org/web/20151203201607/http://www.kimballgroup.com/2015/12/design-tip-180-the-future-is-bright/  | archivedate = 2015-12-03   | quote = Several big changes must take place in the ETL environment. First, the data feeds from original sources must support huge bandwidths, at least gigabytes per second. Learn about Sqoop loading data into Hadoop. If these words mean nothing to you, you have some reading to do! Start with Wikipedia. }}&lt;/ref&gt; 
{{quote|Several big changes must take place in the ETL environment. First, the data feeds from original sources must support huge bandwidths, at least gigabytes per second. Learn about Sqoop loading data into Hadoop. If these words mean nothing to you, you have some reading to do! Start with Wikipedia.}}

==See also==
*[[Apache Hive|Hive]]
*[[Apache Accumulo|Accumulo]]
*[[HBase]]

==References==
{{Reflist|refs=

 &lt;ref name=&quot;2012-07-27_dbta&quot;&gt;{{cite web
  | url = http://www.dbta.com/Editorial/News-Flashes/Big-Data-Analytics-Vendor-Pentaho-Announces-Tighter-Integration-with-Cloudera-Extends-Visual-Interface-to-Include-Hadoop-Sqoop-and-Oozie-84025.aspx
  | title = Big Data Analytics Vendor Pentaho Announces Tighter Integration with Cloudera; Extends Visual Interface to Include Hadoop Sqoop and Oozie
  | publisher = [[Database Trends and Applications]] (dbta.com)
  | date = 2012-07-27
  | accessdate =  2015-12-08
  | archiveurl = https://web.archive.org/web/20151208144234/http://www.dbta.com/Editorial/News-Flashes/Big-Data-Analytics-Vendor-Pentaho-Announces-Tighter-Integration-with-Cloudera-Extends-Visual-Interface-to-Include-Hadoop-Sqoop-and-Oozie-84025.aspx
  | archivedate = 2015-12-08
  | quote = Pentaho’s Business Analytics 4.5 is now certified on Cloudera’s latest releases, Cloudera Enterprise 4.0 and CDH4. Pentaho also announced that its visual design studio capabilities have been extended to the Sqoop and Oozie components of Hadoop.
 }}&lt;/ref&gt;

 &lt;ref name=&quot;2015-12-10_PSE&quot;&gt;{{cite web
  | url = http://wiki.pentaho.com/display/EAI/Sqoop+Export
  | title = Sqoop Export
  | publisher = [[Pentaho]]
  | date = 2015-12-10
  | accessdate =  2015-12-10
  | archiveurl = https://web.archive.org/web/20151210171525/http://wiki.pentaho.com/display/EAI/Sqoop+Export
  | archivedate = 2015-12-10
  | quote = The Sqoop Export job allows you to export data from Hadoop into an RDBMS using Apache Sqoop.
 }}&lt;/ref&gt;

 &lt;ref name=&quot;2015-12-10_PSI&quot;&gt;{{cite web
  | url = http://wiki.pentaho.com/display/EAI/Sqoop+Import
  | title = Sqoop Import
  | publisher = [[Pentaho]]
  | date = 2015-12-10
  | accessdate =  2015-12-10
  | archiveurl = https://web.archive.org/web/20151210170913/http://wiki.pentaho.com/display/EAI/Sqoop+Import
  | archivedate = 2015-12-10
  | quote = The Sqoop Import job allows you to import data from a relational database into the Hadoop Distributed File System (HDFS) using Apache Sqoop.
 }}&lt;/ref&gt;

}}

==Bibliography==
{{Refbegin}}
*{{Cite book
| first1    = Tom
| last1     = White
| title     = Hadoop: The Definitive Guide
| edition   = 2nd
| chapter   = Chapter 15: Sqoop
| publisher = [[O'Reilly Media]]
| pages      = 477–495
| isbn      = 978-1-449-38973-4
| url       = http://oreilly.com/catalog/9780596521974
}}
{{Refend}}

==External links==
*{{Official website|https://sqoop.apache.org}}
*[https://cwiki.apache.org/confluence/display/SQOOP/Home Sqoop Wiki]
*[http://qnalist.com/q/sqoop-user Sqoop Users Mailing List Archives]

{{Apache}}

[[Category:Cloud applications]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation projects]]</text>
      <sha1>8ytr0v10ekuwtmw5qhv300obqjbs40m</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Oozie</title>
    <ns>0</ns>
    <id>6728387</id>
    <revision>
      <id>723953741</id>
      <parentid>714027794</parentid>
      <timestamp>2016-06-06T07:47:09Z</timestamp>
      <contributor>
        <username>StefanPapp</username>
        <id>2062751</id>
      </contributor>
      <minor/>
      <comment>Added workflow scheduler System as category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3073">{{multiple issues|
{{notability|date=January 2013}}
{{primary sources|date=January 2013}}
}}
{{Infobox software
| name                   = 
| logo                   = &lt;!-- Image name is enough --&gt;
| logo alt               = 
| screenshot             = &lt;!-- Image name is enough --&gt;
| caption                = 
| screenshot alt         = 
| collapsible            = 
| author                 = 
| developer              = 
| released               = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| discontinued           = 
| latest release version = 
| latest release date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| status                 = 
| programming language   = 
| operating system       = 
| platform               = 
| size                   = 
| language               = 
| language count         = &lt;!-- Number only --&gt;
| language footnote      = 
| genre                  = 
| license                = 
| alexa                  = 
| website                = &lt;!-- {{URL|example.org}} --&gt;
| standard               = 
| AsOf                   = 
}}
'''Apache Oozie''' is a server-based [[workflow]] [[Scheduling (computing)|scheduling]] system to manage [[Apache Hadoop|Hadoop]] jobs.

Workflows in Oozie are defined as a collection of control flow and action [[Vertex (graph theory)|nodes]] in a [[directed acyclic graph]]. Control flow nodes define the beginning and the end of a workflow (start, end and failure nodes) as well as a mechanism to control the workflow execution path (decision, fork and join nodes). Action nodes are the mechanism by which a workflow triggers the execution of a computation/processing task. Oozie provides support for different types of actions including Hadoop [[MapReduce]], Hadoop distributed file system operations, [[Pig (programming tool)|Pig]], [[Secure Shell|SSH]], and [[email]]. Oozie can also be extended to support additional types of actions.

Oozie workflows can be parameterised using variables such as ${inputDir} within the workflow definition. When submitting a workflow job, values for the parameters must be provided. If properly parameterized (using different output directories), several identical workflow jobs can run concurrently.

Oozie is implemented as a Java [[web application]] that runs in a [[Java servlet]] container and is distributed under the [[Apache License]] 2.0.

==Editors==
* [http://gethue.com/category/oozie/ Hue editor and dashboard]
* [https://github.com/apache/oozie/tree/master/workflowgenerator Oozie workflow generator]
* [http://oep.mashin.io Oozie Eclipse plugin]&lt;ref&gt;{{cite web |url=http://www.infoq.com/articles/oozie-plugin-eclipse |title=Oozie Plugin for Eclipse |last=Mahran |first=Ahmed |date=30 October 2015 |website=www.infoq.com |publisher=InfoQ |access-date=30 October 2015}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
* {{Official website|http://oozie.apache.org/}}

[[Category:Hadoop]]
[[Category:Workflow Scheduler System]]</text>
      <sha1>9ahgj0gl0e57rhnvylw2cdxsthqqtz1</sha1>
    </revision>
  </page>
  <page>
    <title>Cloudera Impala</title>
    <ns>0</ns>
    <id>40147148</id>
    <revision>
      <id>718845654</id>
      <parentid>718845407</parentid>
      <timestamp>2016-05-05T23:52:18Z</timestamp>
      <contributor>
        <username>Faramir1138</username>
        <id>13258862</id>
      </contributor>
      <minor/>
      <comment>Alphabetized the categories.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5026">{{Infobox Software
| name = Cloudera Impala
| logo = 
| screenshot =
| caption =
| developer = [[Cloudera]]
| status = Active
| latest release version = 2.1.0
| latest release date = 
| latest preview version =
| latest preview date =
| programming language = [[C++]]
| operating system = [[Cross-platform]]
| genre = Relational [[Hadoop]]-analytics
| license = [[Apache License]] 2.0
| website = {{URL|http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html|Cloudera Impala Homepage}}
}}
'''Cloudera Impala''' is [[Cloudera]]'s [[open source software|open source]] [[massively parallel processing]] (MPP) SQL query engine for data stored in a [[computer cluster]] running [[Apache Hadoop]].&lt;ref name=&quot;Cloudera Impala&quot;&gt;{{cite web|title=Cloudera Impala|url=http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html|accessdate=14 March 2014}}&lt;/ref&gt;

==Description==
Cloudera Impala is a query engine that runs on Apache Hadoop. 
The project was announced in October 2012 with a public [[beta test]] distribution&lt;ref&gt;{{cite web |url= http://www.zdnet.com/cloudera-aims-to-bring-real-time-queries-to-hadoop-big-data-7000005951/ |title=Cloudera aims to bring real-time queries to Hadoop, big data |author= Larry Digna |date= October 24, 2012 |work= Between the lines blog |publisher= ZDNet |accessdate= January 20, 2014 }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.zdnet.com/clouderas-impala-brings-hadoop-to-sql-and-bi-7000006413/ |title=Cloudera’s Impala brings Hadoop to SQL and BI |author= Andrew Brust |date= October 25, 2012 |work= ZDNet |accessdate= January 20, 2014 }}&lt;/ref&gt; and became generally available in May 2013.&lt;ref&gt;{{cite web |url= http://blog.cloudera.com/blog/2013/05/cloudera-impala-1-0-its-here-its-real-its-already-the-standard-for-sql-on-hadoop/ |title=Cloudera Impala 1.0: It’s Here, It’s Real, It’s Already the Standard for SQL on Hadoop |author= Marcel Kornacker, Justin Erickson |date= May 1, 2013 |accessdate= April 10, 2014 }}&lt;/ref&gt;

Impala brings scalable parallel database technology to Hadoop, enabling users to issue low-latency [[SQL]] queries to data stored in [[HDFS]] and [[Apache HBase]] without requiring data movement or transformation. Impala is integrated with Hadoop to use the same file and data formats, metadata, security and resource management frameworks used by [[MapReduce]], [[Apache Hive]], [[Apache Pig]] and other Hadoop software.

Impala is promoted for analysts and data scientists to perform analytics on data stored in Hadoop via SQL or [[business intelligence]]  tools. The result is that large-scale data processing (via MapReduce) and interactive queries can be done on the same system using the same data and metadata – removing the need to migrate data sets into specialized systems and/or proprietary formats simply to perform analysis.

Features include:
* Supports [[HDFS#Hadoop distributed file system|HDFS]] and [[Apache HBase]] storage,
* Reads Hadoop file formats, including text, [[Lempel–Ziv–Oberhumer|LZO]], [[SequenceFile]], [[Apache_Avro|Avro]], [[RCFile]], and [[Apache_Parquet|Parquet]],
* Supports Hadoop security ([[Kerberos (protocol)|Kerberos authentication]]),
* Fine-grained, role-based authorization with [[Apache Sentry]],
* Uses metadata, [[ODBC]] driver, and SQL syntax from [[Apache Hive]].

In early 2013, a [[column-oriented DBMS|column-oriented file format]] called Parquet was announced for architectures including Impala.&lt;ref&gt;{{Cite web |title= Parquet: Columnar Storage for Hadoop |work= Project web site |year= 2013 |url= http://parquet.io/ |accessdate= January 20, 2014 }}&lt;/ref&gt;
In December 2013, [[Amazon Web Services]] announced support for Impala.&lt;ref&gt;{{Cite web |title= Announcing Support for Impala with Amazon Elastic MapReduce |publisher= Amazon.com |date= December 12, 2013 |url= http://aws.amazon.com/about-aws/whats-new/2013/12/12/announcing-support-for-impala-with-amazon-elastic-mapreduce/ |accessdate= January 20, 2014 }}&lt;/ref&gt;
In early 2014, [[MapR]] added support for Impala.&lt;ref&gt;{{Cite web |title= Impala for MapR |publisher= MapR.com |date= February 2, 2014 |url= http://doc.mapr.com/display/MapR/Impala+for+MapR |accessdate= April 10, 2014 }}&lt;/ref&gt;

==See also==
* [[Presto_(SQL_query_engine)|Presto]] — open source SQL query engine created by Facebook and supported by Teradata. Optimized for high-speed distributed interactive analytics
* [[Dremel (software)|Dremel]] — similar tool from Google
* [[Apache Drill]] — similar open source project inspired by Dremel
* [[Cloudera]] — Hadoop distribution responsible for the Cloudera Impala project

==References==
{{reflist}}

==External links==
*[http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html Cloudera Impala] commercial web site
*[https://github.com/cloudera/impala Impala GitHub] project source code
*[http://impala.io Impala Project Page] project web site

{{DEFAULTSORT:Impala}}
[[Category:Cloud platforms]]
[[Category:Free system software]]
[[Category:Hadoop]]</text>
      <sha1>g7kualvjhnd2wt5wikklvk67xd8odh5</sha1>
    </revision>
  </page>
  <page>
    <title>Hue (Hadoop)</title>
    <ns>0</ns>
    <id>40564013</id>
    <revision>
      <id>701504137</id>
      <parentid>691032429</parentid>
      <timestamp>2016-01-25T00:00:43Z</timestamp>
      <contributor>
        <username>Romainrr</username>
        <id>19747014</id>
      </contributor>
      <comment>/* Striling software */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2031">{{multiple issues|
{{notability|Products|date=September 2013}}
{{refimprove|date=September 2013}}
}}

{{Infobox software
| name                   = Cloudera Hue
| status                 = Active
| programming language   = [[Python (programming language)|Python]]
| genre                  = Web platform
| license                = [[Apache License]] 2.0
| website                = {{URL|http://gethue.com/}}
| logo                   = [[File:Hue official logo.png|frameless|Hue Logo]]
| latest release version = 3.9.0
| latest release date    = {{Start date and age|2015|08|24}}
}}

'''Hue''' is an [[open source|open-source]] Web interface that supports [[Apache Hadoop]] and its ecosystem, licensed under the Apache v2 license.&lt;ref name=&quot;Hue license&quot;&gt;{{cite web|url=https://github.com/cloudera/hue#license|title=Apache v2 License}}&lt;/ref&gt;

== Features ==

Hue aggregates the most common [[Apache Hadoop]] components into a single interface and targets the user experience. Its main goal is to have the users &quot;just use&quot; Hadoop without worrying about the underlying complexity or using a command line.

== Applications ==

Hadoop
* File Browser for [[HDFS]]
* Job Browser for [[MapReduce]]/YARN
* [[Apache HBase]] Browser
* Query editors for [[Apache Hive]], [[Apache Pig]], Impala
* Apache Sqoop2 editor
* Apache Oozie editor and dashboard
* An interactive [[Apache Solr]] dashboard builder
* [[Apache ZooKeeper]] Browser

Generic
* Visual search dashboard for [[Apache Solr]]
* Spark Editor
* SQL Editor for traditional databases

== Distribution ==

The Hue team works with upstream [[Apache Hadoop]] and provides Hue releases on its website.&lt;ref name=&quot;Hue releases&quot;&gt;{{cite web|url=http://gethue.com/category/release/|title=Hue releases}}&lt;/ref&gt; Hue is also present in some major Hadoop distributions ([[Cloudera|CDH]], HDP, [[MapR]] ) and demo VM.

== Interface ==
[[File:Hue_3.6_interface.png|thumb|thumb|center|upright=4|Hue interface]]
103-66209

==References==
{{Reflist|2}}

[[Category:Hadoop]]
[[Category:Big data products]]</text>
      <sha1>3vztf1azv043jo0p4nertm0wvjlj8p8</sha1>
    </revision>
  </page>
  <page>
    <title>Quantcast File System</title>
    <ns>0</ns>
    <id>37854704</id>
    <revision>
      <id>721993504</id>
      <parentid>720375148</parentid>
      <timestamp>2016-05-25T09:11:05Z</timestamp>
      <contributor>
        <username>Minikola</username>
        <id>10463985</id>
      </contributor>
      <comment>Adding Sun/Oracle SAM-QFS disambiguation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3913">{{For|the hierarchical storage manager filesystem from Oracle|QFS}}
{{Infobox software
| name                   = Quantcast File System (QFS)
| logo                   = 
| screenshot             =
| caption                =
| developers              = Sriram Rao, Michael Ovsiannikov, [[Quantcast]]
| status                 = Active
| latest release version = 1.1.4
| latest release date    = {{start date and age|2015|03|05}}&lt;ref&gt;[https://github.com/quantcast/qfs/tree/1.1.4 Release 1.1.4]&lt;/ref&gt;
| latest preview version = 
| latest preview date    = 
| operating system       = 
| programming language   = [[C++]]
| genre                  = [[Distributed file system|Distributed File System]]
| posix compliant        = 
| license                = [[Apache License]] 2.0
| website                = {{URL|quantcast.github.com/qfs}}
}}

'''Quantcast File System''' ('''QFS''') is an open-source [[distributed file system]] software package for large-scale [[MapReduce]] or other batch-processing workloads. It was designed as an alternative to the [[Apache Hadoop]] Distributed File System ([[Apache Hadoop#HDFS|HDFS]]), intended to deliver better performance and cost-efficiency for large-scale processing clusters.

==Design==
QFS is software that runs on a cluster of hundreds or thousands of commodity [[Linux]] servers and allows other software layers to interact with them as if they were one giant hard drive. It has three components:
*A chunk server runs on each machine that will host data, manages I/O to its hard drives, and monitors its activity and capacity.
*A central process called the metaserver keeps the directory structure and maps of files to physical storage. It coordinates activities of all the chunk servers and monitors the overall health of the file system.  For high performance it holds all its data in memory, writing checkpoints and transaction logs to disk for recovery.
*A client component is the interface point that presents a file system [[application programming interface]] (API) to other layers of the software. It makes requests of the metaserver to identify which chunk servers hold (or will hold) its data, then interacts with the chunk servers directly to read and write.

In a cluster of hundreds or thousands of machines, the odds are low that all will be running and reachable at any given moment, so fault tolerance is the central design challenge. QFS meets it with [[Reed–Solomon error correction]]. The form of Reed–Solomon encoding used in QFS stores redundant data in nine places and can reconstruct the file from any six of these stripes.&lt;ref&gt;[http://strata.oreilly.com/2012/09/qfs-improves-performance-of-hadoop-file-system.html QFS improves performance of Hadoop file system - Strata]&lt;/ref&gt; When it writes a file, it by default stripes it across nine physically different machines — six holding the data, three holding parity information. Any three of those can become unavailable. If any six remain readable, QFS can reconstruct the original data. The result is fault tolerance at a cost of a 50% expansion of data. 

QFS is written in the programming language [[C++]], operates within a fixed memory footprint, and uses direct input and output (I/O).

==History==
QFS evolved from the Kosmos File System (KFS), an open source project started by [[Kosmix]] in 2005.  [[Quantcast]] adopted KFS in 2007, built its own improvements on it over the next several years, and released QFS 1.0 as an open source project in September, 2012.&lt;ref&gt;[http://gigaom.com/data/quantcast-releases-bigger-faster-stronger-hadoop-file-system/ Quantcast releases bigger, faster, stronger Hadoop file system — Tech News and Analysis]&lt;/ref&gt;

==References==
&lt;references /&gt;

==External links==
{{Portal|Free software}}
*{{Official website|quantcast.github.com/qfs}}

{{File systems}}

[[Category:Hadoop]]
[[Category:Free system software]]
[[Category:Distributed file systems]]</text>
      <sha1>gor55nlu0d52f9fa1xpt6x21eyjki0b</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Trafodion</title>
    <ns>0</ns>
    <id>43330944</id>
    <revision>
      <id>712659546</id>
      <parentid>712653663</parentid>
      <timestamp>2016-03-30T11:44:07Z</timestamp>
      <contributor>
        <username>Intgr</username>
        <id>246230</id>
      </contributor>
      <minor/>
      <comment>/* top */ simplify link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4234">{{Infobox software
| name                   = Trafodion
| logo                   = 
| developer              = Apache Software Foundation
| latest release version = 1.3
| latest release date    = {{release date|2016|1|11}}
| programming language   = [[C++]], [[Java]]
| operating system       = [[Linux]]
| genre                  = SQL-on-Hadoop [[Relational database management system|DBMS]]
| status                 = Active
| license                = [[Apache License]] 2.0
| website                = [http://trafodion.incubator.apache.org/ trafodion.incubator.apache.org]
}}'''Apache Trafodion''' is an [[open source|open-source]] project is currently incubating at the [[Apache Software Foundation]] and was originally developed by the IT division of [[Hewlett-Packard|Hewlett-Packard Company]] and [[HP Labs]] to provide an enterprise-class transactional [[SQL]]-on-[[Apache HBase|HBase]] solution targeting [[big data]] transactional or operational workloads.&lt;ref name=&quot;trafodion-wiki&quot;&gt;{{cite web | title=Trafodion: Transactional SQL-on-HBase | url=https://wiki.trafodion.org/wiki/index.php/Main_Page | date=June 9, 2014 | accessdate=July 17, 2014}}&lt;/ref&gt; The project is named after the Welsh word for transactions.&lt;ref name=&quot;trafodion-wiki&quot; /&gt;

==Features==
Trafodion is a [[Relational database management system|database management system]] that runs on top of [[Apache Hadoop|Hadoop]],&lt;ref name=&quot;trafodion-features&quot;&gt;{{cite web | title=Trafodion: First Release Features (Release 0.8.0) | url=https://wiki.trafodion.org/wiki/index.php/Release_0.8.0_Features | date=May 29, 2014 | accessdate=October 21, 2015}}&lt;/ref&gt; providing support for transactional or operational workloads in a [[big data]] environment.&lt;ref name=&quot;trafodion-wiki&quot; /&gt; The following is a list of key features:

* [[SQL|ANSI SQL]] language support&lt;ref name=&quot;trafodion-features&quot; /&gt;
* [[Java Database Connectivity|JDBC]] and [[Open Database Connectivity|ODBC]] connectivity for Linux and Windows clients&lt;ref name=&quot;trafodion-features&quot; /&gt;
* Distributed [[ACID]] transaction protection across multiple statements, tables, and rows&lt;ref name=&quot;trafodion-features&quot; /&gt;
* Compile-time and run-time optimizations for real-time operational workloads&lt;ref name=&quot;trafodion-features&quot; /&gt;
* Support for large data sets using a parallel-aware query optimizer and a parallel data-flow execution engine&lt;ref name=&quot;trafodion-features&quot; /&gt;

Transaction management features include:

* Begin, commit, and rollback work syntax, including SET TRANSACTION&lt;ref name=&quot;trafodion-features&quot; /&gt;
* READ COMMITTED transactional isolation level&lt;ref name=&quot;trafodion-features&quot; /&gt;
* Multiple SQL processes participating in the same transaction concurrently&lt;ref name=&quot;trafodion-features&quot; /&gt;
* Recovery after region server, transaction manager, or node failure&lt;ref name=&quot;trafodion-features&quot; /&gt;
* Support for region splits and balancing&lt;ref name=&quot;trafodion-features&quot; /&gt;

==History==
Trafodion was launched by HP as an [[open source|open-source]] project on June 10, 2014.&lt;ref name=&quot;introducing-trafodion&quot;&gt;{{cite web | title=Introducing Trafodion - HP Enterprise Business Community | url=http://h30499.www3.hp.com/t5/Information-Faster-Blog/Introducing-Trafodion/ba-p/6512958#.U73FLvldW0I | date=June 17, 2014 | accessdate=July 17, 2014}}&lt;/ref&gt;

A production ready version of Trafodion was released on January 29, 2015.&lt;ref name=&quot;Trafodion-1.0.0&quot;&gt;{{cite web | title=Release of Trafodion 1.0.0 | url=https://wiki.trafodion.org/wiki/index.php/Event:2015/01/29_Release_of_Trafodion_1.0.0 | date=January 30, 2015 | accessdate=February 23, 2015}}&lt;/ref&gt;

Trafodion became an Apache Incubation Project in May 2015.&lt;ref name=&quot;Apache-Incubation-ProjList&quot;&gt;{{cite web | title=Apache Incubation Project List |
url=http://incubator.apache.org/projects/ | accessdate=October 19, 2015}}&lt;/ref&gt;

==See also==
* [[Big data]]
* [[Apache Hadoop|Hadoop]]
* [[Apache HBase|HBase]]
* [[NewSQL]]

==References==
{{Reflist}}

==External links==
* [http://trafodion.incubator.apache.org Apache Trafodion Incubation website]
* [https://wiki.trafodion.org Pre-Apache Trafodion website - Obsolete]

[[Category:BigTable implementations]]
[[Category:Hadoop]]
[[Category:Free database management systems]]
[[Category:Structured storage]]</text>
      <sha1>ik25d5vn0iof79vna6aw3y3dsutdmxm</sha1>
    </revision>
  </page>
  <page>
    <title>GeoMesa</title>
    <ns>0</ns>
    <id>43326864</id>
    <revision>
      <id>680111049</id>
      <parentid>663213616</parentid>
      <timestamp>2015-09-08T19:49:56Z</timestamp>
      <contributor>
        <username>Welsh</username>
        <id>310131</id>
      </contributor>
      <minor/>
      <comment>Link repair:  Spatio-Temporal Database -&gt; Spatiotemporal database  - [[w:en:WP:RLR|You can help!]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3170">{{multiple issues|
{{notability|Products|date=July 2014}}
{{primary sources|date=July 2014}}
}}

{{Infobox software
| name                   = GeoMesa
| logo                   = 
| screenshot             =
| caption                =
| developer              = [http://www.locationtech.org/ LocationTech],[http://www.ccri.com CCRi]

| status                 = Active
| latest release version = 
| latest release date    = 
| operating system       = [[Linux]]
| programming language   = [[Scala (programming language)|Scala]]
| genre                  = [[Spatiotemporal database]]
| license                = [[Apache License]] 2.0
| website                = {{URL|geomesa.org}}
}}

'''GeoMesa''' is an open-source, distributed, spatio-temporal index built on top of [[BigTable]] databases using an implementation of the [[Geohash]] algorithm.&lt;ref&gt;{{cite conference |url=http://geomesa.github.io/assets/outreach/SpatioTemporalIndexing_IEEEcopyright.pdf|first1=Anthony|last1=Fox|first2=Chris|last2=Eichelberger|first3=James|last3=Hughes|first4=Skylar|last4=Lyon|title=Spatio-temporal Indexing in Non-relational Distributed Databases |conference=IEEE BigData 2013 |year=2013}}&lt;/ref&gt; Written in [[Scala (programming language)|Scala]], GeoMesa is capable of ingesting, indexing, and querying billions of geometry features using a highly parallelized index scheme. GeoMesa builds on top of many existing [http://www.osgeo.org/ Open Source Geo (OSG)] libraries. It implements the [http://geotools.org/ GeoTools] DataStore interface providing standardized access to feature collections as well as implementing a [http://geoserver.org/ GeoServer] plugin.

GeoMesa implements a [[Z-order curve]] via a custom [[Geohash]] implementation to combine three dimensions of geometry and time (i.e. latitude/longitude/timestamp) into a single-dimension lexicographic key space provided by Accumulo.&lt;ref&gt;{{cite web |url=http://www.geomesa.org/2014/08/05/geohash-substrings/ |title=Geohash Ranges |last1=Eichelberger |first1=Chris |date=2014-08-05 |website=GeoMesa |accessdate=2014-09-08}}&lt;/ref&gt;

==Multiple Implementation Support==
Google announced that GeoMesa supported the Google [[BigTable]]&lt;ref&gt;{{cite web |url=http://googlecloudplatform.blogspot.com/2015/05/introducing-Google-Cloud-Bigtable.html |title=Google Cloud Platform Blog: Announcing Google Bigtable |date=2015-05-06 |website=http://googlecloudplatform.blogspot.com/2015/05/introducing-Google-Cloud-Bigtable.html |last1=O’Connor |first1=Cory |accessdate=2015-05-06}}&lt;/ref&gt; hosted NoSQL solution in their release blog post in May 2015. GeoMesa actually supports three Bigtable-derivative implementations:&lt;ref&gt;{{cite web |url=http://www.ccri.com/ |title=CCRI |date=2015-05-06 |website=ccri.com |accessdate=2015-05-06}}&lt;/ref&gt;
* [[Apache Accumulo]]
* Google [[BigTable]] (via HBase API)
* [[Apache HBase]]

==References==
{{Reflist}}

==External links==
*[http://geomesa.github.io/ GeoMesa homepage]
*[https://github.com/locationtech/geomesa GeoMesa Source]
*[http://www.locationtech.org/ LocationTech homepage]
*[https://github.com/ngageoint/geowave Geowave]

[[Category:Hadoop]]
[[Category:Free software programmed in Scala]]</text>
      <sha1>ff7awrx7a5g60eiirdidzxorwklodr2</sha1>
    </revision>
  </page>
  <page>
    <title>Cask (company)</title>
    <ns>0</ns>
    <id>37988515</id>
    <revision>
      <id>723440639</id>
      <parentid>706388544</parentid>
      <timestamp>2016-06-03T02:14:08Z</timestamp>
      <contributor>
        <ip>50.240.223.121</ip>
      </contributor>
      <comment>Grammar and Style plus some content</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5441">
{{Infobox company
| name = Cask
| type = [[Privately held company]]
| logo = Cask logo.jpeg
| industry = [[Big Data]], [[Cloud Computing]], [[Hadoop]]
| foundation = 2011
| founders = Jonathan Gray, Nitin Motgi
| location = [[Palo Alto, California]], [[United States|USA]]
| key_people = {{nowrap begin}}Jonathan Gray &lt;small&gt;(Co-founder and CEO)&lt;/small&gt;{{wrap}}Nitin Motgi &lt;small&gt;(Co-founder)&lt;/small&gt;{{wrap}}Tom Leonard &lt;small&gt;(Sales)&lt;/small&gt;{{wrap}}Vikram Bhan &lt;small&gt;(Operations)&lt;/small&gt;{{wrap}}Boyd Davis &lt;small&gt;(COO)&lt;/small&gt;{{wrap}}Andreas Neumann &lt;small&gt;(Software)&lt;/small&gt;{{wrap}}
| products = CDAP&lt;sup&gt;TM&lt;/sup&gt;{{wrap}}COOPR&lt;sup&gt;TM&lt;/sup&gt;{{wrap}}Tigon&lt;sup&gt;TM&lt;/sup&gt;
| homepage = [http://cask.co// www.cask.co]
| footnotes =
}}

'''Cask''' is a cloud-based [[Big Data]] application platform for developers. Rather than providing a [[cloud service]] for writing and running [[Hadoop]] jobs, Cask allows developers to build, deploy, and manage Big Data applications on top of components within the Hadoop ecosystem more easily.&lt;ref&gt;http://gigaom.com/data/ex-yahoo-facebook-big-data-vets-launch-paas-for-hadoop/&lt;/ref&gt;

==Naming==
The word “Cask” signals that Cask’s software provides containers, a way for different types of applications to run in concert and share data, for instance. “Cask is short, sweet and ‘developer-y,’” Jonathan Gray, CEO said. &lt;ref&gt;http://blogs.wsj.com/venturecapital/2014/09/25/hadoop-developer-cask-formerly-continuuity-goes-open-source/&lt;/ref&gt; It was formerly named Continuuity.

==Product==
The Cask Data Application Platform (CDAP) is an open source project for distributed data and applications. CDAP is a layer of software running on top of Hadoop platforms. 
Cask Coopr is open source cluster management software that provisions, manages and scales clusters on public clouds and private clouds.
Tigon is a distributed framework built on [[Apache Hadoop]]™ and [[Apache HBase]]™ for real-time, high-throughput, low-latency data processing and analytics applications. &lt;ref&gt;http://cask.co/products/&lt;/ref&gt;

The company’s primary offering CDAP, its data application platform, helps users build apps that use Hadoop in smarter ways beyond to build a low-cost, large data warehouse.
Cask also offers Coopr, a tool that helps users quickly and easily provision clusters in the cloud. Given the distributed nature, clusters, like Hadoop, can be difficult and time consuming to deploy. Cask first developed and used Coopr as an internal tool before its customers showed interest because of Coopr's convenience. Coopr works in public, private and OpenStack clouds, so developers can use it to provision CDAP/Hadoop in any of them. &lt;ref&gt;http://thenewstack.io/continuuity-becomes-cask-and-keeps-focus-on-bringing-hadoop-to-the-enterprise/&lt;/ref&gt;

Cask has teamed with [[AT&amp;T]] Labs on an open source project called Tigon (formerly jetStream) that pairs a high-throughput [[SQL]] database with a real-time data-processing engine. The goal is to underpin applications that can handle multiple levels of latency, consistency and analysis on streaming data. &lt;ref&gt;https://gigaom.com/2014/06/03/att-labs-continuuity-will-open-source-a-hadoop-streaming-engine-called-jetstream/&lt;/ref&gt;

==Leadership==
Co-founders Nitin Motgi and Jonathan Gray, were early adopters of the Hbase big data technology while working at Facebook and Yahoo.&lt;ref&gt;http://adtmag.com/articles/2012/10/24/continuuity-targets-big-data-developers.aspx&lt;/ref&gt; Motgi and Gray's firsthand experience of dealing with the limiting complexities of big-data platform motivated them to build the CDAP platform to make big-data development exponentially simpler while completely retaining its power.

==Funding==
Cask’s approach to Hadoop virtualization first attracted attention from renowned venture capital firm [[Andreessen Horowitz]], which participated in Series A and A2 rounds totaling $12.5 million. In November, 2015, Cask announced a further $20 million series B financing. This was led by Safeguard Scientifics, with participation from Battery Ventures, Ignition Partners, and other existing investors.&lt;ref&gt;{{Cite web|url=http://www.prnewswire.com/news-releases/cask-announces-20-million-series-b-financing-led-by-safeguard-scientifics-300173074.html|title=Cask Announces $20 Million Series B Financing Led by Safeguard Scientifics|last=Safeguard Scientifics, Inc.|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; Backing the data and app virtualization approach is Mike Olson, Chief Strategy Officer at [[Cloudera|Cloudera−]]&lt;nowiki/&gt;a software giant specializing on Hadoop, who says Cask will “expand the [big data] market by enabling new use cases while accelerating application development.&lt;ref&gt;http://www.datanami.com/2014/09/25/hadoop-data-virtualization-cask-now-open-source/&lt;/ref&gt;

==See also==
* [[Platform as a Service]]
* [[Big Data]]
* [[Cloudera]]
* [[Hadoop]]
* [[AT&amp;T]]



==References==
{{Reflist}}

==External links==
* [http://www.cask.co// Cask's Corporate Site ]
* [http://blog.cask.co/ Cask's Blog ]

[[Category:Big data companies]]
[[Category:Hadoop]]
[[Category:Information technology companies of the United States]]
[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Companies based in Palo Alto, California]]
[[Category:American companies established in 2011]]
[[Category:Technology companies established in 2011]]
[[Category:Privately held companies based in California]]</text>
      <sha1>5kc4d02xb8dvbzjpg0pqia6z4xkpwgo</sha1>
    </revision>
  </page>
  <page>
    <title>Deeplearning4j</title>
    <ns>0</ns>
    <id>43169442</id>
    <revision>
      <id>723414735</id>
      <parentid>720626431</parentid>
      <timestamp>2016-06-02T22:32:59Z</timestamp>
      <contributor>
        <ip>94.25.74.99</ip>
      </contributor>
      <comment>deeplearning4j GitHub reference was wrong (probably pointing to a competing project)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10146">{{Infobox software
| name                   = Deeplearning4j
| logo                   = 
| screenshot             = 
| caption                =
| collapsible            =
| author                 = [[Adam Gibson (computer scientist)|Adam Gibson]]
| developer              = [https://github.com/SkymindIO/deeplearning4j/graphs/contributors various]
| released               =
| latest release version = 0.0.4
| latest release date    = {{Start date and age|2015|08|24|df=yes}}
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| programming language   = [[Java (programming language)|Java]], [[Scala (programming language)|Scala]], [[CUDA]], [[C (programming language)|C]]
| operating system       = [[Linux]], [[OSX]], [[Windows]], [[Android (operating system)|Android]], [[CyanogenMod]]
| platform               = [[Cross-platform]]
| size                   =
| language               = 
| status                 = active
| genre                  = [[Natural language processing]], [[Deep learning]], [[Machine vision]], [[Artificial intelligence]]
| license                = [[Apache License|Apache 2.0]]
| website                = {{URL|http://deeplearning4j.org}}
}}

{{machine learning bar}}
'''Deeplearning4j''' is an [[open source]] [[deep learning]] library written for [[Java (programming language)|Java]] and the [[JVM|Java Virtual Machine]]&lt;ref name=&quot;wired&quot;&gt;{{cite web
|first=Cade|last=Metz|title=The Mission to Bring Google's AI to the Rest of the World|work=[[Wired.com]]|date=2014-06-02|url=http://www.wired.com/2014/06/skymind-deep-learning/|accessdate=2014-06-28}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.businessweek.com/articles/2014-06-03/teaching-smaller-companies-how-to-probe-deep-learning-on-their-own|title=Deep Learning for (Some of) the People|last=Vance|first=Ashlee|work=[[Bloomberg Businessweek]]|date=2014-06-03|accessdate=2014-06-28}}&lt;/ref&gt; and a [[computing]] framework with wide support for [[deep learning]] algorithms.&lt;ref&gt;{{cite web|url=http://venturebeat.com/2015/11/14/deep-learning-frameworks/|title=Want an open-source deep learning framework? Take your pick|last=Novet|first=Jordan|work=[[VentureBeat]]|date=2015-11-14|accessdate=2015-11-24}}&lt;/ref&gt; Deeplearning4j includes implementations of the [[restricted Boltzmann machine]], [[deep belief net]], deep autoencoder, stacked denoising autoencoder and [[recursive neural network#Recursive Neural Tensor Network|recursive neural tensor network]], as well as [[word2vec]], doc2vec and GloVe. These algorithms all include [[Distributed computing|distributed]] [[Parallel computing|parallel]] versions that integrate with [[Apache Hadoop|Hadoop]] and [[Apache Spark|Spark]].&lt;ref&gt;{{cite web|url=https://www.youtube.com/watch?v=LCsc1hFuNac&amp;feature=youtu.be|title=Adam Gibson, DeepLearning4j on Spark and Data Science on JVM with nd4j, SF Spark @Galvanize 20150212 |last=TV|first=Functional|work=SF Spark Meetup|date=2015-02-12|accessdate=2015-03-01}}&lt;/ref&gt; It is commercially supported by the startup [[Skymind]].

==Introduction==

Deeplearning4j relies on the widely used programming language, [[Java (programming language)|Java]] - though it is compatible with [[Clojure (programming language)|Clojure]] and includes a [[Scala (programming language)|Scala]] API. It is powered by its own open-source numerical computing library, [[ND4J (software)|ND4J]], and works with both [[CPU]]s and [[GPU]]s.&lt;ref name=&quot;om&quot;&gt;{{cite web
|first=Derrick|last=Harris|title=A startup called Skymind launches, pushing open source deep learning|work=[[GigaOM.com]]|date=2014-06-02|url=http://gigaom.com/2014/06/02/a-startup-called-skymind-launches-pushing-open-source-deep-learning/|accessdate=2014-06-29}}&lt;/ref&gt;&lt;ref name=&quot;vb&quot;&gt;{{cite web
|first=Jordan|last=Novet|title=Skymind launches with open-source, plug-and-play deep learning features for your app|date=2014-06-02|url=http://venturebeat.com/2014/06/02/skymind-launches-with-open-source-plug-and-play-deep-learning-features-for-your-app//|accessdate=2014-06-29}}&lt;/ref&gt; Deeplearning4j is an [[open source]] project&lt;ref&gt;{{cite web|title=Github Repository|url=https://github.com/agibsonccc/java-deeplearning}}&lt;/ref&gt; primarily developed by a [[machine learning]] group in San Francisco led by Adam Gibson.&lt;ref name=&quot;deeplearning4j.org&quot;&gt;{{cite web|url=http://deeplearning4j.org/|title=deeplearning4j.org}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Crunchbase Profile|url=http://www.crunchbase.com/person/adam-gibson}}&lt;/ref&gt; Deeplearning4j is the only open-source project listed on [[Google]]'s Word2vec page for its Java implementation.&lt;ref&gt;{{cite web|title=Google Code|url=https://code.google.com/p/word2vec/}}&lt;/ref&gt;

Deeplearning4j has been used in a number of commercial and academic applications. The code is hosted on [[GitHub]]&lt;ref&gt;[https://github.com/deeplearning4j/deeplearning4j Deeplearning4j source code]&lt;/ref&gt; and a support forum is maintained on [[Google Groups]].&lt;ref&gt;[https://groups.google.com/forum/#!forum/deeplearning4j Deeplearning4j Google Group]&lt;/ref&gt;

The framework is composable, meaning shallow neural nets such as restricted Boltzmann machines, convolutional nets, autoencoders and recurrent nets can be added to one another to create deep nets of varying types.

==Distributed==

Training with Deeplearning4j takes place in a cluster. Neural nets are trained in parallel via iterative reduce, which works on [[Hadoop]]/YARN and on [[Apache Spark|Spark]].&lt;ref name=&quot;deeplearning4j.org&quot;/&gt;&lt;ref&gt;{{cite web|url=https://github.com/emsixteeen/IterativeReduce|title=Iterative reduce}}&lt;/ref&gt; Deeplearning4j also integrates with Cuda kernels to conduct pure [[Graphics processing unit|GPU]] operations, and works with distributed GPUs.

==Scientific computing for the JVM==

Deeplearning4j includes an n-dimensional array class using [[ND4J (software)|ND4J]] that allows for scientific computing in Java and Scala, similar to the functionality that [[Numpy]] provides to [[Python programming language|Python]]. It's effectively based on a library for linear algebra and matrix manipulation in a production environment.

==Canova vectorization lib for machine-learning==

Canova vectorizes{{clarify|Which meaning of vectorization is meant here?|date=July 2015}} various file formats and data types using an input/output format system similar to Hadoop's use of MapReduce. A work in progress, Canova is designed to vectorize CSVs, images, sound, text and video. Canova can be used from the command line.

==Text and NLP==

Deeplearning4j includes a [[vector space model]]ing and [[topic model]]ing toolkit, implemented in Java and integrating with parallel GPUs for performance. It is specifically intended for handling large text collections.

Deeplearning4j includes implementations of [[tf–idf]], [[deep learning]], and Mikolov's [https://code.google.com/p/word2vec/ word2vec] algorithm, doc2vec and GloVe – reimplemented and optimized in Java. It relies on [[t-distributed stochastic neighbor embedding|t-SNE]] for word-cloud visualizations.

==Real-world use cases and integrations==

Real-world use cases for Deeplearning4j include fraud detection for the financial sector,&lt;ref&gt;http://www.skymind.io/finance/&lt;/ref&gt; anomaly detection in industries such as manufacturing, recommender systems in e-commerce and advertising,&lt;ref&gt;http://www.skymind.io/commerce/&lt;/ref&gt; and image recognition. Deeplearning4j has integrated with other machine-learning platforms such as RapidMiner and Prediction.io.&lt;ref&gt;https://www.rapidminerchina.com/en/products/shop/product/deeplearning4j/&lt;/ref&gt;

==Related libraries==

* [[OpenNN]], an [[open source]] [[neural networks]] library written in [[C++]] for [[deep learning]].   
* [[Torch (machine learning)|Torch]], an [[open source]] framework written in [[Lua (programming language)|Lua]] with wide support for [[machine learning]] algorithms.
* [[Theano (software)|Theano]], an [[open source]] [[deep learning]] library for [[Python (programming language)|Python]].
* [[Neuroph]]

==See also==
{{Portal|Free software|Java}}
* [[Comparison of deep learning frameworks]]
* [[Artificial intelligence]]
* [[Machine learning]]
* [[Deep learning]]

==References==
{{Reflist}}

==External links==
* {{official website|http://www.deeplearning4j.org}}
* {{cite web|title=Deeplearning4j Github Repositories|url=https://github.com/deeplearning4j}}
* {{cite web|title=Deeplearning4j vs. Torch vs. Caffe vs. Theano|url=http://deeplearning4j.org/compare-dl4j-torch7-pylearn.html}}
* {{cite web|title=Canova: A General Vectorization Lib for Machine Learning|url=http://deeplearning4j.org/canova.html}}
* {{cite web|title=ND4J: N-Dimensional Arrays for Java and Scala, A Linear Algebra Library|url=http://nd4j.org}}
* {{cite web|title=Apache Flink|url=https://flink.apache.org/}}
* {{cite web|title=Java Magazine by Oracle: Deep Learning in Java|url=http://www.oraclejavamagazine-digital.com/javamagazine/may_june_2015?sub_id=DJ9kzXBnuXELe#pg58}}
* {{cite web|title=Deeplearning4j Community of Gitter Chat|url=https://gitter.im/deeplearning4j/deeplearning4j}}

{{Computer vision footer}}

[[Category:Array programming languages]]
[[Category:Numerical programming languages]]
[[Category:Data mining and machine learning software]]
[[Category:Deep learning]]
[[Category:Free statistical software]]
[[Category:Java platform]]
[[Category:Java programming language family]]
[[Category:JVM programming languages]]
[[Category:Scala (programming language)]]
[[Category:Natural language processing]]
[[Category:Artificial neural networks]]
[[Category:Free science software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Java (programming language) libraries]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]
[[Category:Cluster computing]]
[[Category:Free software programmed in Scala]]
[[Category:Applied machine learning]]
[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Information technology companies of the United States]]
[[Category:Image processing]]
[[Category:Machine learning]]</text>
      <sha1>nk4vk7ykeq9sbz3yxsibuyugxafeyvf</sha1>
    </revision>
  </page>
  <page>
    <title>H2O (software)</title>
    <ns>0</ns>
    <id>39775659</id>
    <revision>
      <id>722976191</id>
      <parentid>721222563</parentid>
      <timestamp>2016-05-31T07:24:33Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor/>
      <comment>/* Scientific advisory council */clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17099">{{Advert|article|date=February 2016}}

{{Infobox software
| name                   = H2O (software)
| title                  = H2O
| logo                 =H2O_logo_from_H2O.ai.png &lt;!-- FAIR USE. Not for use elsewhere --&gt;
| logo caption           = 
| logo size              = 
| logo alt               = The corporate logo of H2O
| screenshot             = &lt;!-- Image name is enough --&gt;
| caption                = 
| screenshot size        = 
| screenshot alt         = 
| collapsible            = 
| author                 = [[0xdata]]
| developer              = [[H2O.ai]]
| released               = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| discontinued           = 
| latest release version = Slater (Version 3.2.0.5)
| latest release date    = {{Start date and age|2015|09|24}}
| status                 = Active
| programming language   = H2O (written in [[Java (programming language)|Java]], [[Python (programming language)|Python]], and [[R (programming language)|''R'']])&lt;ref name=&quot;Harris&quot; &gt;{{harvtxt|Harris|2012}}&lt;/ref&gt;&lt;ref name=&quot;Novet&quot; /&gt;&lt;ref name=&quot;Rec&quot; &gt;
{{cite web|publisher=H2O.ai|website=0xdata.com|url=http://0xdata.com/product/recommended-systems-for-h2o/|title=Recommended systems for H2O|ref=harv|date=May 2015}}
&lt;/ref&gt;
| operating system       = [[Linux]], [[Mac OS]], and [[Microsoft Windows]]
| platform               = [[Apache Hadoop]] [[Apache Hadoop#Hadoop distributed file system|Distributed File System]]; [[Amazon EC2]], [[Google Compute Engine]], and [[Microsoft Azure]].
| size                   = 
| language               = English
| language count         = &lt;!-- Number only --&gt;
| language footnote      = 
| genre                  = [[big data|big]] [[data analytics]], [[machine learning]], [[statistical learning theory]]&lt;ref name=&quot;Hardy&quot; &gt;{{harvtxt|Hardy|2014}}&lt;/ref&gt;
| license                = [[Open source software|Open source]]
| alexa                  = 606,809&lt;ref name=&quot;Alexa&quot; &gt;{{cite web|url=http://www.alexa.com/siteinfo/0xdata.com|publisher=Alexa.com|title=How popular is 0xdata.com?|ref=harv|date=1 June 2015}}&lt;/ref&gt;
| website                = {{URL|http://0xdata.com/}}
| standard               = [[Databricks]] certified on [[Apache Spark|Spark]].&lt;ref name=&quot;Rec &quot; /&gt;
| AsOf                   = 1 June 2015
}}

'''H2O''' is [[open-source software]] for [[big data|big-data]] [[data analysis|analysis]]. It is produced by the  [[startup company|start-up]] ''[[H2O.ai]]'' (formerly ''0xdata''), which launched in 2011 &lt;!-- and in 2014 was located --&gt; in [[Silicon Valley|Silicon&amp;nbsp;Valley]]. The speed and flexibility of H2O allow users to fit hundreds or thousands of potential models as part of discovering patterns in data. With H2O, users can throw models at data to find usable information, allowing H2O to discover patterns. Using H2O, [[Cisco]] estimates each month 20 thousand models of its customers' propensities to buy.&lt;ref name=&quot;Woodie&quot; /&gt;

H2O's mathematical core is developed with the leadership of Arno Candel; after H2O was rated as the best &quot;open-source Java machine learning project&quot; by [[GitHub]]'s programming members, Candel was named to the first class of &quot;Big Data All Stars&quot; by [[Fortune magazine|''Fortune'']] in 2014.&lt;ref name=&quot;Hackett&quot; /&gt; The firm's scientific advisors are experts on [[statistical learning theory]] and [[mathematical optimization]].

The H2O software runs can be called from the [[R (programming language)|statistical package ''R'']] and other environments.  It is used for exploring and analyzing datasets held in [[cloud computing]] systems and in the [[Apache Hadoop|Apache]] [[Hadoop Distributed File System]] as well as in the conventional operating-systems [[Linux]], [[Mac OS]], and [[Microsoft Windows]]. The H2O software is written in [[Java (programming language)|Java]], [[Python (programming language)|Python]], and [[R (programming language)|''R'']]. Its graphical-user interface is compatible with four popular browsers: [[Chrome (browser)|Chrome]], [[Safari (browser)|Safari]], [[Firefox (browser)|Firefox]], and [[Internet Explorer]].

==H2O==

The H2O project aims to develop an analytical interface for cloud computing, providing users with intuitive tools for data analysis.&lt;ref name=&quot;Harris&quot; /&gt;

===Leadership===
[[File:Cliff Click and SriSatish Ambati of H2O.ai (0xdata).jpg|thumb|alt=Cliff Click (left) and SriSatish Ambati (right) speak at an event for H2O.ai (0xdata).|H2O.ai was co-founded by Cliff Click and SriSatish Ambati. (Photograph by H2O.ai released under Creative Commons BY 2.0 license.[https://www.flickr.com/photos/h2oai/15684897277/])]]

H2O's chief executive, SriSatish Ambati, had helped to start [[Platfora]], a big-data firm that develops software for the Apache Hadoop distributed file system.&lt;ref name=&quot;2 WSJ&quot;&gt;{{harvtxt|Gage|2013}}&lt;/ref&gt; Ambati was frustrated with the performance of the [[R (programming language)|''R'' programming language]] on large data-sets and started the development of H2O software  with encouragement from [[John Chambers (statistician)|John Chambers]],&lt;ref name=&quot;Novet&quot;&gt;{{harvtxt|Novet|2014}}&lt;/ref&gt; who created the [[S (programming language)|''S'' programming language]] at Bell Labs and who is a member of ''R''{{'}}s core team  (which leads the development of ''R'').&lt;ref name=&quot;Novet&quot; /&gt;&lt;ref&gt;[http://www.acm.org/announcements/ss99.html/ ACM honors Dr. John M. Chambers of Bell Labs with the 1998 ACM Software System Award for creating &quot;S System&quot; software], ACM press release, March 29, 1999. Accessed 8 December 2008.&lt;/ref&gt;&lt;ref name=&quot;StatModS&quot; /&gt;

Ambati co-founded 0xdata with Cliff Click, who serves as the chief technical officer of H2O. Click helped to write the [[HotSpot]] Server Compiler and worked with [[Azul Systems]] to construct a big-data [[Java virtual machine]] (JVM).&lt;ref name=&quot;Schuster&quot; &gt;
{{cite journal|title=Cliff Click on in-memory processing, 0xdata H20, efficient {{nowrap|low latency}} Java and GCs|first=Werner|last=Schuster|date=10 January 2014|ref=harv
|url=http://www.infoq.com/interviews/click-0xdata
|journal=InfoQ|accessdate=2 June 2015}}
&lt;/ref&gt;

Mathematical leadership is provided by the Dr. [[Arno Candel]], who has the title &quot;physicist and hacker&quot;. Candel was a founding engineer at [[Skytree, Inc|Skytree]], where he implemented methods for [[machine learning]], before he developed the mathematical core of H2O. After H2O was rated as the best &quot;open-source Java machine learning project&quot; by [[GitHub]]'s programming members, Candel (with 19 others) was named to the first class of &quot;Big Data All Stars&quot; by [[Fortune magazine|''Fortune'']].&lt;ref name=&quot;Hackett&quot; &gt;{{harvtxt|Hackett|2014}}&lt;/ref&gt;
 
====Scientific advisory council====
[[File:TrevorHastiePic.jpg|right|thumb|Stanford University professor Trevor J. Hastie serves as an advisor to H2O.ai.]]

H2O's Scientific Advisory Council lists three mathematical scientists, who are all professors at Stanford University:&lt;ref name=&quot;About&quot;&gt;
{{cite web|publisher=0xdata|title=About
|url=http://0xdata.com/about/|year=2015|ref=harv}}
&lt;/ref&gt; Professor [[Stephen P. Boyd]] is an expert in [[convex optimization|convex minimization]] and applications in statistics and electrical engineering.&lt;ref name=&quot;CO&quot; &gt;{{cite book|title=Convex optimization|first1=Stephen P.|last1=Boyd|authorlink1=Stephen P. Boyd|first2=Lieven|last2=Vandenberghe|year=2004|publisher=Cambridge University Press|isbn=978-0-521-83378-3|url=http://web.stanford.edu/~boyd/cvxbook/|accessdate=October 15, 2011}} ([http://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf Free download of PDF of corrected 7th printing, 2009])&lt;/ref&gt; [[Robert Tibshirani]], a collaborator with Bradley Efron on [[bootstrapping (statistics)|bootstrapping]],&lt;ref&gt;{{cite book|author = Bradley Efron|author2=[[Robert Tibshirani]] |title = An Introduction to the Bootstrap|publisher = Chapman &amp; Hall/CRC|year = 1994| isbn=978-0-412-04231-7|url=http://books.google.com/books/about/An_Introduction_to_the_Bootstrap.html?id=gLlpIUxRntoC}}&lt;/ref&gt; is an expert on [[generalized additive model]]s and [[statistical learning theory]].&lt;ref name=&quot;GAM1990&quot; &gt;{{cite book|author1=Hastie, T. J.  |author2=Tibshirani, R. J.|title = Generalized additive models|publisher = Chapman &amp; Hall/CRC|year = 1990|isbn=978-0-412-34390-2}}&lt;/ref&gt;&lt;ref name=&quot;SLT&quot;&gt;{{cite book| url=http://www-stat.stanford.edu/~tibs/ElemStatLearn/ | title=The Elements of Statistical Learning | author1=Hastie, Trevor | author2=Tibshirani, Robert | author3=Friedman, Jerome H. | authorlink1=Trevor Hastie | authorlink2=Robert Tibshirani | authorlink3=Jerome H. Friedman |accessdate=15 June 2012|year=2011|edition=second}} ([http://statweb.stanford.edu/~tibs/ElemStatLearn/download.html Free download of 10th printing, June 2013])&lt;/ref&gt; [[Trevor Hastie]], a collaborator of John Chambers on ''S'',&lt;ref name=&quot;StatModS&quot;&gt;J. Chambers and T. Hastie, ''Statistical Models in S'', Wadsworth/Brooks Cole, 1991.&lt;/ref&gt; is an expert on  [[generalized additive model]]s and statistical learning theory.&lt;ref name=&quot;GAM1990&quot; /&gt;&lt;ref name=&quot;SLT&quot; /&gt;

===H2O.ai: A Silicon Valley start-up===
{{Main|H2O.ai}}

The software is open-source and freely distributed. The company receives fees for providing customer service and customized extensions.  In November 2014, its twenty clients included [[Cisco]], [[eBay]], [[Nielsen N.V.|Nielsen]], and [[PayPal]], according to ''VentureBeat''.&lt;ref name=&quot;Novet&quot; /&gt; The speed and flexibility of H2O allow users to fit hundreds or thousands of potential models as part of discovering patterns in data. With H2O, users can throw models at data to find usable information, according to Tye Rattenbury at [[Trifacta]].  Using H2O, Cisco estimates each month 20 thousand models of its customers' propensities to buy while Google fits different models for each client according to the time of day.&lt;ref name=&quot;Woodie&quot; &gt;
{{cite journal|url=http://www.datanami.com/2015/02/09/rise-predictive-modeling-factories/|ref=harv|accessdate=2 June 2015|date=9 February 2015|title=The Rise of Predictive Modeling Factories|first=Alex|last=Woodie|journal=Datanami: Big data, big analytics, big insights}}&lt;/ref&gt;

===Mining of big data===
{{machine learning bar}}
{{See also|Data mining|Machine learning}}

[[Big data]]sets are too large to be analyzed using traditional software like [[R (programming language)|''R'']]. The H2O software provides data structures and methods suitable for big data.

H2O allow users to analyze and visualize whole sets of data without using the [[Procrustean]] strategy of studying only a small subset with a conventional statistical package.&lt;ref name=&quot;Novet&quot; /&gt; H2O's statistical repertoire includes [[generalized linear model]]s and [[K-means clustering]].&lt;ref name=&quot;CRAN&quot; &gt;{{citation|title =h2o: ''R'' Interface for H2O|author =Aiello, Spencer and Tom Kraljevic and Petr Maj and with contributions from the 0xdata team|year=2015|number=3.0.0.12
|url=http://cran.r-project.org/web/packages/h2o/index.html
|ref=harv|series=Contributed Packages|publisher=The R Project for Statistical Computing|website=The Comprehensive R Archive Network (CRAN)}}&lt;/ref&gt;

====Iterative methods for real-time problems====

H2O uses [[iterative method]]s that provide quick answers using all of the client's data. When a client cannot wait for an optimal solution, the client can interrupt the computations and use an approximate solution.&lt;ref name=&quot;Harris&quot; /&gt;

In its approach to [[deep learning]],&lt;ref name=&quot;Novet&quot; /&gt;&lt;ref name=&quot;CRAN&quot; /&gt;&lt;ref&gt;&quot;Prediction of IncRNA using Deep Learning Approach&quot;. Tripathi, Rashmi; Kumari, Vandana; Patel, Sunil; Singh, Yashbir; Varadwaj, Pritish.  ''International Conference on Advances in Biotechnology (BioTech)''. Proceedings: 138-142. Singapore: Global Science and Technology Forum. (2015)&lt;/ref&gt; H2O divides all the data into subsets and then analyzing each subset simultaneously using the same method. These processes are combined to estimate parameters by using the  Hogwild scheme,&lt;ref&gt;[http://h2o-release.s3.amazonaws.com/h2o/rel-shannon/12/docs-website/h2o-docs/index.html#Data%20Science%20Algorithms-GLM-GLM%20Algorithm Description of the iterative method for computing maximum-likelihood estimates for a generalized linear model].&lt;/ref&gt; a parallel [[stochastic gradient descent|stochastic gradient]] method.&lt;ref&gt;{{cite journal|title=Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent|author =Benjamin Recht and Re, Christopher and Wright, Stephen and Feng Niu
|journal=Advances in Neural Information Processing Systems|volume=24|editors=J. Shawe-Taylor and R.S. Zemel and P.L. Bartlett and F. Pereira and K.Q. Weinberger|pages=693–701|year=2011|publisher=Curran Associates, Inc.
|url=http://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf
|ref=harv}} [https://www.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf Recht's PDF]&lt;/ref&gt; These methods allow H2O to provide answers that use all the client's  data, rather than throwing away most of it and analyzing a subset with conventional software.

===Software===
&lt;!-- Following the guidelines of the article's WikiProjects and the software information-box template, this section provides the information requested by the template.  Citations can be expanded to include facts from Novet and from Harris, but this would just clutter the article. It is simplest and cleanest to use H2O.ai's recommendations as the uniform source. --&gt;

====Programming languages====

The H2O software was written with three programming languages: [[Java (programming language)|Java]] (6 or later), [[Python (programming language)|Python]] (2.7.x), and [[R (programming language)|''R'']] (3.0.0 or later).&lt;ref name=&quot;Novet&quot; /&gt;&lt;ref name=&quot;Rec&quot; /&gt;

====Operating systems====
The H2O software can be run on conventional operating-systems: [[Microsoft Windows]] ([[Microsoft Windows 7|7]] or later), Mac [[OS X]] ([[OS X Mavericks|10.9]] or later), and [[Linux]] ([[Ubuntu 12.04]] &lt;!-- not &quot;or later&quot; 1 June 2015 --&gt;; [[RHEL]]/[[CentOS]] 6 or later),&lt;ref name=&quot;Rec &quot; /&gt; It also runs on big-data systems, particularly [[Apache Hadoop]] [[Apache Hadoop#Hadoop distributed file system|Distributed File System]] (HDFS), several popular versions: [[Cloudera]] (5.1 or later), [[MapR]] (3.0 or later), and [[Hortonworks]] (HDP 2.1 or later). It also operates on [[cloud computing]] environments, for example using [[Amazon EC2]], [[Google Compute Engine]], and [[Microsoft Azure]]. The H2O Sparkling Water software is [[Databricks]]-certified on [[Apache Spark]].&lt;ref name=&quot;Rec &quot; /&gt;

====Graphical user interface and browsers====

Its graphical user interface is compatible with four browsers (unless specified, in their latest versions as of 1 June 2015): [[Chrome (browser)|Chrome]], [[Safari (browser)|Safari]], [[Firefox (browser)|Firefox]], [[Internet Explorer]] ([[Internet Explorer 10|IE10]]).&lt;ref name=&quot;Rec &quot; /&gt;

==Notes==
{{Reflist}}

==References==
* {{cite news|last=Gage|first=Deborah|title=Platfora founder goes in search of big-data answers|url=http://blogs.wsj.com/venturecapital/2013/04/15/platfora-founder-goes-in-search-of-big-data-answers/|date=15 April 2013|newspaper=Wall Street Journal|ref=harv|accessdate=2 June 2015}}
* {{citation
|url=http://fortune.com/2014/08/03/meet-fortunes-2014-big-data-all-stars/
|journal=Fortune|first1=Robert|last1=Hackett
|title=Arno Candel, physicist and hacker, 0xdata
|editor-first2=Robert|editor-last2=Hackett|series=Meet Fortune's 2014 Big Data All-Stars|ref=harv|accessdate=2 June 2015|editor-last1=Nusca|editor-first1=Andrew|editor-last3=Gupta|editor-first3=Shalene|date=3 August 2014}}
* {{cite news|title=Valuable humans in our digital future|first=Quentin|last=Hardy|date=3 May 2014|newspaper=New York Times
|url=http://bits.blogs.nytimes.com/2014/05/03/valuable-humans-in-our-digital-future/?_r=0
|ref=harv|accessdate=1 June 2015}}
* {{cite journal|title=How 0xdata wants to help everyone become data scientists|first=Derrick|last=Harris|date=14 August 2012|ref=harv
|url=https://gigaom.com/2012/08/14/how-0xdata-wants-to-help-everyone-become-data-scientists/|journal=Gigaom Research|accessdate=1 June 2015}}
* {{cite journal|title=0xdata takes $8.9M and becomes H2O to match its open-source machine-learning project|date=7 November 2014|first=Jordan|last=Novet|journal=VentureBeat
|url=http://venturebeat.com/2014/11/07/h2o-funding/|ref=harv|accessdate=1 June 2015}}

==External links==
* {{URL|http://h2o.ai/product/|H2O software}} at {{URL|http://0xdata.com/ |H2O.ai (formerly 0xdata)}}
* {{URL|https://github.com/0xdata/h2o|Github repository of H2O software}}
* {{URL|https://www.youtube.com/channel/UCk6ONJlPzjw3DohAeMSgsng|YouTube channel of H2O.ai}}

{{Statistical software}}
{{Data warehouse}}

[[Category:Free data analysis software]]
[[Category:Data mining and machine learning software]]
[[Category:Open-source cloud applications]]
[[Category:R (programming language)| ]]
[[Category:Cross-platform free software]]
[[Category:Data-centric programming languages]]
[[Category:Free statistical software]]
[[Category:Free data visualization software]]
[[Category:Hadoop]]
[[Category:Stanford University]]
[[Category:Big data products]]</text>
      <sha1>o70xzuqbw0ei4rs6acv9by5v1ytyly0</sha1>
    </revision>
  </page>
  <page>
    <title>ND4S</title>
    <ns>0</ns>
    <id>47459238</id>
    <revision>
      <id>679853426</id>
      <parentid>675066891</parentid>
      <timestamp>2015-09-07T05:39:05Z</timestamp>
      <contributor>
        <username>MalapJPL</username>
        <id>26194588</id>
      </contributor>
      <comment>To reference an institution like JPL, there needs to be an official reference. Please refrain from mentioning such association without a reference/paper that says so.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3763">{{refimprove|date=August 2015}}

{{Infobox software
| name                   = ND4S: N-Dimensional Arrays for Scala
| logo                   = 
| screenshot             = 
| caption                =
| collapsible            =
| author                 = Adam Gibson, Taisuke Oe
| developer              = [https://github.com/deeplearning4j/nd4s various]
| released               =
| latest release version = 0.0.3.5.5
| latest release date    = {{Start date and age|2014|09|15|df=yes}}
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| programming language   = [[Scala (programming language)|Java]]
| operating system       = [[Linux]], [[OSX]], [[Windows]], [[Android (operating system)|Android]]
| platform               = [[Cross-platform]]
| size                   =
| language               = 
| status                 = active
| genre                  = [[Natural language processing]], [[Deep learning]], [[Machine vision]]
| license                = [[Apache License|Apache 2.0]]
| website                = {{URL|https://github.com/deeplearning4j/nd4s}}
}}

'''ND4S''' is a free, [[Open-source software|open-source]] extension of the [[Scala (programming language)|Scala programming language]] operating on the [[JVM|Java Virtual Machine]]—though it is compatible with both [[Java (programming language)|Java]] and [[Clojure (programming language)|Clojure]].&lt;ref&gt;{{cite web|title=Official Website|url=http://nd4j.org/}}&lt;/ref&gt; 

ND4S is a scientific computing library for linear algebra and matrix manipulation in a production environment, integrating with Hadoop and Spark to work with distributed GPUs. It supports n-dimensional arrays for JVM-based languages.

ND4S has primarily been developed by the group in San Francisco that built [[Deeplearning4j]], led by Adam Gibson.&lt;ref&gt;{{cite web|title=Github Repository|url=https://github.com/deeplearning4j/nd4s}}&lt;/ref&gt; It was created under an [[Apache Software Foundation]] license.

==Distributed==

ND4S's operations include [[Distributed computing|distributed]] [[Parallel computing|parallel]] versions. They can take place in a cluster and process massive amounts of data. Matrix manipulation occurs in parallel on GPUs or CPUs in the cloud, and can work within Spark or Hadoop clusters.

==Benchmarks==

Benchmarks show that ND4S runs roughly twice as fast as Numpy on large matrices.&lt;ref&gt;{{cite web|title=ND4J Benchmarks|url=http://nd4j.org/benchmarking}}&lt;/ref&gt;

==See also==
{{Portal|Free software|Java}}
* [[NumPy]]
* [[SciPy]]

==References==
{{Reflist}}

==External links==
* {{official website|https://github.com/deeplearning4j/nd4s}}

[[Category:Array programming languages]]
[[Category:Numerical programming languages]]
[[Category:Data mining and machine learning software]]
[[Category:Free statistical software]]
[[Category:Java platform]]
[[Category:Java programming language family]]
[[Category:JVM programming languages]]
[[Category:Scala (programming language)]]
[[Category:Linear algebra]]
[[Category:Numerical analysis]]
[[Category:Computational statistics]]
[[Category:Artificial neural networks]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free science software]]
[[Category:Numerical analysis software for Linux]]
[[Category:Numerical analysis software for OS X]]
[[Category:Numerical analysis software for Windows]]
[[Category:Free mathematics software]]
[[Category:Java (programming language) libraries]]
[[Category:Numerical software]]
[[Category:Cluster computing]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]
[[Category:Free software programmed in Scala]]


{{compu-stub}}</text>
      <sha1>en1qg8tbb4u4tvv5agayp85varshfap</sha1>
    </revision>
  </page>
  <page>
    <title>Gremlin (programming language)</title>
    <ns>0</ns>
    <id>33800942</id>
    <revision>
      <id>717303489</id>
      <parentid>717303462</parentid>
      <timestamp>2016-04-26T22:17:05Z</timestamp>
      <contributor>
        <ip>66.66.71.166</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13188">{{Infobox programming language
| name = Gremlin
| logo = [[File:Gremlin (programming language).png|255px]]
| year = {{Start date and age|2009}}
| designer = Marko A. Rodriguez
| developer = Apache TinkerPop of the [[Apache Software Foundation]]
| written_in = [[Java (programming language)]]
| latest release version = Gremlin 3.0.1&lt;ref name=&quot;Gremlin Releases&quot;&gt;{{cite web|url=http://tinkerpop.incubator.apache.org/|title=Gremlin 3.0.1|accessdate=September 18, 2015}}&lt;/ref&gt;
| dialects = Gremlin-Java8, Gremlin-Groovy, Gremlin-Scala, Gremlin-Clojure, Gremlin-PHP, Gremlin-JavaScript, Gremlin-Typeset
| operating_system = [[Cross-platform]] (multi-platform)
| license = [[Apache License]]
| website =  [http://tinkerpop.com/ Official Site]
| status = Active
| influenced_by = [[Regular expression]], [[XPath]], Ripple, [[SPARQL]], [[SQL]], [[Java_(programming_language)|Java]]/[[Java_virtual_machine|JVM]]
}}

'''Gremlin''' is a [[graph traversal]] language and [[virtual machine]] developed by Apache TinkerPop of the [[Apache Software Foundation]]. Gremlin works for both [[Online transaction processing|OLTP]]-based graph databases as well as [[Online analytical processing|OLAP]]-based graph processors. Gremlin's [[Automata theory|automata]] and [[Functional programming|functional language]] foundation enable Gremlin to naturally support [[Imperative programming|imperative]] and [[Declarative programming|declarative]] querying, host language agnosticism, user-defined [[Domain-specific language|domain specific languages]], an extensible compiler/optimizer, single- and multi-machine execution models, hybrid depth- and breadth-first evaluation, as well as [[Turing Complete]]ness.&lt;ref name=&quot;Gremlin Machine and Language&quot;&gt;{{cite web|url=http://arxiv.org/abs/1508.03843|title=The Gremlin Graph Traversal Machine and Language|accessdate=August 21, 2015}}&lt;/ref&gt;

As an explanatory analogy, Apache TinkerPop and Gremlin are to [[graph databases]] what the [[Java Database Connectivity|JDBC]] and [[SQL]] are to [[RDBMS|relational databases]]. Likewise, the Gremlin traversal machine is to graph computing as what the [[Java virtual machine]] is to general purpose computing.&lt;ref name=&quot;The Benefits of the Gremlin Graph Traversal Machine&quot;&gt;{{cite web|url=http://www.datastax.com/dev/blog/the-benefits-of-the-gremlin-graph-traversal-machine|title=The Benefits of the Gremlin Graph Traversal Machine|accessdate=September 17, 2015}}&lt;/ref&gt;

==Vendor Integration==

Gremlin is an [[Apache License|Apache2-licensed]] graph traversal language that can be used by graph system vendors. There are typically two types of graph system vendors: OLTP [[graph databases]] and OLAP graph processors. The table below outlines those graph vendors that support Gremlin.

{| class=&quot;wikitable&quot;
|-
! Vendor
! Graph System
|-
| Titan&lt;ref name=&quot;titan&quot;&gt;{{cite web|url=http://thinkaurelius.github.io/titan/|title=Titan Graph Database Homepage|accessdate=August 27, 2015}}&lt;/ref&gt; &lt;ref name=&quot;dynamodb&quot;&gt;{{cite web|url=http://www.allthingsdistributed.com/2015/08/titan-graphdb-integration-in-dynamodb.html|title=Titan GraphDB Integration in DynamoDB|accessdate=August 21, 2015}}&lt;/ref&gt;
| graph database
|-
| [[Neo4j]]
| graph database
|-
| [[OrientDB]]
| graph database
|-
| [[Apache Hadoop|Hadoop]] ([[Apache Giraph|Giraph]])
| graph processor
|-
| [[Apache Hadoop|Hadoop]] ([[Apache Spark|Spark]])
| graph processor
|-
| [[InfiniteGraph]]
| graph database
|-
| BlazeGraph
| graph database
|-
| DEX
| graph database
|-
| Sqlg
| graph database
|-
| Bitsy
| graph database
|}

==Traversal Examples==
The following examples of Gremlin queries and responses in a Gremlin-Groovy environment are relative to a graph representation of the [http://grouplens.org/datasets/movielens/ MovieLens] dataset.&lt;ref name=&quot;NoSQLNow Slides&quot;&gt;{{cite web|url=http://www.slideshare.net/slidarko/the-gremlin-traversal-language|title=The Gremlin Graph Traversal Language|accessdate=August 22, 2015}}&lt;/ref&gt; The dataset includes users who rate movies. Users each have one occupation, and each movie has one or more categories associated with it. The MovieLens graph schema is detailed below.
&lt;source lang=&quot;cypher&quot;&gt;
user--rated[stars:0-5]--&gt;movie
user--occupation--&gt;occupation
movie--category--&gt;category
&lt;/source&gt;

===Simple Traversals===

{{pull quote|For each vertex in the graph, emit its label, then group and count each distinct label.}}

&lt;source lang=&quot;groovy&quot;&gt;
gremlin&gt; g.V().label().groupCount()
==&gt;[occupation:21, movie:3883, category:18, user:6040]
&lt;/source&gt;

{{pull quote|What year was the oldest movie made?}}

&lt;source lang=&quot;groovy&quot;&gt;
gremlin&gt; g.V().hasLabel('movie').values('year').min()
==&gt;1919
&lt;/source&gt;

{{pull quote|What is Die Hard's average rating?}}

&lt;source lang=&quot;groovy&quot;&gt;
gremlin&gt; g.V().has('movie','name','Die Hard').inE('rated').values('stars').mean()
==&gt;4.121848739495798
&lt;/source&gt;

===Projection Traversals===

{{pull quote|For each category, emit a map of its name and the number of movies it represents.}}

&lt;source lang=&quot;groovy&quot;&gt;
gremlin&gt; g.V().hasLabel('category').as('a','b').
           select('a','b').
             by('name').
             by(inE('category').count())
==&gt;[a:Animation, b:105]
==&gt;[a:Children's, b:251]
==&gt;[a:Comedy, b:1200]
==&gt;[a:Adventure, b:283]
==&gt;[a:Fantasy, b:68]
==&gt;[a:Romance, b:471]
==&gt;[a:Drama, b:1603]
==&gt;[a:Action, b:503]
==&gt;[a:Crime, b:211]
==&gt;[a:Thriller, b:492]
==&gt;[a:Horror, b:343]
==&gt;[a:Sci-Fi, b:276]
==&gt;[a:Documentary, b:127]
==&gt;[a:War, b:143]
==&gt;[a:Musical, b:114]
==&gt;[a:Mystery, b:106]
==&gt;[a:Film-Noir, b:44]
==&gt;[a:Western, b:68]
&lt;/source&gt;

{{pull quote|For each movie with at least 11 ratings, emit a map of its name and average rating. Sort the maps in decreasing order by their average rating. Emit the first 10 maps (i.e. top 10).}}

&lt;source lang=&quot;groovy&quot;&gt;
gremlin&gt; g.V().hasLabel('movie').as('a','b').
           where(inE('rated').count().is(gt(10))).
           select('a','b').
             by('name').
             by(inE('rated').values('stars').mean()).
           order().by(select('b'),decr).
           limit(10)
==&gt;[a:Sanjuro, b:4.608695652173913]
==&gt;[a:Seven Samurai (The Magnificent Seven), b:4.560509554140127]
==&gt;[a:Shawshank Redemption, The, b:4.554557700942973]
==&gt;[a:Godfather, The, b:4.524966261808367]
==&gt;[a:Close Shave, A, b:4.52054794520548]
==&gt;[a:Usual Suspects, The, b:4.517106001121705]
==&gt;[a:Schindler's List, b:4.510416666666667]
==&gt;[a:Wrong Trousers, The, b:4.507936507936508]
==&gt;[a:Sunset Blvd. (a.k.a. Sunset Boulevard), b:4.491489361702127]
==&gt;[a:Raiders of the Lost Ark, b:4.47772]
&lt;/source&gt;

=== Declarative Pattern Matching Traversals===

Gremlin supports declarative graph pattern matching similar to [[SPARQL]]. For instance, the following query below uses Gremlin's ''match()''-step.

{{pull quote|What 80's action movies do 30-something programmers like? Group count the movies by their name and sort the group count map in decreasing order by value.  Clip the map to the top 10 and emit the map entries.}}

&lt;source lang=&quot;groovy&quot;&gt;
gremlin&gt; g.V().
           match(
             __.as('a').hasLabel('movie'),     
             __.as('a').out('category').has('name','Action'),
             __.as('a').has('year',between(1980,1990)),
             __.as('a').inE('rated').as('b'),
             __.as('b').has('stars',5),
             __.as('b').outV().as('c'),
             __.as('c').out('occupation').has('name','programmer'),
             __.as('c').has('age',between(30,40))).
           select('a').groupCount().by('name').
           order(local).by(valueDecr).
           limit(local,10)
==&gt;Raiders of the Lost Ark=26
==&gt;Star Wars Episode V - The Empire Strikes Back=26
==&gt;Terminator, The=23
==&gt;Star Wars Episode VI - Return of the Jedi=22
==&gt;Princess Bride, The=19
==&gt;Aliens=18
==&gt;Boat, The (Das Boot)=11
==&gt;Indiana Jones and the Last Crusade=11
==&gt;Star Trek The Wrath of Khan=10
==&gt;Abyss, The=9
&lt;/source&gt;

===OLAP Traversal===

{{pull quote|Which movies are most central in the ''implicit'' 5-stars graph?}}

&lt;source lang=&quot;groovy&quot;&gt;
gremlin&gt; g = graph.traversal(computer(SparkGraphComputer))
==&gt;graphtraversalsource[hadoopgraph[gryoinputformat-&gt;gryooutputformat], sparkgraphcomputer]
gremlin&gt; g.V().repeat(outE('rated').has('stars', 5).inV().
                 groupCount('m').by('name').
                 inE('rated').has('stars', 5).outV()).
               times(4).cap('m')
==&gt;Star Wars Episode IV - A New Hope	  35405394353105332
==&gt;American Beauty	  31943228282020585
==&gt;Raiders of the Lost Ark	31224779793238499
==&gt;Star Wars Episode V - The Empire Strikes Back  30434677119726223
==&gt;Godfather, The	30258518523013057
==&gt;Shawshank Redemption, The	28297717387901031
==&gt;Schindler's List	27539336654199309
==&gt;Silence of the Lambs, The	26736276376806173
==&gt;Fargo	 26531050311325270
==&gt;Matrix, The	 26395118239203191
&lt;/source&gt;

==Gremlin Graph Traversal Machine==

Gremlin is a [[virtual machine]] composed of an [[instruction set]] as well as an execution engine. An analogy is drawn between Gremlin and [[Java_(programming_language)|Java]].

{| class=&quot;wikitable&quot;
|-
! Java Ecosystem
! Gremlin Ecosystem
|-
| [[Groovy_(programming_language)|Groovy programming language]]
| Gremlin-Groovy
|-
| [[Scala_(programming_language)|Scala programming language]]
| Gremlin-Scala
|-
| [[Clojure_(programming_language)|Clojure programming language]]
| Gremlin-Clojure
|-
| ...
| ...
|-
| [[Java_(programming_language)|Java programming language]]
| Gremlin-Java8
|-
| Java instruction set
| Gremlin step library
|-
| [[Java virtual machine]]
| Gremlin traversal machine
|}

===Gremlin Steps (Instruction Set)===

The following traversal is a Gremlin traversal in the Gremlin-Java8 dialect.

&lt;source lang=&quot;java&quot;&gt;
g.V().as(&quot;a&quot;).out(&quot;knows&quot;).as(&quot;b&quot;).
  select(&quot;a&quot;,&quot;b&quot;).
    by(&quot;name&quot;).
    by(&quot;age&quot;)
&lt;/source&gt;
The Gremlin language (i.e. the [[Fluent_interface|fluent-style]] of expressing a graph traversal) can be represented in any host language that supports [[function composition]] and [[Nested_function|function nesting]]. Due to this simple requirement, there exists various Gremlin dialects including Gremlin-Groovy, Gremlin-Scala, Gremlin-Clojure, etc. The above Gremlin-Java8 traversal is ultimately compiled down to a step sequence called a ''traversal''. A string representation of the traversal above provided below.
&lt;source lang=&quot;java&quot;&gt;
[GraphStep([],vertex)@[a], VertexStep(OUT,[knows],vertex)@[b], SelectStep([a, b],[value(name), value(age)])]
&lt;/source&gt;
The ''steps'' are the primitives of the Gremlin graph traversal machine. They are the parameterized instructions that the machine ultimately executes. The Gremlin [[instruction set]] is approximately 30 steps. These steps are sufficient to provide general purpose computing and what is typically required to express the common motifs of any graph traversal query.

Given that Gremlin is a language, an instruction set, and a virtual machine, it is possible to design another traversal language that compiles to the Gremlin traversal machine (analogous to how Scala compiles to the [[Java virtual machine|JVM]]). For instance, the popular [[SPARQL]] graph pattern match language can be compiled to execute on the Gremlin machine. The following SPARQL query
&lt;source lang=&quot;sparql&quot;&gt;
SELECT ?a ?b ?c 
WHERE {
  ?a a Person .
  ?a ex:knows ?b .
  ?a ex:created ?c .
  ?b ex:created ?c .
  ?b ex:age ? d .
    FILTER(?d &lt; 30)
}
&lt;/source&gt;
would compile to
&lt;source lang=&quot;java&quot;&gt;
[GraphStep([],vertex), MatchStep(AND,[[MatchStartStep(a), LabelStep, IsStep(eq(Person)), MatchEndStep], [MatchStartStep(a), VertexStep(OUT,[knows],vertex), MatchEndStep(b)], [MatchStartStep(a), VertexStep(OUT,[created],vertex), MatchEndStep(c)], [MatchStartStep(b), VertexStep(OUT,[created],vertex), MatchEndStep(c)], [MatchStartStep(b), PropertiesStep([age],value), MatchEndStep(d)], [MatchStartStep(d), IsStep(gt(30)), MatchEndStep]]), SelectStep([a, b, c])].
&lt;/source&gt;
In Gremlin-Java8, the SPARQL query above would be represented as below and compile to the identical Gremlin step sequence (i.e. traversal).
&lt;source lang=&quot;java&quot;&gt;
g.V().match(
  as(&quot;a&quot;).label().is(&quot;person&quot;),
  as(&quot;a&quot;).out(&quot;knows&quot;).as(&quot;b&quot;),
  as(&quot;a&quot;).out(&quot;created&quot;).as(&quot;c&quot;),
  as(&quot;b&quot;).out(&quot;created&quot;).as(&quot;c&quot;),
  as(&quot;b&quot;).values(&quot;age&quot;).as(&quot;d&quot;),
  as(&quot;d&quot;).is(gt(30))).
    select(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)
&lt;/source&gt;

===Gremlin Machine (Virtual Machine)===

The Gremlin graph traversal machine can execute on a single machine or across a multi-machine compute cluster. Execution agnosticism allows Gremlin to run over both [[graph databases]] (OLTP) and graph processors (OLAP).

==References==
{{reflist}}

==External links==
# [http://www.tinkerpop.com Apache TinkerPop Homepage]
# [http://gremlindocs.com/ GremlinDocs.com (TinkerPop2)]
# [http://sql2gremlin.com/ sql2gremlin.com (TinkerPop2)]
# Rodriguez, M.A., &quot;[http://arxiv.org/abs/1508.03843 The Gremlin Graph Traversal Machine and Language],&quot; Proceedings of the ACM Database Programming Languages Conference, October, 2015.

{{Apache}}

[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation]]
[[Category:Software using the Apache license]]
[[Category:Java platform]]
[[Category:Free software programmed in Scala]]</text>
      <sha1>22xd8mm22t5eonu45o6sa4n7li22amg</sha1>
    </revision>
  </page>
  <page>
    <title>Jumbune</title>
    <ns>0</ns>
    <id>42732975</id>
    <revision>
      <id>705250090</id>
      <parentid>685702323</parentid>
      <timestamp>2016-02-16T11:20:33Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor/>
      <comment>/* top */Removed the extra frame from the infobox image, removed redundant caption; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3536">{{Multiple issues|
{{Orphan|date=September 2015}}
{{refimprove|date=September 2015}}
}}

{{Infobox software
| name                   = Jumbune
| logo                   = Jumbune Logo.png
| logo size              = 200px
| developer              = [[Impetus Technologies]]
| status                 = Active
| latest release version = 1.5.0 &lt;ref name=&quot;Hadoop Releases&quot;&gt;{{cite                                                                  web|url=http://jumbune.org/documents/Release%20Notes.pdf |title=Jumbune Releases |publisher=jumbune.org }}&lt;/ref&gt;
| operating system       = [[Linux]]
| programming language   = [[Java (programming language)|Java]]
| website                = {{URL|http://jumbune.org/}}
}}

'''Jumbune''' [juhm-b-yoon] is an [[open source|open-source]] multi-module framework intended for Hadoop MapReduce developers and cluster administrators. It is designed to monitor, profile, analyse and validate the Hadoop job and cluster.

Jumbune provides complete view of whole Hadoop cluster and jobs at one go. This framework comprises various modules &lt;ref name=&quot;Jumbune Overview&quot;&gt;{{cite                                                                  web|url=http://jumbune.org/documents/Quick%20Start%20Guide.pdf |title=Jumbune Overview |publisher=jumbune.org }}&lt;/ref&gt;

*''Cluster Monitor'' -Ad-hoc monitoring hundreds of Hadoop and system parameters even on a heavily loaded production cluster from anywhere in the world, on any device with a browser. 
*''Job Profiler'' -Profiles the various phases of MapReduce job execution with fine-grained node level analysis and a cluster level resource consumption view, all of which can be used for optimizing the job or the cluster.
*''Data Validation'' -Discrepancy and fault detection on the HDFS data in a cost and time effective manner using various distributed fault detection algorithms.
*''Debugger'' -Detailed data flow analysis for the various MapReduce code levels, which helps the developer to fine-tune the algorithm and detect faults.

==Prominent Users==

===Hadoop Cluster Administrator===

It helps the admins to monitor the cluster with node system-level fine grained statistics. They have the option to mark frequently monitored stats as favorites and fetch refreshed results at specified interval of time. Selection of trends for specific stats is also available.

===Hadoop Mapreduce Developer===

It's a simple and easy utility that helps MapReduce developers, to find discrepancies and errors in the HDFS data. User may check for data violations under various categories like: data type, null values or regular expression values. It provides developers to code level control flow statistics of MapReduce job. User may apply regex validations or its own user defined validation classes. As per the validations applied, Jumbune’s flow debugger checks the flow of &lt;key,value&gt; data for mapper and reducer respectively. It enables the developers to understand which resources are creating a bottleneck and the MapReduce phases thatare consuming more time than estimated and requires optimization to achieve an efficient job performance.

==See also==
* [[Big data]]
* [[Data Intensive Computing]]
* [[Apache Hadoop]] – Distributive computation framework

==References==
{{Reflist|1}}

==External links==
*{{Official website|jumbune.org|name=Jumbune Homepage}}
*[http://github.com/impetus-opensource/jumbune Source code]

[[Category:Hadoop]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Hadoop cluster administration]]</text>
      <sha1>l6jlzik47ho095hyi1vipc2qyy2d4e8</sha1>
    </revision>
  </page>
  <page>
    <title>Apache SystemML</title>
    <ns>0</ns>
    <id>49338480</id>
    <revision>
      <id>710593225</id>
      <parentid>706279888</parentid>
      <timestamp>2016-03-17T22:14:28Z</timestamp>
      <contributor>
        <username>Kri</username>
        <id>253188</id>
      </contributor>
      <comment>New section: See also + added [[Comparison of datasets in machine learning]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4212">{{Infobox Software
| name = Apache SystemML (incubating)
| logo = [[File:Apache SystemML logo.png|frameless|Logo for the Apache SystemML (incubating) project.]]
| author = [[Shivakumar Vaithyanathan]]
| developer = [[Apache Software Foundation]], [[IBM]]
| status = Active
| released = {{Start date and age|2015|11|02}}
| operating system = [[Linux]], [[Mac OS]], [[Windows]]
| programming language = [[Java (programming language)|Java]]
| genre = [[Machine Learning]]
| license = [[Apache License]] 2.0 
| website = {{URL|http://systemml.apache.org/}}
}}

'''Apache SystemML (incubating)''' is a flexible machine learning system that automatically scales to [[Apache Spark|Spark]] and [[Apache Hadoop|Hadoop]] clusters. SystemML's
distinguishing characteristics are:

# Algorithm customizability via R-like and Python-like languages.
# Multiple execution modes, including Standalone, [[Apache Spark|Spark]] Batch, [[Apache Spark|Spark]] MLContext, [[Apache Hadoop|Hadoop]] Batch, and JMLC.
# Automatic optimization based on data and cluster characteristics to ensure both efficiency and scalability.

==History==
SystemML was created in 2010 by researchers at the [[IBM Almaden Research Center]] led by IBM Fellow Shivakumar Vaithyanathan. It was observed that data scientists would write machine learning algorithms in languages such as [[R (programming language)|R]] and [[Python (programming language)|Python]] for small data. When it came time to scale to big data, a systems programmer would be needed to scale the algorithm in a language such as [[Scala (programming language)|Scala]]. This process typically involved days or weeks per iteration, and errors would occur translating the algorithms to operate on big data. SystemML seeks to simplify this process. A primary goal of SystemML is to automatically scale an algorithm written in an R-like or Python-like language to operate on big data, generating the same answer without the error-prone, multi-iterative translation approach.

On June 15, 2015, at the Spark Summit in San Francisco, Beth Smith, General Manager of IBM Analytics, announced that IBM was open-sourcing SystemML as part of IBM's major commitment to [[Apache Spark]] and Spark-related projects. SystemML became publicly available on [[GitHub]] on August 27, 2015 and became an [[Apache Incubator]] project on November 2, 2015.

==External links==
*[http://systemml.apache.org/ Apache SystemML (incubating) website]
*[http://researcher.watson.ibm.com/researcher/view_group.php?id=3174 IBM Research - SystemML]
*[http://www.spark.tc/q-a-with-shiv-vaithyanathan-creator-of-systemml-and-ibm-fellow/ Q &amp; A with Shiv Vaithyanathan, Creator of SystemML and IBM Fellow]
*[http://www.spark.tc/a-universal-translator-for-big-data-and-machine-learning/ A Universal Translator for Big Data and Machine Learning]
*[https://www.youtube.com/watch?v=WkYqjWL1xzk SystemML: Declarative Machine Learning at Scale presentation by Fred Reiss]
*[http://researcher.watson.ibm.com/researcher/files/us-ytian/systemML.pdf SystemML: Declarative Machine Learning on MapReduce]
*[http://www.vldb.org/pvldb/vol7/p553-boehm.pdf Hybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML]
*[http://sites.computer.org/debull/A14sept/p52.pdf SystemML’s Optimizer: Plan Generation for Large-Scale Machine Learning Programs]
*[http://www.zdnet.com/article/ibms-systemml-machine-learning-system-becomes-apache-incubator-project/ IBM's SystemML machine learning system becomes Apache Incubator project]
*[http://www.theinquirer.net/inquirer/news/2413132/ibm-donates-machine-learning-tech-to-apache-spark-open-source-community IBM donates machine learning tech to Apache Spark open source community]
*[http://www.eweek.com/developer/ibms-systemml-moves-forward-as-apache-incubator-project.html IBM's SystemML Moves Forward as Apache Incubator Project]

==See also==
*[[Comparison of deep learning software]]

{{Apache}}

[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Hadoop]]
[[Category:Spark]]
[[Category:Apache Software Foundation|SystemML]]
[[Category:Software using the Apache license]]
[[Category:Java platform]]
[[Category:Big data products]]</text>
      <sha1>rjeg5qwcjeoow31lr56dwdds0p8fdhr</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Ambari</title>
    <ns>0</ns>
    <id>47478005</id>
    <revision>
      <id>706279206</id>
      <parentid>700627099</parentid>
      <timestamp>2016-02-22T12:56:19Z</timestamp>
      <contributor>
        <username>演歌ビニール</username>
        <id>23956730</id>
      </contributor>
      <minor/>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2102">{{Infobox software
| name                   = Ambari
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 2.2.0
| latest release date    = {{release date|2015|12|19}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| platform               =
| programming language   = 
| genre                  = [[Distributed computing]]
| license                = [[Apache License]] 2.0
| website                = {{url|http://Ambari.apache.org}}
}}

'''Apache Ambari''' 
is a software project of the [[Apache Software Foundation]], is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs. Ambari was a sub-project of [[Hadoop]] but is now a [[Apache_Software_Foundation#Projects|top-level]] project in its own right.

Ambari is used by companies including [[Cardinal_Health]], [[EBay]], [[Expedia]], [[Kayak]], [[Lending club]], [[Neustar]], [[Pandora]], [[Priceline.com]], [[Samsung]], [[Shutterfly]], [[Spotify]]. {{cn|date=December 2015}} 

==Overview==

Ambari enables system administrators to provision, manage and monitor a Hadoop cluster, and also to integrate Hadoop with the existing enterprise infrastructure.

* Provision a Hadoop Cluster
* Manage a Hadoop cluster
* Monitor a Hadoop cluster
* Integrate Hadoop with the Enterprise

Hadoop cluster provisioning and ongoing management can be a complicated task, especially when there are hundreds or thousands of hosts involved. 
Ambari provides a single control point for viewing, updating and managing Hadoop service life cycles.


==See also==
{{Portal|Java}}
* [[Hadoop]]

==References==
{{Reflist}}

==External links==
*{{Official website|http://Ambari.apache.org}}

{{Apache}}

{{DEFAULTSORT:Apache Ambari}}
[[Category:Apache Software Foundation|Ambari]]
[[Category:Configuration management]]
[[Category:Hadoop]]</text>
      <sha1>rignpzuvkylsggaq184d8l6zd6h5wej</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Hadoop</title>
    <ns>0</ns>
    <id>5919308</id>
    <revision>
      <id>723454971</id>
      <parentid>722743426</parentid>
      <timestamp>2016-06-03T04:25:12Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <comment>[[User:Green Cardamom/WaybackMedic|WaybackMedic]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="55603">{{multiple issues|
{{advert|date=October 2013}}
{{buzzword|date=October 2013}}
}}
{{Infobox software
| name                   = Apache Hadoop
| logo                   = [[File:Hadoop logo.svg|frameless|Hadoop Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| released              = {{Start date and age|2011|12|10}}&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/releases.html#27+December%2C+2011%3A+release+1.0.0+available |title=Hadoop Releases &lt;!-- |author=Staff writer(s); no by-line.--&gt; |website=apache.org |publisher=Apache Software Foundation |accessdate=2014-12-06}}&lt;/ref&gt;
| latest release version = 2.7.2
| latest release date    = {{release date|2016|01|25}}&lt;ref name=&quot;Hadoop Releases&quot;&gt;{{cite web|url=http://hadoop.apache.org/releases.html |title=Hadoop Releases |publisher=Hadoop.apache.org |accessdate=2016-01-31}}&lt;/ref&gt;
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Distributed file system]]
| posix compliant        = Not [[POSIX]]-compliant
| license                = [[Apache License]] 2.0
| website                = {{Official URL}}
}}

'''Apache Hadoop''' ({{IPAc-en|pron|h|ə|ˈ|d|u:|p}}) is an [[open source|open-source]] [[software framework]] for [[Clustered file system|distributed storage]] and [[distributed processing]] of very large data sets on [[computer cluster]]s built from [[commodity hardware]]. All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework.&lt;ref name=&quot;homepage&quot;&gt;{{Cite web|title= Welcome to Apache Hadoop!|url= http://hadoop.apache.org|website= hadoop.apache.org|accessdate = 2015-12-16}}&lt;/ref&gt;

The core of Apache Hadoop consists of a storage part, known as [[#HDFS|Hadoop Distributed File System]] (HDFS), and a processing part called [[MapReduce]]. Hadoop splits files into large blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers [[JAR (file format)|packaged code]] for nodes to process in parallel based on the data that needs to be processed. This approach takes advantage of [[data locality]]&lt;ref&gt;{{cite web |url=http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ |title=What is the Hadoop Distributed File System (HDFS)? &lt;!-- |author

=Staff writer(s); no by-line.--&gt; |website=ibm.com |publisher=[[IBM]] |accessdate=2014-10-30 }}&lt;/ref&gt;— nodes manipulating the data they have access to— to allow the dataset to be [[distributed processing|processed]] faster and more efficiently than it would be in a more conventional [[supercomputer architecture]] that relies on a [[parallel file system]] where computation and data are distributed via high-speed networking.&lt;ref&gt;{{cite web |url=http://www.datascienceassn.org/content/data-locality-hpc-vs-hadoop-vs-spark |title=Data Locality: HPC vs. Hadoop vs. Spark |last1=Malak |first1=Michael |date=2014-09-19 |website=datascienceassn.org |publisher=Data Science Association |accessdate=2014-10-30 }}&lt;/ref&gt;

The base Apache Hadoop framework is composed of the following modules:
*''Hadoop Common'' – contains libraries and utilities needed by other Hadoop modules;
*''Hadoop Distributed File System (HDFS)'' – a distributed file-system that stores data on commodity machines, providing very high aggregate bandwidth across the cluster;
*''Hadoop YARN'' – a resource-management platform responsible for managing computing resources in clusters and using them for scheduling of users' applications;&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/yarn/api/records/Resource.html#newInstance(int,%20int) |title=Resource (Apache Hadoop Main 2.5.1 API) &lt;!-- |author=Staff writer(s); no by-line.--&gt; |date=2014-09-12 |website=apache.org |publisher=Apache Software Foundation |accessdate=2014-09-30 }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/ |title=Apache Hadoop YARN – Concepts and Applications |last1=Murthy |first1=Arun |date=2012-08-15 |website=hortonworks.com |publisher=Hortonworks |accessdate=2014-09-30 }}&lt;/ref&gt; and
*''Hadoop MapReduce'' – an implementation of the [[MapReduce]] programming model for large scale data processing.

The term ''Hadoop'' has come to refer not just to the base modules above, but also to the ''ecosystem'',&lt;ref&gt;{{cite web |url=http://finance.yahoo.com/news/continuuity-raises-10-million-series-120500471.html |title=Continuuity Raises $10 Million Series A Round to Ignite Big Data Application Development Within the Hadoop Ecosystem &lt;!-- |author=Staff writer(s); no by-line.--&gt; |date=2012-11-14 |website=finance.yahoo.com |publisher=[[Marketwired]] |accessdate=2014-10-30 }}&lt;/ref&gt; or collection of additional software packages that can be installed on top of or alongside Hadoop, such as [[Pig (programming tool)|Apache Pig]], [[Apache Hive]], [[Apache HBase]], [[Apache Phoenix]], [[Apache Spark]], [[Apache ZooKeeper]], [[Cloudera Impala]], [[Apache Flume]], [[Apache Sqoop]], [[Apache Oozie]], [[Apache Storm]].&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/ |title=Hadoop-related projects at |publisher=Hadoop.apache.org |accessdate=2013-10-17 }}&lt;/ref&gt;

Apache Hadoop's MapReduce and HDFS components were inspired by [[Google]] papers on their [[MapReduce]] and [[Google File System]].&lt;ref&gt;{{cite book &lt;!-- |author=Staff writer(s); no by-line.--&gt; |title=Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data |url=https://books.google.com/books?id=axruBQAAQBAJ&amp;pg=PA300|publisher=John Wiley &amp; Sons |page=300 |date=2014-12-19 |isbn=9781118876220 |access-date=2015-01-29 }}&lt;/ref&gt;

The Hadoop framework itself is mostly written in the [[Java (programming language)|Java programming language]], with some native code in [[C (programming language)|C]] and [[Command-line interface|command line]] utilities written as [[shell scripts]]. Though MapReduce Java code is common, any programming language can be used with &quot;Hadoop Streaming&quot; to implement the &quot;map&quot; and &quot;reduce&quot; parts of the user's program.&lt;ref&gt;{{cite web |url=http://www.mail-archive.com/nlpatumd@yahoogroups.com/msg00570.html |title=[nlpatumd&amp;#93; Adventures with Hadoop and Perl |publisher=Mail-archive.com |date=2010-05-02 |accessdate=2013-04-05 }}&lt;/ref&gt; Other projects in the Hadoop ecosystem expose richer user interfaces.

==History==
The genesis of Hadoop came from the Google File System paper&lt;ref&gt;{{cite web|url=http://research.google.com/archive/gfs.html|title=The Google File System|first1=Sanjay|last1=Ghemawat|first2=Howard|last2=Gobioff|first3=Shun-Tak|last3=Leung}}&lt;/ref&gt; that was published in October 2003. This paper spawned another research paper from Google - MapReduce: Simplified Data Processing on Large Clusters.&lt;ref&gt;{{cite web|url=http://research.google.com/archive/mapreduce.html|title=MapReduce: Simplified Data Processing on Large Clusters|first1=Jeffrey|last1=Dean|first2=Sanjay|last2=Ghemawat}}&lt;/ref&gt; Development started in the [[Apache Nutch]] project, but was moved to the new Hadoop subproject in January 2006.&lt;ref&gt;{{cite web|url=https://issues.apache.org/jira/browse/INFRA-700|title=new mailing lists request: hadoop|last=Cutting|first=Doug|date=28 Jan 2006|website=issues.apache.org|quote=The Lucene PMC has voted to split part of Nutch into a new sub-project named Hadoop}}&lt;/ref&gt; [[Doug Cutting]], who was working at [[Yahoo!]] at the time,&lt;ref&gt;{{cite web|url=https://intellipaat.com/blog/hadoop-creator-goes-to-cloudera/|title=Hadoop Creator goes to Cloudera|author=Intellipaat|work=Intellipaat Blog|accessdate=2 February 2016}}&lt;/ref&gt; named it after his son's toy elephant.&lt;ref&gt;{{cite news |title=Hadoop, a Free Software Program, Finds Uses Beyond Search |first=Ashlee |last=Vance |newspaper=The New York Times |date=2009-03-17 |url=http://www.nytimes.com/2009/03/17/technology/business-computing/17cloud.html |accessdate=2010-01-20 | archiveurl= https://web.archive.org/web/20110830130350/http://www.nytimes.com/2009/03/17/technology/business-computing/17cloud.html|archivedate=August 30, 2011&lt;!--DASHBot--&gt;| deadurl=no }}&lt;/ref&gt; The initial code that was factored out of Nutch consisted of 5k lines of code for NDFS and 6k lines of code for MapReduce.

The first committer added to the Hadoop project was Owen O’Malley in March 2006.&lt;ref&gt;{{cite mailing list|first=Doug|last=Cutting|author-link=Doug Cutting|title=[RESULT] VOTE: add Owen O'Malley as Hadoop committer|mailing-list=hadoop-common-dev|date=30 March 2006|url=http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/200603.mbox/%3C442B27A6.8080500@apache.org%3E}}&lt;/ref&gt; Hadoop 0.1.0 was released in April 2006 &lt;ref&gt;{{cite web|url=https://archive.apache.org/dist/hadoop/core/|title=archive.apache.org}}&lt;/ref&gt; and continues to evolve by the many contributors&lt;ref&gt;{{cite web|url=https://hadoop.apache.org/who.html|title=Apache Hadoop Project Members}}&lt;/ref&gt; to the Apache Hadoop project.

==Timeline==
{| class=&quot;wikitable&quot; width=&quot;100%&quot;
! style=&quot;width:6%&quot; | Year || style=&quot;width:10%&quot; | Month || Event || Ref.
|-
|2003||October||Google File System paper released ||&lt;ref&gt;
{{cite web 
| title = Google Research Publication: The Google File System 
| url = http://research.google.com/archive/gfs.html 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2004||December||MapReduce: Simplified Data Processing on Large Clusters||&lt;ref&gt;
{{cite web 
| title = Google Research Publication: MapReduce 
| url = http://research.google.com/archive/mapreduce.html 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2006||January||Hadoop subproject created with mailing lists, jira, and wiki||&lt;ref&gt;
{{cite web 
| title = [INFRA-700] new mailing lists request: hadoop - ASF JIRA 
| url = https://issues.apache.org/jira/browse/INFRA-700 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2006||January||Hadoop is born from Nutch 197 ||&lt;ref&gt;
{{cite web 
| title = [HADOOP-1] initial import of code from Nutch - ASF JIRA 
| url = https://issues.apache.org/jira/browse/HADOOP-1 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2006||February||NDFS+ MapReduce moved out of Apache Nutch to create Hadoop||&lt;ref name=&quot;tom-white-book&quot;&gt;{{cite book
| last =White
| first =Tom
| title =Hadoop: The Definitive Guide
| publisher =O'Reilly
| edition =3rd
| date =2012
| isbn = 9781449328917}}&lt;/ref&gt;
|-
|2006||February||Owen Omalley's first patch goes into Hadoop||&lt;ref&gt;
{{cite web 
| title = [NUTCH-197] NullPointerException in TaskRunner if application jar does not have &quot;lib&quot; directory - ASF JIRA 
| url = https://issues.apache.org/jira/browse/NUTCH-197 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2006||February||Hadoop is named after Cutting's son's yellow plush toy||&lt;ref name=&quot;datanami.com&quot;&gt;{{cite web 
| title = From Spiders to Elephants: The History of Hadoop 
| url = http://www.datanami.com/2015/04/15/from-spiders-to-elephants-the-history-of-hadoop/ 
| accessdate = 2016-03-09 
 }}&lt;/ref&gt;
|-
|2006||April||Hadoop 0.1.0 released||&lt;ref&gt;
{{cite web 
| title = Index of /dist/hadoop/core 
| url = https://archive.apache.org/dist/hadoop/core/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2006||April||Hadoop sorts 1.8TB on 188 nodes in 47.9 hours||&lt;ref name=&quot;tom-white-book&quot; /&gt;
|-
|2006||May||Yahoo deploys 300 machine Hadoop cluster||&lt;ref name=&quot;tom-white-book&quot; /&gt;
|-
|2006||October||Yahoo Hadoop cluster reaches 600 machines||&lt;ref name=&quot;tom-white-book&quot; /&gt;
|-
|2007||April||Yahoo runs 2 clusters of 1,000 machines||&lt;ref name=&quot;tom-white-book&quot; /&gt;
|-
|2007||June||Only 3 companies on &quot;Powered by Hadoop Page&quot;||&lt;ref name=&quot;powered-by-hadoop&quot;&gt;
{{cite web 
| title = Hadoop Summit 2009 
| url = http://riccomini.name/posts/hadoop/2009-06-13-hadoop-summit-2009/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2007||October||First release of Hadoop that includes HBase||&lt;ref&gt;
{{cite web 
| title = Apache Hadoop Releases 
| url = http://hadoop.apache.org/releases.html 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2007||October||Yahoo Labs creates Pig, and donates it to the ASF||&lt;ref&gt;{{cite book
| last =Gates
| first =Alan
| title =Programming Pig
| publisher =O'Reilly
| date =2011
| page =10
| isbn = 978-1-4493-0264-1}}&lt;/ref&gt;
|-
|2008||January||YARN JIRA opened||Yarn Jira (Mapreduce 279)
|-
|2008||January||20 companies on &quot;Powered by Hadoop Page&quot;||&lt;ref name=&quot;powered-by-hadoop&quot; /&gt;
|-
|2008||February||Yahoo moves its web index onto Hadoop||&lt;ref&gt;
{{cite web 
| title = Yahoo! Launches World’s Largest Hadoop Production Application | work=hadoopnew - Yahoo 
| url = https://developer.yahoo.com/blogs/hadoop/yahoo-launches-world-largest-hadoop-production-application-398.html 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2008||February||Yahoo! production search index generated by a 10,000-core Hadoop cluster||&lt;ref name=&quot;tom-white-book&quot; /&gt;
|-
|2008||March||First Hadoop Summit||&lt;ref&gt;
{{cite web 
| title = RE: Hadoop summit / workshop at Yahoo! 
| url = http://mail-archives.apache.org/mod_mbox/hadoop-common-user/200802.mbox/%3CDD27C9769EA63D43BE3CF4CCCC12DD977ED49E@SNV-EXVS02.ds.corp.yahoo.com%3E 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2008||April||Hadoop world record fastest system to sort a terabyte of data. Running on a 910-node cluster, Hadoop sorted one terabyte in 209 seconds||&lt;ref name=&quot;tom-white-book&quot; /&gt;
|-
|2008||May||Hadoop wins TeraByte Sort (World Record sortbenchmark.org)||&lt;ref&gt;http://sortbenchmark.org/YahooHadoop.pdf&lt;/ref&gt;
|-
|2008||July||Hadoop wins Terabyte Sort Benchmark||&lt;ref&gt;
{{cite web 
| title = Apache Hadoop Wins Terabyte Sort Benchmark | work=hadoopnew - Yahoo 
| url = https://developer.yahoo.com/blogs/hadoop/apache-hadoop-wins-terabyte-sort-benchmark-408.html 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2008||October||Loading 10TB/day in Yahoo clusters||&lt;ref name=&quot;tom-white-book&quot; /&gt;
|-
|2008||October||Cloudera, Hadoop distributor is founded||&lt;ref&gt;
{{cite web 
| title = Cloudera
| url = https://www.crunchbase.com/organization/cloudera#/entity 
| accessdate = 2016-03-09 
 }}&lt;/ref&gt;
|-
|2008||November||Google MapReduce implementation sorted one terabyte in 68 seconds||&lt;ref name=&quot;tom-white-book&quot; /&gt;
|-
|2009||March||Yahoo runs 17 clusters with 24,000 machines||&lt;ref name=&quot;tom-white-book&quot; /&gt;
|-
|2009||April||Hadoop sorts a petabyte||&lt;ref&gt;http://sortbenchmark.org/Yahoo2009.pdf&lt;/ref&gt;
|-
|2009||May||Yahoo! used Hadoop to sort one terabyte in 62 seconds||&lt;ref name=&quot;tom-white-book&quot; /&gt;
|-
|2009||June||Second Hadoop Summit||&lt;ref&gt;http://www.mollynix.com/images_content/01commdes/hadoopschedulepdf.pdf&lt;/ref&gt;
|-
|2009||June||Second Hadoop Summit||&lt;ref&gt;
{{cite web 
| title = Hadoop Summit 2009 Tickets, Wed, Jun 10, 2009 at 8:00 AM
| url = http://www.eventbrite.com/e/hadoop-summit-2009-tickets-338131360 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2009||July||Hadoop Core is renamed Hadoop Common||&lt;ref name=&quot;hadoop-apache-org&quot;&gt;
{{cite web 
| title = Welcome to Apache™ Hadoop®! 
| url = http://hadoop.apache.org/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2009||July||MapR, Hadoop distributor founded||&lt;ref&gt;
{{cite web 
| title = MapR Technologies
| url = https://www.crunchbase.com/organization/mapr-technologies#/entity 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2009||July||HDFS now a separate subproject||&lt;ref name=&quot;hadoop-apache-org&quot; /&gt;
|-
|2009||July||MapReduce now a separate subproject||&lt;ref name=&quot;hadoop-apache-org&quot; /&gt;
|-
|2010||January||Kerberos support added to Hadoop||&lt;ref&gt;{{cite web |url=https://thinkbiganalytics.com/yahoo-updates-from-hadoop-summit-2010/ |title=Yahoo! Updates from Hadoop Summit 2010 |publisher=Think Big Analytics |accessdate=April 25, 2016 |quote=Baldeschwieler announced that Yahoo has released a beta test of Hadoop Security, which uses Kerberos for authentication and allows colocation of business sensitive data within the same cluster.}}&lt;/ref&gt;
|-
|2010||May||Apache HBase Graduates||&lt;ref&gt;
{{cite web 
| title = Apache HBase – Apache HBase™ Home 
| url = http://hbase.apache.org/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2010||June||Third Hadoop Summit||&lt;ref&gt;
{{cite web 
| title = Hadoop Summit 2010 – Agenda is available! | work=hadoopnew - Yahoo 
| url = https://developer.yahoo.com/blogs/hadoop/hadoop-summit-2010-agenda-available-455.html 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2010||June||Yahoo 4,000 nodes/70 petabytes ||&lt;ref name=&quot;hadoop-summit-2010&quot;&gt;
{{cite web 
| title = Hadoop Summit 2010
| url = http://perspectives.mvdirona.com/2010/07/hadoop-summit-2010/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2010||June||Facebook 2,300 clusters/40 petabytes ||&lt;ref name=&quot;hadoop-summit-2010&quot; /&gt;
|-
|2010||September||Apache Hive Graduates||&lt;ref&gt;
{{cite web 
| title = Apache Hive TM 
| url = http://hive.apache.org/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2010||September||Apache Pig Graduates||&lt;ref&gt;
{{cite web 
| title = Welcome to Apache Pig! 
| url = http://pig.apache.org/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2011||January||Apache Zookeeper Graduates ||&lt;ref&gt;
{{cite web 
| title = Apache ZooKeeper - Home 
| url = http://zookeeper.apache.org/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2011||January||Facebook, LinkedIn, eBay and IBM collectively contribute 200,000 lines of code ||&lt;ref name=&quot;contributions-to-apache-hadoop&quot;&gt;
{{cite web 
| title = Reality Check: Contributions to Apache Hadoop - Hortonworks 
| url = http://hortonworks.com/blog/reality-check-contributions-to-apache-hadoop/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2011||March||Apache Hadoop takes top prize at Media Guardian Innovation Awards||&lt;ref&gt;
{{cite web 
| title = Apache Hadoop takes top prize at Media Guardian Innovation Awards |work= The Guardian 
| url = http://www.theguardian.com/technology/2011/mar/25/media-guardian-innovation-awards-apache-hadoop 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2011||June||Rob Beardon and Eric Badleschieler spin out Hortonworks out of Yahoo.||&lt;ref name=&quot;history-of-hadoop&quot;&gt;
{{cite web 
| title = The history of Hadoop: From 4 nodes to the future of data | publisher=Gigaom 
| url = https://gigaom.com/2013/03/04/the-history-of-hadoop-from-4-nodes-to-the-future-of-data/ 
| accessdate = 2016-03-09 
| first = Derrick 
| last = Harris 
 }}
&lt;/ref&gt;
|-
|2011||June||Yahoo has 42K Hadoop notes and hundreds of petabytes of storage||&lt;ref name=&quot;history-of-hadoop&quot; /&gt;
|-
|2011||June||Third Annual Hadoop Summit (1,700 attendees)||&lt;ref&gt;
{{cite web 
| title = Hadoop Summit 2011: June 29th, Santa Clara Convention Center | work= hadoopnew - Yahoo 
| url = https://developer.yahoo.com/blogs/hadoop/hadoop-summit-2011-june-29th-santa-clara-convention-5061.html 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2011||October||Debate over which company had contributed more to Hadoop.||&lt;ref name=&quot;contributions-to-apache-hadoop&quot; /&gt;
|-
|2012||January||Hadoop community moves to separate from MapReduce and replace with YARN||&lt;ref name=&quot;datanami.com&quot;/&gt;
|-
|2012||June||San Jose Hadoop Summit (2,100 attendees)||&lt;ref&gt;
{{cite web 
| title = Fifth Annual Hadoop Summit 2012 Kicks Off with Record Attendance - Hortonworks 
| url = http://hortonworks.com/press-releases/fifth-annual-hadoop-summit-2012-kicks-off-with-record-attendance/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2012||November||Apache Hadoop 1.0 Available||&lt;ref name=&quot;hadoop-apache-org&quot; /&gt;
|-
|2013||March||Hadoop Summit - Amsterdam (500 attendees)||&lt;ref&gt;
{{cite web 
| title = Hadoop Summit 2013 Amsterdam - It's A Wrap! - Hortonworks 
| url = http://hortonworks.com/blog/hadoop-summit-2013-amsterdam-its-a-wrap/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2013||March||YARN deployed in production at Yahoo||&lt;ref&gt;
{{cite web 
| title = Hadoop at Yahoo!: More Than Ever Before 
| url = https://developer.yahoo.com/blogs/ydn/hadoop-yahoo-more-ever-54421.html 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2013||June||San Jose Hadoop Summit (2,700 attendees)||&lt;ref&gt;
{{cite web 
| title = Hadoop Summit North America 2013 Draws Record Ecosystem Support | work= Business Wire 
| url = http://www.businesswire.com/news/home/20130610006449/en/Hadoop-Summit-North-America-2013-Draws-Record 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2013||October||Apache Hadoop 2.2 Available||&lt;ref name=&quot;hadoop-apache-org&quot; /&gt;
|-
|2014||February||Apache Hadoop 2.3 Available||&lt;ref name=&quot;hadoop-apache-org&quot; /&gt;
|-
|2014||February||Apache Spark top Level Apache Project||&lt;ref&gt;
{{cite web 
| title = The Apache Software Foundation Announces Apache™ Spark™ as a Top-Level Project : The Apache Software Foundation Blog 
| url = https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2014||April||Hadoop summit Amsterdam (750 attendees)||&lt;ref&gt;
{{cite web 
| title = Loved Hadoop Summit Europe 2014 - Hope you did too! - SAP HANA 
| url = https://blogs.saphana.com/2014/04/14/loved-hadoop-summit-europe-2014-hope-you-did-too/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2014||June||Apache Hadoop 2.4 Available||&lt;ref name=&quot;hadoop-apache-org&quot; /&gt;
|-
|2014||June||San Jose Hadoop Summit (3,200 attendees)||&lt;ref&gt;
{{cite web 
| title = Hadoop Summit 2014 – Big Data Keeps Getting Bigger |publisher=Pentaho 
| url = http://www.pentaho.com/blog/2014/06/06/hadoop-summit-2014-big-data-keeps-getting-bigger 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2014||August||Apache Hadoop 2.5 Available||&lt;ref name=&quot;hadoop-apache-org&quot; /&gt;
|-
|2014||November||Apache Hadoop 2.6 Available||&lt;ref name=&quot;hadoop-apache-org&quot; /&gt;
|-
|2015||April||Hadoop Summit Europe||&lt;ref&gt;
{{cite web 
| title = Hadoop Summit Europe 2015, 15th-16th April 2015 |publisher= Lanyrd 
| url = http://lanyrd.com/2015/hadoopsummit/ 
| accessdate = 2016-03-09 
 }}
&lt;/ref&gt;
|-
|2015||June||Apache Hadoop 2.7 Available||&lt;ref name=&quot;hadoop-apache-org&quot; /&gt;
|}

==Architecture==
{{See also|#Hadoop_distributed_file_system|Apache HBase|MapReduce|l1=Hadoop Distributed File System}}

Hadoop consists of the ''Hadoop Common'' package, which provides filesystem and OS level abstractions, a MapReduce engine (either MapReduce/MR1 or YARN/MR2)&lt;ref&gt;{{cite web |url=http://blog.cloudera.com/blog/2012/10/mr2-and-yarn-briefly-explained/ |title=MR2 and YARN Briefly Explained |first=Harsh |last=Chouraria |date=21 October 2012 |website=cloudera.com |publisher=[[Cloudera]] |accessdate=23 October 2013 }}&lt;/ref&gt; and the [[#Hadoop distributed file system|Hadoop Distributed File System]] (HDFS). The Hadoop Common package contains the necessary [[JAR (file format)|Java ARchive (JAR)]] files and scripts needed to start Hadoop.

For effective scheduling of work, every Hadoop-compatible file system should provide location awareness: the name of the rack (more precisely, of the network switch) where a worker node is. Hadoop applications can use this information to execute code on the node where the data is, and, failing that, on the same rack/switch to reduce backbone traffic. HDFS uses this method when replicating data for data redundancy across multiple racks. This approach reduces the impact of a rack power outage or switch failure; if one of these hardware failures occurs, the data will remain available.&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html |title=HDFS User Guide |publisher=Hadoop.apache.org |accessdate=2014-09-04 }}&lt;/ref&gt;
[[File:Hadoop 1.png|thumb|upright=1.2|right|alt=Hadoop cluster|A multi-node Hadoop cluster]]

A small Hadoop cluster includes a single master and multiple worker nodes. The master node consists of a Job Tracker, Task Tracker, NameNode, and DataNode. A slave or ''worker node'' acts as both a DataNode and TaskTracker, though it is possible to have data-only worker nodes and compute-only worker nodes. These are normally used only in nonstandard applications.&lt;ref name=&quot;michael-noll.com_2&quot;&gt;{{cite web |title=Running Hadoop on Ubuntu Linux System(Multi-Node Cluster) |url=http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ }}&lt;/ref&gt;

Hadoop requires [[JRE|Java Runtime Environment]] (JRE) 1.6 or higher. The standard startup and shutdown scripts require that [[Secure Shell]] (ssh) be set up between nodes in the cluster.&lt;ref name=&quot;michael-noll.com_1&quot;&gt;{{cite web |title=Running Hadoop on Ubuntu Linux (Single-Node Cluster) |url=http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/#prerequisites |accessdate=6 June 2013 }}&lt;/ref&gt;

In a larger cluster, HDFS nodes are managed through a dedicated NameNode server to host the file system index, and a secondary NameNode that can generate snapshots of the namenode's memory structures, thereby preventing file-system corruption and loss of data. Similarly, a standalone JobTracker server can manage job scheduling across nodes. When Hadoop MapReduce is used with an alternate file system, the NameNode, secondary NameNode, and DataNode architecture of HDFS are replaced by the file-system-specific equivalents.

===File systems===

===={{Anchor|HDFS}}Hadoop distributed file system====
The Hadoop distributed file system (HDFS) is a distributed, scalable, and portable file-system written in [[Java (software platform)|Java]] for the Hadoop framework. A Hadoop cluster has nominally a single namenode plus a cluster of datanodes, although [[redundancy (engineering)|redundancy]] options are available for the namenode due to its criticality. Each datanode serves up blocks of data over the network using a block protocol specific to HDFS. The file system uses [[TCP/IP]] [[Internet socket|sockets]] for communication. Clients use [[remote procedure call]] (RPC) to communicate between each other.

HDFS stores large files (typically in the range of gigabytes to terabytes&lt;ref&gt;
{{cite web |title=HDFS Architecture |url=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Large_Data_Sets |accessdate=1 September 2013 }}
&lt;/ref&gt;) across multiple machines. It achieves reliability by [[Replication (computer science)|replicating]] the data across multiple hosts, and hence theoretically does not require [[RAID]] storage on hosts (but to increase I/O performance some RAID configurations are still useful). With the default replication value, 3, data is stored on three nodes: two on the same rack, and one on a different rack. Data nodes can talk to each other to rebalance data, to move copies around, and to keep the replication of data high. HDFS is not fully [[POSIX]]-compliant, because the requirements for a POSIX file-system differ from the target goals for a Hadoop application. The trade-off of not having a fully POSIX-compliant file-system is increased performance for data [[throughput]] and support for non-POSIX operations such as Append.&lt;ref name=&quot;openlibrary1&quot;&gt;
{{Cite journal
|publisher = Amazon.com
|first = Yaniv |last = Pessach
|url = http://openlibrary.org/books/OL25423189M/Distributed_Storage_Concepts_Algorithms_and_Implementations
|title = Distributed Storage
|edition = Distributed Storage: Concepts, Algorithms, and Implementations
|publication-date = 2013
|postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}
}}&lt;/ref&gt;

HDFS added the high-availability capabilities, as announced for release 2.0 in May 2012,&lt;ref name=&quot;failover&quot;&gt;{{cite web |title=Version 2.0 provides for manual failover and they are working on automatic failover: |url=https://hadoop.apache.org/releases.html#23+May%2C+2012%3A+Release+2.0.0-alpha+available  |accessdate= 30 July 2013 |publisher=Hadoop.apache.org }}&lt;/ref&gt; letting the main metadata server (the NameNode) fail over manually to a backup. The project has also started developing automatic [[fail-over]].

The HDFS file system includes a so-called ''secondary namenode'', a misleading name that some might incorrectly interpret as a backup namenode for when the primary namenode goes offline. In fact, the secondary namenode regularly connects with the primary namenode and builds snapshots of the primary namenode's directory information, which the system then saves to local or remote directories. These checkpointed images can be used to restart a failed primary namenode without having to replay the entire journal of file-system actions, then to edit the log to create an up-to-date directory structure. Because the namenode is the single point for storage and management of metadata, it can become a bottleneck for supporting a huge number of files, especially a large number of small files. HDFS Federation, a new addition, aims to tackle this problem to a certain extent by allowing multiple namespaces served by separate namenodes. Moreover, there are some issues in HDFS, namely, small file issue, scalability problem, Single Point of Failure (SPoF), and bottleneck in huge metadata request. 
An advantage of using HDFS is data awareness between the job tracker and task tracker. The job tracker schedules map or reduce jobs to task trackers with an awareness of the data location. For example: if node A contains data (x,y,z) and node B contains data (a,b,c), the job tracker schedules node B to perform map or reduce tasks on (a,b,c) and node A would be scheduled to perform map or reduce tasks on (x,y,z). This reduces the amount of traffic that goes over the network and prevents unnecessary data transfer. When Hadoop is used with other file systems, this advantage is not always available. This can have a significant impact on job-completion times, which has been demonstrated when running data-intensive jobs.&lt;ref&gt;{{cite web |url=
http://www.eng.auburn.edu/~xqin/pubs/hcw10.pdf |format=PDF |title= Improving MapReduce performance through data placement in heterogeneous Hadoop Clusters |date=April 2010 |publisher=Eng.auburn.ed }}&lt;/ref&gt;

HDFS was designed for mostly immutable files&lt;ref name=&quot;openlibrary1&quot; /&gt; and may not be suitable for systems requiring concurrent write-operations.

HDFS can be [[Mount (computing)|mounted]] directly with a [[Filesystem in Userspace]] (FUSE) [[virtual file system]] on [[Linux]] and some other [[Unix]] systems.

File access can be achieved through the native Java [[application programming interface]] (API), the [[Thrift (protocol)|Thrift]] API to generate a client in the language of the users' choosing (C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, [[Cocoa (API)|Cocoa]], Smalltalk, and [[OCaml]]), the [[command-line interface]], browsed through the HDFS-UI [[Web application]] (webapp) over [[HTTP]], or via 3rd-party network client libraries.&lt;ref&gt;{{cite web |url=https://wiki.apache.org/hadoop/MountableHDFS |title=Mounting HDFS |accessdate=May 2014 }}&lt;/ref&gt;

====Other file systems====
Hadoop works directly with any [[distributed file system]] that can be mounted by the underlying operating system simply by using a &lt;code&gt;file://&lt;/code&gt; URL; however, this comes at a price: the loss of locality. To reduce network traffic, Hadoop needs to know which servers are closest to the data; this is information that Hadoop-specific file system bridges can provide.

In May 2011, the list of supported file systems bundled with Apache Hadoop were:

* HDFS: Hadoop's own rack-aware file system.&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Rack_Awareness |title=HDFS Users Guide&amp;nbsp;– Rack Awareness |publisher=Hadoop.apache.org |accessdate=2013-10-17 }}&lt;/ref&gt; This is designed to scale to tens of petabytes of storage and runs on top of the file systems of the underlying [[operating system]]s.
* [[FTP]] File system: this stores all its data on remotely accessible FTP servers.
* [[Amazon Simple Storage Service|Amazon S3 (Simple Storage Service)]] file system. This is targeted at clusters hosted on the [[Amazon Elastic Compute Cloud]] server-on-demand infrastructure. There is no rack-awareness in this file system, as it is all remote.
* [http://azure.microsoft.com/en-us/documentation/articles/hdinsight-use-blob-storage Windows Azure Storage Blobs (WASB)] file system. WASB, an extension on top of HDFS, allows distributions of Hadoop to access data in Azure blob stores without moving the data permanently into the cluster.

A number of third-party file system bridges have also been written, none of which are currently in Hadoop distributions. However, some commercial distributions of Hadoop ship with an alternative filesystem as the default—specifically IBM and MapR.

* In 2009, [[IBM]] discussed running Hadoop over the [[IBM General Parallel File System]].&lt;ref&gt;{{cite web |url=http://www.usenix.org/events/hotcloud09/tech/full_papers/ananthanarayanan.pdf |title= Cloud analytics: Do we really need to reinvent the storage stack? |date=June 2009 |publisher=IBM }}&lt;/ref&gt; The source code was published in October 2009.&lt;ref&gt;{{cite web |url=https://issues.apache.org/jira/browse/HADOOP-6330 |title=HADOOP-6330: Integrating IBM General Parallel File System implementation of Hadoop Filesystem interface |date=2009-10-23 |publisher=IBM }}&lt;/ref&gt;
* In April 2010, [[Parascale]] published the source code to run Hadoop against the Parascale file system.&lt;ref&gt;{{cite web |url=https://issues.apache.org/jira/browse/HADOOP-6704 |title=HADOOP-6704: add support for Parascale filesystem |date=2010-04-14 |publisher=Parascale }}&lt;/ref&gt;
* In April 2010, [[Appistry]] released a Hadoop file system driver for use with its own CloudIQ Storage product.&lt;ref&gt;{{cite web |url=http://resources.appistry.com/news-and-events/press/06072010-appistry-cloudiq-storage-now-generally-available |title=HDFS with CloudIQ Storage |date=2010-07-06 |publisher=Appistry,Inc. }}&lt;/ref&gt;
* In June 2010, [[Hewlett-Packard|HP]] discussed a location-aware [[IBRIX Fusion]] file system driver.&lt;ref&gt;{{cite web |url=http://www.slideshare.net/steve_l/high-availability-hadoop |title=High Availability Hadoop |date=2010-06-09 |publisher=HP }}&lt;/ref&gt;
* In May 2011, [[MapR]] Technologies, Inc. announced the availability of an alternative file system for Hadoop, which replaced the HDFS file system with a full random-access read/write file system.

===JobTracker and TaskTracker: the MapReduce engine===
{{main|MapReduce}}

Above the file systems comes the [[MapReduce]] Engine, which consists of one ''JobTracker'', to which client applications submit MapReduce jobs. The JobTracker pushes work out to available ''TaskTracker'' nodes in the cluster, striving to keep the work as close to the data as possible. With a rack-aware file system, the JobTracker knows which node contains the data, and which other machines are nearby. If the work cannot be hosted on the actual node where the data resides, priority is given to nodes in the same rack. This reduces network traffic on the main backbone network. If a TaskTracker fails or times out, that part of the job is rescheduled. The TaskTracker on each node spawns a separate [[Java Virtual Machine]] process to prevent the TaskTracker itself from failing if the running job crashes its JVM. A heartbeat is sent from the TaskTracker to the JobTracker every few minutes to check its status. The Job Tracker and TaskTracker status and information is exposed by [[Jetty (web server)|Jetty]] and can be viewed from a web browser.

Known limitations of this approach are:
* The allocation of work to TaskTrackers is very simple. Every TaskTracker has a number of available ''slots'' (such as &quot;4 slots&quot;). Every active map or reduce task takes up one slot. The Job Tracker allocates work to the tracker nearest to the data with an available slot. There is no consideration of the current [[load (computing)|system load]] of the allocated machine, and hence its actual availability.
* If one TaskTracker is very slow, it can delay the entire MapReduce job—especially towards the end of a job, where everything can end up waiting for the slowest task. With speculative execution enabled, however, a single task can be executed on multiple slave nodes.

====Scheduling====
By default Hadoop uses [[FIFO (computing and electronics)|FIFO]] scheduling, and optionally 5 scheduling priorities to schedule jobs from a work queue.&lt;ref&gt;[http://hadoop.apache.org/common/docs/current/commands_manual.html#job job] {{wayback|url=http://hadoop.apache.org/common/docs/current/commands_manual.html#job |date=20110817053520 }}&lt;/ref&gt; In version 0.19 the job scheduler was refactored out of the JobTracker, while adding the ability to use an alternate scheduler (such as the ''Fair scheduler'' or the ''Capacity scheduler'', described next).&lt;ref&gt;{{cite web |title=Refactor the scheduler out of the JobTracker |url=https://issues.apache.org/jira/browse/HADOOP-3412 |work=Hadoop Common |publisher=Apache Software Foundation |accessdate=9 June 2012 }}&lt;/ref&gt;

=====Fair scheduler=====
The fair scheduler was developed by [[Facebook]].&lt;ref&gt;{{cite web |url=http://www.ibm.com/developerworks/library/os-hadoop-scheduling/ |title=Scheduling in Hadoop |first=M. Tim |last=Jones |date=6 December 2011 |website=ibm.com |publisher=[[IBM]] |accessdate=20 November 2013 }}&lt;/ref&gt; The goal of the fair scheduler is to provide fast response times for small jobs and [[Quality of service|QoS]] for production jobs. The fair scheduler has three basic concepts.&lt;ref&gt;[https://svn.apache.org/repos/asf/hadoop/common/branches/MAPREDUCE-233/src/contrib/fairscheduler/designdoc/fair_scheduler_design_doc.pdf Hadoop Fair Scheduler Design Document]&lt;/ref&gt;
# Jobs are grouped into [[Pool (computer science)|pools]].
# Each pool is assigned a guaranteed minimum share.
# Excess capacity is split between jobs.

By default, jobs that are uncategorized go into a default pool. Pools have to specify the minimum number of map slots, reduce slots, and a limit on the number of running jobs.

=====Capacity scheduler=====
The capacity scheduler was developed by [[Yahoo]]. The capacity scheduler supports several features that are similar to the fair scheduler.&lt;ref&gt;{{cite web|url=http://hadoop.apache.org/docs/stable1/capacity_scheduler.html|title=CapacityScheduler Guide|publisher=|accessdate=31 December 2015}}&lt;/ref&gt;

* Queues are allocated a fraction of the total resource capacity.
* Free resources are allocated to queues beyond their total capacity.
* Within a queue a job with a high level of priority has access to the queue's resources.

There is no [[preemption (computing)|preemption]] once a job is running.

===Other applications===
The HDFS file system is not restricted to MapReduce jobs. It can be used for other applications, many of which are under development at Apache. The list includes the [[HBase]] database, the [[Apache Mahout]] [[machine learning]] system, and the [[Apache Hive]] [[Data Warehouse]] system. Hadoop can in theory be used for any sort of work that is batch-oriented rather than real-time, is very data-intensive, and benefits from parallel processing of data. It can also be used to complement a real-time system, such as [[lambda architecture]].

{{As of|2009|10}}, commercial applications of Hadoop&lt;ref&gt;{{cite web |date=10 October 2009 |url=http://www.dbms2.com/2009/10/10/enterprises-using-hadoo/ |title=&quot;How 30+ enterprises are using Hadoop&quot;, in DBMS2 |publisher=Dbms2.com |accessdate=2013-10-17 }}&lt;/ref&gt; included:
* Log and/or clickstream analysis of various kinds
* Marketing analytics
* Machine learning and/or sophisticated data mining
* Image processing
* Processing of XML messages
* Web crawling and/or text processing
* General archiving, including of relational/tabular data, e.g. for compliance

==Prominent users==

On February 19, 2008, [[Yahoo! Inc.]] launched what it claimed was the world's largest Hadoop production application. The Yahoo! Search Webmap is a Hadoop application that runs on a Linux cluster with more than 10,000 [[Multi-core|core]]s and produced data that was used in every Yahoo! web search query.&lt;ref&gt;{{cite web|url=https://developer.yahoo.com/blogs/hadoop/yahoo-launches-world-largest-hadoop-production-application-398.html|title=Yahoo! Launches World’s Largest Hadoop Production Application|date=19 February 2008|work=Yahoo|accessdate=31 December 2015}}&lt;/ref&gt;  There are multiple Hadoop clusters at Yahoo! and no HDFS file systems or MapReduce jobs are split across multiple datacenters. Every Hadoop cluster node bootstraps the Linux image, including the Hadoop distribution. Work that the clusters perform is known to include the index calculations for the Yahoo! search engine. In June 2009, Yahoo! made the source code of the Hadoop version it runs available to the public via the open-source community.&lt;ref&gt;{{cite web |url=http://developer.yahoo.com/hadoop/ |title=Hadoop and Distributed Computing at Yahoo! |publisher=Yahoo! |date=2011-04-20 |accessdate=2013-10-17 }}&lt;/ref&gt;

In 2010, [[Facebook]] claimed that they had the largest Hadoop cluster in the world with 21 [[Petabyte|PB]] of storage.&lt;ref&gt;{{cite web |url=http://hadoopblog.blogspot.com/2010/05/facebook-has-worlds-largest-hadoop.html |title=HDFS: Facebook has the world's largest Hadoop cluster! |publisher=Hadoopblog.blogspot.com |date=2010-05-09 |accessdate=2012-05-23 }}&lt;/ref&gt; In June 2012, they announced the data had grown to 100 PB&lt;ref&gt;{{cite web |url=http://www.facebook.com/notes/facebook-engineering/under-the-hood-hadoop-distributed-filesystem-reliability-with-namenode-and-avata/10150888759153920 |title=Under the Hood: Hadoop Distributed File system reliability with Namenode and Avatarnode |publisher=Facebook |accessdate=2012-09-13 }}&lt;/ref&gt; and later that year they announced that the data was growing by roughly half a PB per day.&lt;ref&gt;{{cite web |url=https://www.facebook.com/notes/facebook-engineering/under-the-hood-scheduling-mapreduce-jobs-more-efficiently-with-corona/10151142560538920 |title=Under the Hood: Scheduling MapReduce jobs more efficiently with Corona |publisher=Facebook |accessdate=2012-11-09 }}&lt;/ref&gt;

{{As of|2013}}, Hadoop adoption had become widespread: more than half of the Fortune 50 used Hadoop.&lt;ref&gt;{{cite press release &lt;!-- |author=Staff writer(s); no by-line.--&gt; |title=Altior's AltraSTAR – Hadoop Storage Accelerator and Optimizer Now Certified on CDH4 (Cloudera's Distribution Including Apache Hadoop Version 4) |url=http://www.prnewswire.com/news-releases/altiors-altrastar---hadoop-storage-accelerator-and-optimizer-now-certified-on-cdh4-clouderas-distribution-including-apache-hadoop-version-4-183906141.html |location=Eatontown, NJ |publisher=Altior Inc. |date=2012-12-18 |accessdate=2013-10-30 }}&lt;/ref&gt;

==Hadoop hosting in the Cloud==
Hadoop can be deployed in a traditional onsite datacenter as well as in the cloud.&lt;ref&gt;{{cite web |title=What is Hadoop? |url=http://azure.microsoft.com/en-us/solutions/hadoop/ }}&lt;/ref&gt; The cloud allows organizations to deploy Hadoop without hardware to acquire or specific setup expertise.&lt;ref&gt;{{cite web |url=http://azure.microsoft.com/en-us/solutions/hadoop/ |title=Hadoop |publisher=Azure.microsoft.com |accessdate=2014-07-22 }}&lt;/ref&gt; Vendors who currently have an offer for the cloud include [[Microsoft]], [[Amazon.com|Amazon]], [[IBM]],&lt;ref&gt;{{cite web |url=http://www-03.ibm.com/software/products/en/ibm-biginsights-on-cloud |title=ibm-biginsights-on-cloud }}&lt;/ref&gt; [[Google]] and [[Oracle Corporation|Oracle]].&lt;ref&gt;{{cite web |url=http://searchoracle.techtarget.com/feature/Oracles-cloud-analytics-platform-comprises-several-tools|title=Oracle's cloud analytics platform comprises several tools|accessdate=8 April 2016}}&lt;/ref&gt;

===On Microsoft Azure===
Azure [[HDInsight]]&lt;ref name=&quot;azure.microsoft.com&quot;&gt;{{cite web |url=http://azure.microsoft.com/en-us/services/hdinsight/ |title=HDInsight &amp;#124; Cloud Hadoop |publisher=Azure.microsoft.com |accessdate=2014-07-22 }}&lt;/ref&gt; is a service that deploys Hadoop on [[Microsoft Azure]]. HDInsight uses [[Hortonworks]] HDP and was jointly developed for HDI with [[Hortonworks]]. HDI allows programming extensions with .NET (in addition to Java). HDInsight also supports creation of Hadoop clusters using Linux with Ubuntu.&lt;ref name=&quot;azure.microsoft.com&quot; /&gt; By deploying HDInsight in the cloud, organizations can spin up the number of nodes they want and only get charged for the compute and storage that is used.&lt;ref name=&quot;azure.microsoft.com&quot; /&gt; [[Hortonworks]] implementations can also move data from the on-premises datacenter to the cloud for backup, development/test, and bursting scenarios.&lt;ref name=&quot;azure.microsoft.com&quot; /&gt;
It is also possible to run Cloudera or Hortonworks Hadoop clusters on Azure Virtual Machines.

===On Amazon EC2/S3 services===
It is possible to run Hadoop on [[Amazon Elastic Compute Cloud]] (EC2) and [[Amazon Simple Storage Service]] (S3).&lt;ref&gt;{{cite web |last=Varia |first=Jinesh (@jinman) |title=Taking Massive Distributed Computing to the Common Man – Hadoop on Amazon EC2/S3 |url=http://aws.typepad.com/aws/2008/02/taking-massive.html |work=Amazon Web Services Blog |publisher=Amazon.com |accessdate=9 June 2012 }}&lt;/ref&gt; As an example, [[The New York Times]] used 100 Amazon EC2 instances and a Hadoop application to process 4&amp;nbsp;TB of raw image [[TIFF]] data (stored in S3) into 11 million finished [[PDF]]s in the space of 24 hours at a computation cost of about $240 (not including bandwidth).&lt;ref&gt;{{cite news |url=http://open.blogs.nytimes.com/2007/11/01/self-service-prorated-super-computing-fun/?scp=1&amp;sq=self%20service%20prorated&amp;st=cse |work=The New York Times |title=Self-service, Prorated Super Computing Fun! |first=Derek |last=Gottfrid |date=1 November 2007 |accessdate=4 May 2010}}&lt;/ref&gt;

There is support for the S3 object store in the Apache Hadoop releases, though this is below what one expects from a traditional POSIX filesystem. Specifically, operations such as rename() and delete() on directories are not atomic, and can take time proportional to the number of entries and the amount of data in them.

===Amazon Elastic MapReduce===
Elastic MapReduce (EMR)&lt;ref&gt;{{cite web |url=http://aws.amazon.com/elasticmapreduce/ |title=AWS &amp;#124; Amazon Elastic MapReduce (EMR) &amp;#124; Hadoop MapReduce in the Cloud |publisher=Aws.amazon.com |accessdate=2014-07-22 }}&lt;/ref&gt; was introduced by [[Amazon.com]] in April 2009. Provisioning of the Hadoop cluster, running and terminating jobs, and handling data transfer between EC2(VM) and S3(Object Storage) are automated by Elastic MapReduce. [[Apache Hive]], which is built on top of Hadoop for providing data warehouse services, is also offered in Elastic MapReduce.&lt;ref&gt;{{cite web |url=http://s3.amazonaws.com/awsdocs/ElasticMapReduce/latest/emr-dg.pdf |title=Amazon Elastic MapReduce Developer Guide |format=PDF |accessdate=2013-10-17 }}&lt;/ref&gt;

Support for using Spot Instances&lt;ref&gt;{{cite web |url=http://aws.amazon.com/ec2/spot-instances/ |title=Amazon EC2 Spot Instances |publisher=Aws.amazon.com |accessdate=2014-07-22 }}&lt;/ref&gt; was later added in August 2011.&lt;ref&gt;{{cite web |url=http://aws.amazon.com/about-aws/whats-new/2011/08/18/amazon-elastic-mapreduce-now-supports-spot-instances/ |title=Amazon Elastic MapReduce Now Supports Spot Instances |publisher=Amazon.com |date=2011-08-18 |accessdate=2013-10-17 }}&lt;/ref&gt; Elastic MapReduce is fault-tolerant for slave failures,&lt;ref&gt;{{cite web |url=http://aws.amazon.com/elasticmapreduce/faqs/#cluster-10 |title=Amazon Elastic MapReduce FAQs |publisher=Amazon.com |accessdate=2013-10-17 }}&lt;/ref&gt; and it is recommended to only run the Task Instance Group on spot instances to take advantage of the lower cost while maintaining availability.&lt;ref&gt;{{Youtube |id=66rfnFA0jpM |title=Using Spot Instances with EMR }}&lt;/ref&gt;

===On CenturyLink Cloud (CLC)===
CenturyLink Cloud &lt;ref&gt;{{cite web|url=http://ctl.io|title=Cloud Computing Services and Managed Services - CenturyLink Cloud|publisher=}}&lt;/ref&gt; offers Hadoop via both a managed and un-managed model via their Hadoop&lt;ref&gt;{{cite web|url=https://www.ctl.io/managed-services/cloudera/|title=Managed Cloudera|publisher=}}&lt;/ref&gt; offering. CLC also offers customers several managed Cloudera Blueprints, the newest managed service in the CenturyLink Cloud big data Blueprints portfolio, which also includes Cassandra and MongoDB solutions.&lt;ref&gt;{{cite web|url=https://www.ctl.io/blog/post/hadoop-simplified-managed-cloudera-centurylink-cloud/|title=Hadoop Simplified: Managed Cloudera &amp;amp;amp; CenturyLink Cloud - CenturyLink Cloud|publisher=}}&lt;/ref&gt;

===Google Cloud Platform===
There are multiple ways to run the Hadoop ecosystem on [[Google Cloud Platform]] ranging from self-managed to Google-managed.&lt;ref&gt;{{cite web|title=Hadoop on Google Cloud Platform|url=https://cloud.google.com/hadoop/}}&lt;/ref&gt;

* [[Google Cloud Dataproc]] — A managed Spark and Hadoop service&lt;ref&gt;{{cite web|title=Google Cloud Dataproc Official Website|url=https://cloud.google.com/dataproc/}}&lt;/ref&gt;
* [https://cloud.google.com/hadoop/bdutil Command line tools (bdutil)] — A collection of shell scripts to manually create and manage Spark and Hadoop clusters&lt;ref&gt;{{cite web|title=Hadoop on Google Cloud Platform - Command-Line Deployment|url=https://cloud.google.com/hadoop/setting-up-a-hadoop-cluster}}&lt;/ref&gt;
* Third party Hadoop distributions:
** Cloudera — Using the Cloudera Director Plugin for Google Cloud Platform&lt;ref&gt;{{cite web|title=Cloudera now Certified on Google Cloud Platform|url=https://vision.cloudera.com/cloudera-now-certified-on-google-cloud-platform/}}&lt;/ref&gt;
** Hortonworks — Using bdutil support for Hortonworks HDP&lt;ref&gt;{{cite web|title=HDP on Google Cloud Platform|url=http://hortonworks.com/blog/hdp-google-cloud-platform/}}&lt;/ref&gt;
** MapR — Using bdutil support for MapR&lt;ref&gt;{{cite web|title=MapR Google Cloud Platform|url=https://www.mapr.com/resources/mapr-google-cloud-platform}}&lt;/ref&gt;

Google also offers connectors for using other Google Cloud Platform products with Hadoop, such as a [https://www.mapr.com/resources/mapr-google-cloud-platform Google Cloud Storage connector] for using [[Google Storage|Google Cloud Storage]] and a [https://cloud.google.com/hadoop/bigquery-connector Google BigQuery connector] for using [[BigQuery|Google BigQuery]].

==Commercial support==
&lt;!--
Please don't go overboard in marketing here, as it will only be edited out. Use external citations rather than press releases, and be aware of Wikipedia's rules regarding conflict of interest and external links, WP:COI and WP:EL specifically
--&gt;
A number of companies offer commercial implementations or support for Hadoop.&lt;ref&gt;{{cite web |url=http://gigaom.com/cloud/why-we-need-more-hadoop-innovation/ |title=Why the Pace of Hadoop Innovation Has to Pick Up |publisher=Gigaom.com |date=2011-04-25 |accessdate=2013-10-17 }}&lt;/ref&gt;

===ASF's view on the use of &quot;Hadoop&quot; in product names===
The Apache Software Foundation has stated that only software officially released by the Apache Hadoop Project can be called ''Apache Hadoop'' or ''Distributions of Apache Hadoop''.&lt;ref&gt;{{cite web |url=http://wiki.apache.org/hadoop/Defining%20Hadoop |title=Defining Hadoop |publisher=Wiki.apache.org |date=2013-03-30 |accessdate=2013-10-17 }}&lt;/ref&gt; The naming of products and derivative works from other vendors and the term &quot;compatible&quot; are somewhat controversial within the Hadoop developer community.&lt;ref&gt;{{cite web |url=http://mail-archives.apache.org/mod_mbox/hadoop-general/201105.mbox/%3C4DC91392.2010308@apache.org%3E |title=Defining Hadoop Compatibility: revisited |publisher=Mail-archives.apache.org |date=2011-05-10 |accessdate=2013-10-17 }}&lt;/ref&gt;

==Papers==
Some papers influenced the birth and growth of Hadoop and big data processing. Here is a partial list:
* Jeffrey Dean, Sanjay Ghemawat (2004) [https://www.usenix.org/legacy/publications/library/proceedings/osdi04/tech/full_papers/dean/dean_html/index.html MapReduce: Simplified Data Processing on Large Clusters], Google. This paper inspired Doug Cutting to develop an open-source implementation of the Map-Reduce framework. He named it Hadoop, after his son's toy elephant.
* Michael Franklin, Alon Halevy, David Maier (2005) [http://www.eecs.berkeley.edu/~franklin/Papers/dataspaceSR.pdf From Databases to Dataspaces: A New Abstraction for Information Management]. The authors highlight the need for storage systems to accept all data formats and to provide APIs for data access that evolve based on the storage system’s understanding of the data.
* Fay Chang et al.&lt;!-- Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, Robert E. Gruber --&gt; (2006) [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/bigtable-osdi06.pdf Bigtable: A Distributed Storage System for Structured Data], Google.
* Robert Kallman et al.&lt;!-- Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alexander Rasin, Stanley Zdonik, Evan P. C. Jones, Samuel Madden, Michael Stonebraker, Yang Zhang, John Hugg, Daniel J. Abadi --&gt; (2008) [http://www.vldb.org/pvldb/1/1454211.pdf H-store: a high-performance, distributed main memory transaction processing system]

==See also==
{{Portal|Free software}}
* [[Apache Accumulo]] – Secure [[BigTable]]&lt;ref&gt;{{cite web |url=https://accumulo.apache.org/1.4/user_manual/Security.html |title=Apache Accumulo User Manual: Security &lt;!-- |author=Staff writer(s); no by-line.--&gt; |website=apache.org |publisher=Apache Software Foundation |accessdate=2014-12-03 }}&lt;/ref&gt;
* [[Apache Cassandra]] – A column-oriented database that supports access from Hadoop
* [[Apache CouchDB]] is a database that uses JSON for documents, JavaScript for MapReduce queries, and regular HTTP for an API
* [[Big data]]
* [[Cloud computing]]
* [[Data Intensive Computing]]
* [[HPCC]] – [[LexisNexis]] Risk Solutions High Performance Computing Cluster
* [[Hypertable]] – HBase alternative
* [[Sector/Sphere]] – Open source distributed storage and processing
* [[Simple Linux Utility for Resource Management]]

==References==
{{Reflist|2}}

==Bibliography==
{{Refbegin}}
*{{Cite book
| first1    = Chuck
| last1     = Lam
| date      = July 28, 2010
| title     = Hadoop in Action
| edition   = 1st
| publisher = [[Manning Publications]]
| page     = 325
| isbn      = 1-935-18219-6
| url       =
}}
*{{Cite book
| first1    = Jason
| last1     = Venner
| date      = June 22, 2009
| title     = Pro Hadoop
| edition   = 1st
| publisher = [[Apress]]
| page     = 440
| isbn      = 1-430-21942-4
| url       = http://www.apress.com/book/view/1430219424
}}
*{{Cite book
| first1    = Tom
| last1     = White
| date      = June 16, 2009
| title     = Hadoop: The Definitive Guide
| edition   = 1st
| publisher = [[O'Reilly Media]]
| page     = 524
| isbn      = 0-596-52197-9
| url       = http://oreilly.com/catalog/9780596521974
}}
{{Refend}}

==External links==
*{{Official website}}
*[http://apiwave.com/java/api/org.apache.hadoop Apache Hadoop popular APIs in GitHub]
*[http://www.stanford.edu/class/ee380/Abstracts/111116.html Introducing Apache Hadoop: The Modern Data Operating System] — lecture given at [[Stanford University]] by Co-Founder and CTO of Cloudera, Amr Awadallah ([http://ee380.stanford.edu/cgi-bin/videologger.php?target=111116-ee380-300.asx video archive]) ([https://www.youtube.com/watch?v=d2xeNpfzsYI YouTube])
* [http://www.se-radio.net/2010/03/episode-157-hadoop-with-philip-zeyliger/ Hadoop with Philip Zeyliger, Software Engineering Radio, IEEE Computer Society, March 8 2010]
{{Apache}}

{{Authority control}}

{{DEFAULTSORT:Hadoop}}
[[Category:Apache Software Foundation]]
[[Category:Big data products]]
[[Category:Cloud infrastructure]]
[[Category:Distributed file systems]]
[[Category:Free software for cloud computing]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free system software]]
[[Category:Hadoop|*]]
[[Category:Software using the Apache license]]</text>
      <sha1>quqo3udk13jdpeoe1qd7mbx5vg72zz4</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Hive</title>
    <ns>0</ns>
    <id>30248516</id>
    <revision>
      <id>723882486</id>
      <parentid>721911835</parentid>
      <timestamp>2016-06-05T21:27:17Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (12020)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8498">{{Infobox Software
| name = Apache Hive
| logo = [[File:Apache Hive logo.svg|150px|Apache Hive]]
| screenshot =
| caption = Apache Hive
| developer = [https://hive.apache.org/people.html Contributors]
| status = Active
| latest release version = 2.0.0&lt;ref&gt;{{cite web|url=http://hive.apache.org/downloads.html|title=Apache Hive Download News}}&lt;/ref&gt;
| latest release date = {{Start date and age|2016|02|15}}
| latest preview version =
| latest preview date =
| operating system = [[Cross-platform]]
| programming language = [[Java (programming language)|Java]]
| genre = [[Database management system]]
| license = [[Apache License]] 2.0
| website = {{URL|https://hive.apache.org}}
}}

'''Apache Hive''' is a [[data warehouse]] infrastructure built on top of [[Hadoop]] for providing data summarization, query, and analysis.&lt;ref&gt;{{cite book |last=Venner |first=Jason |title=Pro Hadoop |publisher=[[Apress]] |year=2009 |isbn=978-1-4302-1942-2}}&lt;/ref&gt; While developed by [[Facebook]], Apache Hive is now used and developed by other companies such as [[Netflix]] and the [[Financial Industry Regulatory Authority]] (FINRA).&lt;ref&gt;[http://www.slideshare.net/evamtse/hive-user-group-presentation-from-netflix-3182010-3483386 Use Case Study of Hive/Hadoop ]&lt;/ref&gt;&lt;ref&gt;{{YouTube|id=Idu9OKnAOis|title=OSCON Data 2011, Adrian Cockcroft, &quot;Data Flow at Netflix&quot;}}&lt;/ref&gt; Amazon maintains a software fork of Apache Hive that is included in [[Apache Hadoop#Amazon Elastic MapReduce|Amazon Elastic MapReduce]] on [[Amazon Web Services]].&lt;ref&gt;[http://s3.amazonaws.com/awsdocs/ElasticMapReduce/latest/emr-dg.pdf Amazon Elastic MapReduce Developer Guide]&lt;/ref&gt;

==Features==
Apache Hive supports analysis of large datasets stored in Hadoop's [[HDFS]] and compatible file systems such as [[Amazon S3]] filesystem. It provides an [[SQL]]-like language called HiveQL&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/LanguageManual HiveQL Language Manual]&lt;/ref&gt; with schema on read and transparently converts queries to [[MapReduce]], Apache Tez&lt;ref&gt;[http://tez.apache.org/ Apache Tez]&lt;/ref&gt; and [[Apache Spark|Spark]] jobs. All three execution engines can run in [[Hadoop]] YARN. To accelerate queries, it provides indexes, including [[bitmap index]]es.&lt;ref&gt;[http://www.facebook.com/notes/facebook-engineering/working-with-students-to-improve-indexing-in-apache-hive/10150168427733920 Working with Students to Improve Indexing in Apache Hive]&lt;/ref&gt;
Other features of Hive include:
* Indexing to provide acceleration, index type including compaction and [[Bitmap index]] as of 0.10, more index types are planned.
* Different storage types such as plain text, [[RCFile]], [[HBase]], ORC, and others.
* Metadata storage in an [[Relational database management system|RDBMS]], significantly reducing the time to perform semantic checks during query execution.
* Operating on compressed data stored into the Hadoop ecosystem using algorithms including [[DEFLATE]], [[BWT]], [[snappy (software)|snappy]], etc.
* Built-in user defined functions (UDFs) to manipulate dates, strings, and other data-mining tools. Hive supports extending the UDF set to handle use-cases not supported by built-in functions.
* SQL-like queries (HiveQL), which are implicitly converted into MapReduce or Tez, or Spark jobs.
By default, Hive stores metadata in an embedded [[Apache Derby]] database, and other client/server databases like [[MySQL]] can optionally be used.&lt;ref&gt;{{cite book |last=Lam |first=Chuck |title=Hadoop in Action |publisher=[[Manning Publications]] |year=2010 |isbn=1-935182-19-6}}&lt;/ref&gt;

Four file formats are supported in Hive, which are TEXTFILE,&lt;ref&gt;[http://www.semantikoz.com/blog/optimising-hadoop-big-data-text-hive/ Optimising Hadoop and Big Data with Text and HiveOptimising Hadoop and Big Data with Text and Hive]&lt;/ref&gt; SEQUENCEFILE, ORC&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC LanguageManual ORC]&lt;/ref&gt; and [[RCFile|RCFILE]].&lt;ref&gt;[http://www.semantikoz.com/blog/faster-big-data-hadoop-hive-rcfile/ Faster Big Data on Hadoop with Hive and RCFile]&lt;/ref&gt;&lt;ref&gt;[http://www.sfbayacm.org/wp/wp-content/uploads/2010/01/sig_2010_v21.pdf Facebook's Petabyte Scale Data Warehouse using Hive and Hadoop]&lt;/ref&gt;&lt;ref&gt;{{cite web| url=http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-4.pdf| title=RCFile: A Fast and Space-efﬁcient Data Placement Structure in MapReduce-based Warehouse Systems|author1=Yongqiang He |author2=Rubao Lee |author3=Yin Huai |author4=Zheng Shao |author5=Namit Jain |author6=Xiaodong Zhang |author7=Zhiwei Xu |format=PDF}}&lt;/ref&gt; [[Apache Parquet]] can be read via plugin in versions later than 0.10 and natively starting at 0.13.&lt;ref&gt;{{cite web|title=Parquet|url=https://cwiki.apache.org/confluence/display/Hive/Parquet|accessdate=2 February 2015|archiveurl=http://web.archive.org/web/20150202145641/https://cwiki.apache.org/confluence/display/Hive/Parquet|archivedate=2 February 2015|date=18 Dec 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Massie|first1=Matt|title=A Powerful Big Data Trio: Spark, Parquet and Avro|url=http://zenfractal.com/2013/08/21/a-powerful-big-data-trio/|website=zenfractal.com|accessdate=2 February 2015|archiveurl=http://web.archive.org/web/20150202145026/http://zenfractal.com/2013/08/21/a-powerful-big-data-trio/|archivedate=2 February 2015|date=21 August 2013}}&lt;/ref&gt; Additional Hive plugins support querying of the [[Bitcoin]] [[Blockchain]].&lt;ref&gt;{{cite web|last1=Franke|first1=Jörn|title=Hive &amp; Bitcoin: Analytics on Blockchain data with SQL|url=https://snippetessay.wordpress.com/2016/04/28/hive-bitcoin-analytics-on-blockchain-data-with-sql/}}&lt;/ref&gt;

==HiveQL==
While based on SQL, HiveQL does not strictly follow the full [[SQL-92]] standard. HiveQL offers extensions not in SQL, including ''multitable inserts'' and ''create table as select'', but only offers basic support for [[index (database)|indexes]]. Also, HiveQL lacks support for [[database transaction|transactions]] and [[materialized view]]s, and only limited subquery support.&lt;ref&gt;{{cite book |last=White |first=Tom |title=Hadoop: The Definitive Guide |publisher=[[O'Reilly Media]] |year=2010 |isbn=978-1-4493-8973-4}}&lt;/ref&gt;&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/LanguageManual Hive Language Manual]&lt;/ref&gt; Support for insert, update, and delete with full ACID functionality was made available with release 0.14.&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions ACID and Transactions in Hive]&lt;/ref&gt;

Internally, a [[compiler]] translates HiveQL statements into a [[directed acyclic graph]] of [[MapReduce]] or Tez, or Spark jobs, which are submitted to Hadoop for execution.&lt;ref&gt;[http://www.vldb.org/pvldb/2/vldb09-938.pdf Hive A Warehousing Solution Over a MapReduce Framework]&lt;/ref&gt;

==Hive unit testing frameworks==
* [http://finraos.github.io/HiveQLUnit/ HiveQLUnit]
* [https://github.com/klarna/HiveRunner/ HiveRunner]
* [https://github.com/edwardcapriolo/hive_test hive_test]

==See also==
* [[Pig (programming tool)|Apache Pig]]
* [[Sqoop]]
* [[Cloudera Impala]]
* [[Apache Drill]]
* [[Apache Flume]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website}}
* [http://www.semantikoz.com/blog/the-free-apache-hive-book/ The Free Hive Book] (CC by-nc licensed)
* [http://www.vldb.org/pvldb/2/vldb09-938.pdf Hive A Warehousing Solution Over a MapReduce Framework] - Original paper presented by Facebook at [[VLDB]] 2009
* [http://www.youtube.com/watch?v=Y3UXDtDR9bg Using Apache Hive With Amazon Elastic MapReduce (Part 1)] and {{Youtube|id=1hDhpVmeSGI|title=Part 2}}, presented by an AWS Engineer
* [https://github.com/2013Commons/hive-cassandra Using hive + cassandra + shark. A hive cassandra cql storage handler.]
* [http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-14-2.pdf Major Technical Advancements in Apache Hive], Yin Huai, Ashutosh Chauhan, Alan Gates, Gunther Hagleitner, Eric N. Hanson, Owen O’Malley, Jitendra Pandey, Yuan Yuan, Rubao Lee and Xiaodong Zhang, [[SIGMOD]] 2014
* [https://cwiki.apache.org/confluence/display/Hive/Home Apache Hive Wiki]

{{Apache}}
{{Facebook navbox}}

{{DEFAULTSORT:Apache Hive}}
[[Category:2015 software]]
[[Category:Apache Software Foundation|Hive]]
[[Category:Apache Software Foundation projects|Hive]]
[[Category:Cloud computing]]
[[Category:Facebook software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]</text>
      <sha1>cdnimml98jajwbbym8gyxwmhzv0o7aa</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Giraph</title>
    <ns>0</ns>
    <id>37752641</id>
    <revision>
      <id>706280995</id>
      <parentid>671521765</parentid>
      <timestamp>2016-02-22T13:12:25Z</timestamp>
      <contributor>
        <username>演歌ビニール</username>
        <id>23956730</id>
      </contributor>
      <minor/>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2364">{{ Infobox Software
| name                   = Apache Giraph
| logo                   = 
| screenshot             = 
| caption                = 
| collapsible            = yes
| developer              = [[Apache Software Foundation]] 
| status                 = Active
| latest release version = 1.1.0
| latest release date    = {{release date|2014|11|19|df=yes}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| size                   = 
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Graph (computer science)|graph processing]]
| license                = [[Apache License|Apache 2.0 Licence]] 
| website                = {{url|http://giraph.apache.org}}
}}
'''Apache Giraph''' is an [[Apache Software Foundation|Apache]] project to perform [[Graph (computer science)|graph processing]] on big data. Giraph utilizes [[Apache Hadoop]]'s MapReduce implementation to process graphs. [[Facebook]] used Giraph with some performance improvements to analyze one trillion edges using 200 machines in 4 minutes.&lt;ref&gt;{{cite web|last=Ching|first=Avery|title=Scaling Apache Giraph to a trillion edges|url=http://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920|publisher=Facebook|accessdate=8 February 2014|date=August 14, 2013}}&lt;/ref&gt; Giraph is based on a paper published by Google about its own graph processing system called Pregel.&lt;ref&gt;{{cite news|last=Jackson|first=Joab|title=Facebook's Graph Search puts Apache Giraph on the map|url=http://www.pcworld.com/article/2046680/facebooks-graph-search-puts-apache-giraph-on-the-map.html|accessdate=8 February 2014|newspaper=[[PC World]]|date=Aug 14, 2013}}&lt;/ref&gt; It can be compared to other Big Graph processing libraries such as Cassovary.&lt;ref&gt;{{cite web|last=Harris|first=Derrick|title=Facebook’s trillion-edge, Hadoop-based and open source graph-processing engine|url=http://gigaom.com/2013/08/14/facebooks-trillion-edge-hadoop-based-graph-processing-engine/|publisher=[[Gigaom]]|accessdate=8 February 2014|date=Aug 14, 2013}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
* {{Official website|http://giraph.apache.org/}}

{{Apache}}

[[Category:Apache Software Foundation|Giraph]]
[[Category:Hadoop]]
[[Category:Data mining and machine learning software]]</text>
      <sha1>b4u9z3qorkty18t6x2au2b5gdp0cqm8</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Mahout</title>
    <ns>0</ns>
    <id>18706674</id>
    <revision>
      <id>720968353</id>
      <parentid>714790361</parentid>
      <timestamp>2016-05-19T00:31:32Z</timestamp>
      <contributor>
        <ip>216.40.74.84</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4846">{{Infobox Software
| name                   = Apache Mahout
| logo                   = 
| screenshot             = 
| caption                = 
| collapsible            = yes
| developer              = [[Apache Software Foundation]] 
| status                 = Active
| latest release version = 0.12.1
| latest release date    = {{release date|2016|05|18|df=yes}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| size                   = 
| programming language   = [[Java (programming language)|Java]], [[Scala (programming language)|Scala]]
| genre                  = [[Machine Learning]]
| license                = [[Apache License|Apache 2.0 Licence]] 
| website                = {{url|http://mahout.apache.org}}
}}

'''Apache Mahout''' is a project of the [[Apache Software Foundation]] to produce [[free software|free]] implementations of [[distributed computing|distributed]] or otherwise [[Scalability|scalable]] [[machine learning]] algorithms focused primarily in the areas of [[collaborative filtering]], clustering and classification. Many of the implementations use the [[Apache Hadoop]] platform.&lt;ref&gt;{{cite web |url= http://www.ibm.com/developerworks/java/library/j-mahout/ |title=Introducing Apache Mahout |work=ibm.com |year=2011  |accessdate=13 September 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.infoq.com/news/2009/04/mahout |title=InfoQ: Apache Mahout: Highly Scalable Machine Learning Algorithms |work=infoq.com |year=2011 |accessdate=13 September 2011}}&lt;/ref&gt; Mahout also provides Java libraries for common maths operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; the number of implemented algorithms has grown quickly,&lt;ref&gt;{{cite web |url= http://mahout.apache.org/users/basics/algorithms.html |title=Algorithms - Apache Mahout - Apache Software Foundation|work=cwiki.apache.org |year=2011 |accessdate=13 September 2011}}&lt;/ref&gt; but various algorithms are still missing.

While Mahout's core algorithms for [[Cluster analysis|clustering]], classification and batch based collaborative filtering are implemented on top of Apache Hadoop using the [[MapReduce|map/reduce]] paradigm, it does not restrict contributions to Hadoop-based implementations. Contributions that run on a single node or on a non-Hadoop cluster are also welcomed. For example, the 'Taste' collaborative-filtering recommender component of Mahout was originally a separate project and can run stand-alone without Hadoop.

Starting with the release 0.10.0, the project shifts its focus to building backend-independent programming environment, code named &quot;Samsara&quot;.&lt;ref&gt;{{Cite web
| url = http://mahout.apache.org/users/environment/in-core-reference.html
| title = Mahout-Samsara's In-Core Linear Algebra DSL Reference
| last = 
| first = 
| date = 
| website = 
| publisher = 
| access-date = 
}}&lt;/ref&gt;&lt;ref&gt;{{Cite web
| url = http://mahout.apache.org/users/environment/out-of-core-reference.html
| title = Mahout-Samsara's Distributed Linear Algebra DSL Reference
| last = 
| first = 
| date = 
| website = 
| publisher = 
| access-date = 
}}&lt;/ref&gt;&lt;ref&gt;{{Cite web
| url = http://www.weatheringthroughtechdays.com/2015/04/mahout-010x-first-mahout-release-as.html
| title = Mahout 0.10.x: first Mahout release as a programming environment
| website = www.weatheringthroughtechdays.com
| access-date = 2016-02-29
}}&lt;/ref&gt; The environment consists of an algebraic backend-independent optimizer and an algebraic Scala DSL unifying in-memory and distributed algebraic operators. At the time of this writing supported algebraic platforms are [[Apache Spark]] and [[H2O_(software)|H20]], and [[Apache Flink]]. Support for [[MapReduce]] algorithms is being gradually phased out.&lt;ref&gt;{{Cite web
| url = https://issues.apache.org/jira/browse/MAHOUT-1510
| title = MAHOUT-1510 (&quot;Good-bye MapReduce&quot;)
| last = 
| first = 
| date = 
| website = 
| publisher = 
| access-date = 
}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*{{Official website}}
*[http://incubator.apache.org/giraph/ Giraph] - a Graph processing infrastructure that runs on Hadoop (see Pregel).
*[http://portal.acm.org/citation.cfm?id=1582723 Pregel] - Google's internal graph processing platform, released details in ACM paper.
*[http://qnalist.com/g/mahout Mahout Mailing List Archives] - Mahout Mailing List Archives

===Resources===
*[http://bhagyas.github.io/spring-mahout-demo/ A Spring based Java demo application that demonstrates a simple recommender using Apache Mahout]
*[http://test-triprecommender.rhcloud.com Demo of travel recommendations using anonymous user-based recommender of Mahout]

{{Apache}}

[[Category:Apache Software Foundation|Mahout]]
[[Category:Hadoop]]
[[Category:Data mining and machine learning software]]
[[Category:Big data products]]</text>
      <sha1>9bq0a6a834u6ndh7ydmivrke4973q92</sha1>
    </revision>
  </page>
  <page>
    <title>Apache ZooKeeper</title>
    <ns>0</ns>
    <id>26039352</id>
    <revision>
      <id>714715827</id>
      <parentid>708075582</parentid>
      <timestamp>2016-04-11T12:17:11Z</timestamp>
      <contributor>
        <ip>168.9.196.115</ip>
      </contributor>
      <comment>Typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3012">{{Infobox software
| name                   =
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 3.4.8
| latest release date    = {{release date|2016|02|20}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| platform               =
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Distributed computing]]
| license                = [[Apache License]] 2.0
| website                = {{url|http://zookeeper.apache.org}}
}}

'''Apache ZooKeeper''' is a software project of the [[Apache Software Foundation]], providing an [[open source]] distributed configuration service, synchronization service, and naming registry for large [[distributed systems]].{{Clarify|date=March 2012}} ZooKeeper was a sub-project of [[Hadoop]] but is now a [[Apache_Software_Foundation#Projects|top-level]] project in its own right.

ZooKeeper's architecture supports [[High-availability cluster|high availability]] through redundant services. The clients can thus ask another ZooKeeper leader if the first fails to answer. ZooKeeper nodes store their data in a hierarchical name space, much like a file system or a [[Tree (data structure)|tree]] data structure. Clients can read from and write to the nodes and in this way have a shared configuration service. Updates are [[total order|totally ordered]].&lt;ref&gt;{{cite web |url=https://cwiki.apache.org/confluence/display/ZOOKEEPER/ProjectDescription|title=Zookeeper Overview}}&lt;/ref&gt;

ZooKeeper is used by companies including [[Rackspace]], [[Yahoo!]],&lt;ref&gt;{{cite web |url=http://wiki.apache.org/hadoop/ZooKeeper/PoweredBy |title=ZooKeeper/Powered By}}&lt;/ref&gt; [[Odnoklassniki]] and [[eBay]]  as well as [[open source]] [[enterprise search]] systems like [[Solr]].&lt;ref&gt;{{cite web |url=https://cwiki.apache.org/confluence/display/solr/SolrCloud|title=SolrCloud}}&lt;/ref&gt;

==Typical use cases==
* [[Name service|Naming service]]
* [[Configuration management]]
* [[Synchronization]]
* [[Leader election]]
* [[Message Queue]]
* [[Notification system]]

==See also==
{{Portal|Java}}
* [[Hadoop]]

==References==
{{Reflist}}

==External links==
*{{Official website|https://zookeeper.apache.org}}
*[http://highscalability.com/blog/2008/7/15/zookeeper-a-reliable-scalable-distributed-coordination-syste.html Article in highscalability.com]
*[http://www.sdtimes.com/ZOOKEEPER_SERVICES_COORDINATOR_MOVES_TO_APACHE/About_SERVICECOORDINATION_and_ZOOKEEPER_and_APACHE_and_YAHOO/33011 Software Development Times article of ZooKeeper moving to Apache]
*[http://wiki.eclipse.org/index.php?title=Zookeeper_Based_ECF_Discovery  Eclipse ECF Discovery based on Apache ZooKeeper]

{{Apache}}

{{DEFAULTSORT:Apache Zookeeper}}
[[Category:Apache Software Foundation|ZooKeeper]]
[[Category:Configuration management]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Hadoop]]</text>
      <sha1>iemhnf4rldk0ae7fpzbjrw9h3ki1x38</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Spark</title>
    <ns>0</ns>
    <id>42164234</id>
    <revision>
      <id>724009769</id>
      <parentid>722833458</parentid>
      <timestamp>2016-06-06T16:07:22Z</timestamp>
      <contributor>
        <username>Glamracket</username>
        <id>10284738</id>
      </contributor>
      <minor/>
      <comment>Normalised formatting.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17303">{{Infobox Software
| name                   = Apache Spark
| logo                   = [[File:Spark-logo-192x100px.png|frameless|Spark Logo]]
| caption                =
|author = [[Matei Zaharia]]
| developer              = [[Apache Software Foundation]], [[UC Berkeley]] AMPLab, [[Databricks]]
| status                 = Active
| released               = {{Start date and age|2014|05|30}}
| latest release version = v1.6.1
| latest release date    = {{Start date and age|2016|03|09}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Microsoft Windows]], [[OS X]], [[Linux]]
| size                   = 
| programming language   = [[Scala (programming language)|Scala]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[R (programming language)|R]]
| genre                  = Data analytics, [[machine learning]] algorithms
| license                = [[Apache License]] 2.0 
| website                = {{URL|https://spark.apache.org}}
}}
'''Apache Spark''' is an [[Open-source software|open source]] [[distributed computing|cluster computing]] [[Software framework|framework]]. Originally developed at the [[UC Berkeley|University of California, Berkeley]]'s [[AMPLab]], the Spark [[codebase]] was later donated to the [[Apache Software Foundation]] that has maintained it since. Spark provides an [[application programming interface|interface]] for programming entire clusters with implicit [[data parallelism]] and [[fault-tolerant computing|fault-tolerance]].

==Overview==
Apache Spark provides programmers with an [[application programming interface]] centered on a [[data structure]] called the resilient distributed dataset (RDD), a read-only [[multiset]] of data items distributed over a cluster of machines, that is maintained in a [[fault-tolerant computing|fault-tolerant]] way.&lt;ref name=&quot;hc10&quot;&gt;{{cite conference |first1=Matei |last1=Zaharia |first2=Mosharaf |last2=Chowdhury |first3=Michael J. |last3=Franklin |first4=Scott |last4=Shenker |first5=Ion |last5=Stoica |title=Spark: Cluster Computing with Working Sets |conference=USENIX Workshop on Hot Topics in Cloud Computing (HotCloud) |url=https://amplab.cs.berkeley.edu/wp-content/uploads/2011/06/Spark-Cluster-Computing-with-Working-Sets.pdf}}&lt;/ref&gt;
It was developed in response to limitations in the [[MapReduce]] cluster computing [[Programming paradigm|paradigm]], which forces a particular linear [[dataflow]] structure on distributed programs: MapReduce programs read input data from disk, [[Map (parallel pattern)|map]] a function across the data, [[Fold (higher-order function)|reduce]] the results of the map, and store reduction results on disk. Spark's RDDs function as a [[working set]] for distributed programs that offers a (deliberately) restricted form of distributed [[shared memory]].&lt;ref&gt;{{cite conference |first1=Matei |last1=Zaharia |first2=Mosharaf |last2=Chowdhury |first3=Tathagata |last3=Das |first4=Ankur |last4=Dave |first5=Justin |last5=Ma, |first6=Murphy |last6=McCauley |first7=Michael |last7=J. |first8=Scott |last8=Shenker |first9=Ion |last9=Stoica |title=Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing |url=http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf |conference=USENIX Symp. Networked Systems Design and Implementation}}&lt;/ref&gt;

The availability of RDDs facilitates the implementation of both [[iterative algorithm]]s, that visit their dataset multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated [[database]]-style querying of data. The [[latency (engineering)|latency]] of such applications (compared to [[Apache Hadoop]], a popular MapReduce implementation) may be reduced by several orders of magnitude.{{r|hc10}}&lt;ref&gt;{{cite paper|first1=Reynold| last1=Xin| first2=Josh |last2=Rosen| first3=Matei| last3=Zaharia| first4=Michael| last4=Franklin| first5=Scott| last5=Shenker| first6=Ion| last6=Stoica|title=Shark: SQL and Rich Analytics at Scale| conference=SIGMOD 2013|date=June 2013| url=https://amplab.cs.berkeley.edu/wp-content/uploads/2013/02/shark_sigmod2013.pdf}}&lt;/ref&gt;
Among the class of iterative algorithms are the training algorithms for [[machine learning]] systems, which formed the initial impetus for developing Apache Spark.&lt;ref&gt;{{cite web |title=4 reasons why Spark could jolt Hadoop into hyperdrive |first=Derrick |last=Harris |date=28 June 2014 |url=https://gigaom.com/2014/06/28/4-reasons-why-spark-could-jolt-hadoop-into-hyperdrive/ |website=[[Gigaom]]}}&lt;/ref&gt;

Apache Spark requires a [[cluster manager]] and a [[Clustered file system|distributed storage system]]. For cluster management, Spark supports standalone (native Spark cluster), [[Apache Hadoop|Hadoop YARN]], or [[Apache Mesos]].&lt;ref&gt;{{cite web |url=https://spark.apache.org/docs/1.2.0/cluster-overview.html#cluster-manager-types |title=Cluster Mode Overview - Spark 1.2.0 Documentation - Cluster Manager Types |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=2014-12-18 |website=apache.org |publisher=Apache Foundation |accessdate=2015-01-18}}&lt;/ref&gt; For distributed storage, Spark can interface with a wide variety, including [[Apache Hadoop#Hadoop distributed file system|Hadoop Distributed File System (HDFS)]],&lt;ref&gt;[https://amplab.cs.berkeley.edu/software/ Figure showing Spark in relation to other open-source Software projects including Hadoop]&lt;/ref&gt; [[MapR#MapR converged data platform|MapR File System (MapR-FS)]],&lt;ref&gt;[http://doc.mapr.com/display/MapR/Ecosystem+Support+Matrix MapR ecosystem support matrix]&lt;/ref&gt; [[Apache Cassandra|Cassandra]],&lt;ref&gt;{{cite mailing list |url=http://mail-archives.apache.org/mod_mbox/cassandra-user/201409.mbox/%3CCABNXB2DE5Apmvn1nNg79+VdPCSZiCsGdt=ZB4s4OF_5JzS60iA@mail.gmail.com%3E |title=Re: cassandra + spark / pyspark |date=2014-09-10 |accessdate=2014-11-21 |mailinglist=Cassandra User |last=Doan |first=DuyHai }}&lt;/ref&gt; [[OpenStack#Object Storage (Swift)|OpenStack Swift]], [[Amazon S3]], [http://getkudu.io Kudu], or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per [[CPU core]].

===Spark Core===
Spark Core is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic [[I/O interface|I/O]] functionalities, exposed through an application programming interface (for [[Java]], [[Python (programming language)|Python]], [[Scala (programming language)|Scala]], and [[R (programming language)|R]]) centered on the RDD [[Abstraction (computer science)|abstraction]]. This interface mirrors a [[functional programming|functional]]/[[higher-order programming|higher-order]] model of programming: a &quot;driver&quot; program invokes parallel operations such as map, [[Filter (computer science)|filter]] or reduce on an RDD by passing a function to Spark, which then schedules the function's execution in parallel on the cluster.{{r|hc10}} These operations, and additional ones such as [[Join (database)|joins]], take RDDs as input and produce new RDDs. RDDs are [[Immutable object|immutable]] and their operations are [[lazy evaluation|lazy]]; fault-tolerance is achieved by keeping track of the &quot;lineage&quot; of each RDD, the sequence of operations produced it, so that it can be reconstructed in the case of data loss. RDDs can contain any type of Python, Java, or Scala objects.

Aside from the RDD-oriented functional style of programming, Spark provides two restricted forms of shared variables: ''broadcast variables'' reference read-only data that needs to be available on all nodes, while ''accumulators'' can be used to program reductions in an [[imperative programming|imperative]] style.{{r|hc10}}

A typical example of RDD-centric functional programming is the following Scala program that computes the frequencies of all words occurring in a set of text files and prints the most common ones. Each {{mono|map}}, {{mono|flatMap}} (a variant of {{mono|map}}) and {{mono|reduceByKey}} takes an [[anonymous function]] that performs a simple operation on a single data item (or a pair of items), and applies its argument to transform an RDD into a new RDD.

&lt;source lang=&quot;scala&quot;&gt;
val conf = new SparkConf().setAppName(&quot;wiki_test&quot;)          // create a spark config object
val sc = new SparkContext(conf)                             // Create a spark context
val data = sc.textFile(&quot;/path/to/somedir&quot;)                  // Read files from &quot;somedir&quot; into an RDD of (filename, content) pairs.
val tokens = data.flatMap(_.split(&quot; &quot;))                     // Split each file into a list of tokens (words).
val wordFreq = tokens.map((_, 1)).reduceByKey(_ + _)        // Add a count of one to each token, then sum the counts per word type.
wordFreq.sortBy(s =&gt; -s._2).map(x =&gt; (x._2, x._1)).top(10)  // Get the top 10 words. Swap word and count to sort by count.
&lt;/source&gt;

===Spark SQL===
Spark [[SQL]] is a component on top of Spark Core that introduces a new data abstraction called DataFrames,{{efn|Called SchemaRDDs before Spark 1.3.{{Citation needed|date=September 2015}}}} which provides support for structured and [[semi-structured data]]. Spark SQL provides a [[domain-specific language]] to manipulate DataFrames in [[Scala (programming language)|Scala]], [[Java (programming language)|Java]], or [[Python (programming language)|Python]]. It also provides SQL language support, with [[command-line interface]]s and [[Open Database Connectivity|ODBC]]/[[Java Database Connectivity|JDBC]] server.

&lt;source lang=&quot;scala&quot;&gt;
import org.apache.spark.sql.SQLContext

val url = &quot;jdbc:mysql://yourIP:yourPort/test?user=yourUsername;password=yourPassword&quot;   // URL for your database server.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)              // Create a sql context object

val df = sqlContext
    .read
    .format(&quot;jdbc&quot;)
    .option(&quot;url&quot;, url)
    .option(&quot;dbtable&quot;, &quot;people&quot;)
    .load()

df.printSchema() // Looks the schema of this DataFrame.
val countsByAge = df.groupBy(&quot;age&quot;).count() // Counts people by age
&lt;/source&gt;

===Spark Streaming===
Spark Streaming leverages Spark Core's fast scheduling capability to perform [[Event stream processing|streaming analytics]]. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data. This design enables the same set of application code written for batch analytics to be used in streaming analytics, on a single engine.{{Citation needed|date=September 2015}}

===MLlib Machine Learning Library===
Spark MLlib is a [[Distributed computing|distributed]] machine learning framework on top of Spark Core that, due in large part of the distributed memory-based Spark architecture, is as much as nine times as fast as the disk-based implementation used by [[Apache Mahout]] (according to benchmarks done by the MLlib developers against the [[Linear regression|Alternating Least Squares]] (ALS) implementations, and before Mahout itself gained a Spark interface), and [[Scale (computing)|scales]] better than [[Vowpal Wabbit]].&lt;ref&gt;{{cite web |url=http://www.slideshare.net/chaochen5496/mlllib-sparkmeetup8613finalreduced/68 |title=Spark Meetup: MLbase, Distributed Machine Learning with Spark |last1=Sparks |first1=Evan |last2=Talwalkar |first2=Ameet |date=2013-08-06 |website=slideshare.net |publisher=Spark User Meetup, San Francisco, California |accessdate=10 February 2014}}&lt;/ref&gt; Many common machine learning and statistical algorithms have been implemented and are shipped with MLlib which simplifies large scale machine learning [[Pipeline (software)|pipelines]], including:
* [[summary statistics]], [[Correlation and dependence|correlations]], [[stratified sampling]], [[hypothesis testing]], random data generation&lt;ref&gt;{{Cite web|title = MLlib {{!}} Apache Spark|url = http://spark.apache.org/mllib/|website = spark.apache.org|access-date = 2016-01-18}}&lt;/ref&gt;
* [[Statistical classification|classification]] and [[Regression analysis|regression]]: [[support vector machines]], [[logistic regression]], [[linear regression]], decision trees, [[Naive Bayes classifier|naive Bayes classification]]
* [[collaborative filtering]] techniques including alternating least squares (ALS)
* [[Cluster analysis|cluster analysis methods]] including [[K-means clustering|k-means]], and [[Latent Dirichlet allocation|Latent Dirichlet Allocation]] (LDA)
* [[dimensionality reduction|dimensionality reduction techniques]] such as [[singular value decomposition]] (SVD), and [[principal component analysis]] (PCA)
* [[feature extraction]] and [[Data transformation (statistics)|transformation]] functions
* [[optimization (mathematics)|optimization]] algorithms such as [[stochastic gradient descent]], [[limited-memory BFGS]] (L-BFGS)

===GraphX===
GraphX is a distributed [[Graph (abstract data type)|graph processing]] framework on top of Apache Spark. It provides an API for expressing graph computation that can model the [[Google Pregel|Pregel]] abstraction. It also provides an optimized [[Run time (program lifecycle phase)|runtime]] for this abstraction.

Like Apache Spark, GraphX initially started as a research project at UC Berkeley's AMPLab and Databricks, and was later donated to the Apache Software Foundation and the Spark project.&lt;ref&gt;{{cite paper|first1=Joseph| last1=Gonzalez| first2=Reynold |last2=Xin| first3=Ankur| last3=Dave| first4=Daniel| last4=Crankshaw| first5=Michael| last5=Franklin| first6=Ion| last6=Stoica|title=GraphX: Graph Processing in a Distributed Dataflow Framework| conference=OSDI 2014|date=Oct 2014| url=https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-gonzalez.pdf}}&lt;/ref&gt;

==History==
Spark was initially started by [[Matei Zaharia]] at UC Berkeley's AMPLab in 2009, and open sourced in 2010 under a [[BSD licenses|BSD license]].

In 2013, the project was donated to the Apache Software Foundation and switched its license to [[Apache License|Apache 2.0]]. In February 2014, Spark became a [[Apache Software Foundation#Projects|Top-Level Apache Project]].&lt;ref&gt;{{cite web |url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50 |title=The Apache Software Foundation Announces Apache&amp;#8482 Spark&amp;#8482 as a Top-Level Project |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=27 February 2014 |website=apache.org |publisher=Apache Software Foundation |accessdate=4 March 2014}}&lt;/ref&gt;

In November 2014, Spark founder M. Zaharia's company [[Databricks]] set a new world record in large scale sorting using Spark.&lt;ref&gt;[http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html Spark officially sets a new record in large-scale sorting]&lt;/ref&gt;{{third-party inline|date=February 2016}}

Spark had in excess of 1000 contributors in 2015,&lt;ref&gt;[https://www.openhub.net/p/apache-spark Open HUB Spark development activity]&lt;/ref&gt; making it one of the most active projects in the Apache Software Foundation&lt;ref&gt;{{cite web |url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50 |title=The Apache Software Foundation Announces Apache&amp;#8482 Spark&amp;#8482 as a Top-Level Project |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=27 February 2014 |website=apache.org |publisher=Apache Software Foundation |accessdate=4 March 2014}}&lt;/ref&gt; and one of the most active open source [[big data]] projects.&lt;ref name=sparkbigdataproject&gt;[https://intellipaat.com/tutorial/spark-tutorial/ Introduction to Apache Spark]&lt;/ref&gt;

{| class=&quot;wikitable&quot;
|-
! Version
! Original release date
! Latest version
! Release date
|-
| {{Version|o|0.5}}
| 2012-06-12
| 0.5.1
| 2012-10-07
|-
| {{Version|o|0.6}}
| 2012-10-14
| 0.6.1
| 2012-11-16
|-
| {{Version|o|0.7}}
| 2013-02-27
| 0.7.3
| 2013-07-16
|-
| {{Version|o|0.8}}
| 2013-09-25
| 0.8.1
| 2013-12-19
|-
| {{Version|o|0.9}}
| 2014-02-02
| 0.9.2
| 2014-07-23
|-
| {{Version|o|1.0}}
| 2014-05-30
| 1.0.2
| 2014-08-05
|-
| {{Version|o|1.1}}
| 2014-09-11
| 1.1.1
| 2014-11-26
|-
| {{Version|o|1.2}}
| 2014-12-18
| 1.2.2
| 2015-04-17
|-
| {{Version|c|1.3}}
| 2015-03-13
| 1.3.1
| 2015-04-17
|-
| {{Version|c|1.4}}
| 2015-06-11
| 1.4.1
| 2015-07-15
|-
| {{Version|c|1.5}}
| 2015-09-09
| 1.5.2
| 2015-11-09
|-
| {{Version|c|1.6}}
| 2016-01-04
| 1.6.1
| 2016-03-09
|-
| {{Version|p|2.0}}
| 2016
| 2.0.0
| 2016
|-
| colspan=&quot;5&quot; | &lt;small&gt;{{Version |l |show=111110}}&lt;/small&gt;
|}
&lt;!-- o=Old-Not-Supported; c=Released; p=Planned-Future --&gt;

==Notes==
{{notelist}}

==References==
{{Reflist|30em}}

==External links==
*{{Official website}}
*[https://spark.apache.org/sql/ Spark SQL]
*[https://spark.apache.org/streaming/ Spark Streaming]
*[https://spark.apache.org/mllib/ MLlib machine learning library]
*[https://spark.apache.org/graphx/ GraphX graph processing library]

{{Apache}}
{{DEFAULTSORT:Spark}}

[[Category:Apache Software Foundation|Spark]]
[[Category:Big data products]]
[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Free software programmed in Scala]]
[[Category:Hadoop]]
[[Category:Java platform]]
[[Category:Software using the Apache license]]
[[Category:University of California, Berkeley]]</text>
      <sha1>6lv8qy8an73c6eae69y4ysh3gz0nqnd</sha1>
    </revision>
  </page>
  <page>
    <title>ND4J (software)</title>
    <ns>0</ns>
    <id>49594059</id>
    <revision>
      <id>717889546</id>
      <parentid>707468552</parentid>
      <timestamp>2016-04-30T09:48:50Z</timestamp>
      <contributor>
        <ip>2600:1010:B052:35C4:F887:B9A6:E01B:3359</ip>
      </contributor>
      <comment>Deleted implausible benchmark results</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3509">{{Infobox software
| name                   = ND4J: N-Dimensional Arrays for Java
| logo                   = 
| screenshot             = 
| caption                =
| collapsible            =
| author                 = Adam Gibson
| developer              = [https://github.com/deeplearning4j/nd4j various]
| released               =
| latest release version = 0.4-rc3.8 
| latest release date    = {{Start date and age|2014|09|15|df=yes}}
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
| programming language   = [[Java|Scala (programming language)]]
| operating system       = [[Linux]], [[OSX]], [[Windows]], [[Android (operating system)|Android]]
| platform               = [[Cross-platform]]
| size                   =
| language               = 
| status                 = active
| genre                  = [[Natural language processing]], [[Deep learning]], [[Machine vision]]
| license                = [[Apache License|Apache 2.0]]
| website                = {{URL|https://github.com/deeplearning4j/nd4j}}
}}

'''ND4J''' is a free, [[Open-source software|open-source]] scientific computing library operating on the [[JVM|Java Virtual Machine]] and compatible with [[Java (programming language)|Java]], [[Scala (programming language)|Scala]] and [[Clojure (programming language)|Clojure]].&lt;ref&gt;{{cite web|title=Official Website|url=http://nd4j.org/}}&lt;/ref&gt; &lt;ref&gt;{{cite web|title=The Deeplearning4j Framework|url=http://gtts.ehu.es/WDW/biblio/JavaMagazine/javamagazine20150506-dl.pdf}}&lt;/ref&gt; 

ND4J is a scientific computing library for linear algebra and matrix manipulation in a production environment, integrating with Hadoop and Spark to work with distributed GPUs. It supports n-dimensional arrays for JVM-based languages.

ND4J has primarily been developed by the group in San Francisco that built [[Deeplearning4j]], led by Adam Gibson.&lt;ref&gt;{{cite web|title=Github Repository|url=https://github.com/deeplearning4j/nd4s}}&lt;/ref&gt; It was created under an [[Apache Software Foundation]] license.

==Distributed==

ND4J's operations include [[Distributed computing|distributed]] [[Parallel computing|parallel]] versions. They can take place in a cluster and process massive amounts of data. Matrix manipulation occurs in parallel on GPUs or CPUs in the cloud, and can work within Spark or Hadoop clusters.

==See also==
{{Portal|Free software|Java}}
* [[NumPy]]
* [[SciPy]]

==References==
{{Reflist}}

==External links==
* {{official website|https://github.com/deeplearning4j/nd4j}}

[[Category:Array programming languages]]
[[Category:Numerical programming languages]]
[[Category:Data mining and machine learning software]]
[[Category:Free statistical software]]
[[Category:Java platform]]
[[Category:Java programming language family]]
[[Category:JVM programming languages]]
[[Category:Scala (programming language)]]
[[Category:Linear algebra]]
[[Category:Numerical analysis]]
[[Category:Computational statistics]]
[[Category:Artificial neural networks]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free science software]]
[[Category:Numerical analysis software for Linux]]
[[Category:Numerical analysis software for OS X]]
[[Category:Numerical analysis software for Windows]]
[[Category:Free mathematics software]]
[[Category:Java (programming language) libraries]]
[[Category:Numerical software]]
[[Category:Cluster computing]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]

{{compu-stub}}</text>
      <sha1>a96gx1u1tbgygif0487vqhdxu2uyauj</sha1>
    </revision>
  </page>
  <page>
    <title>Google Cloud Dataproc</title>
    <ns>0</ns>
    <id>50339609</id>
    <revision>
      <id>721018924</id>
      <parentid>721018812</parentid>
      <timestamp>2016-05-19T09:14:44Z</timestamp>
      <contributor>
        <username>Alvin Seville</username>
        <id>8629244</id>
      </contributor>
      <comment>adding a category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4678">{{Infobox software
| name                   = Google Cloud Dataproc
| title                  = Google Cloud Dataproc
| logo                   =[[File:Dataproc-128px-Retina.png|100px|Cloud Dataproc Logo 128px]]
| logo caption           = Google Cloud Dataproc
| screenshot             = &lt;!-- [[File: ]] --&gt;
| caption                = 
| collapsible            = 
| author                 = [[Google Cloud Platform]]
| developer              = [[Google Cloud Platform]]
| released               = {{Start date and age|2016|02|22}} 
| discontinued           = 
| latest release version = 1.0
| latest release date    =  {{Start date and age|2016|03|30}}
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| programming language   = 
| operating system       = {{Plainlist |
* [[Debian]]
* [[Apache Hadoop]]
* [[Apache Spark]]
* [[Apache Hive]]
* [[Apache Pig]]
}}
| platform               = 
| size                   = 
| language               = English 
| status                 = Active 
| genre                  = [[Apache Hadoop#Hadoop hosting in the Cloud|Managed Spark and Hadoop]]
| license                = 
| alexa                  = 
| website                = {{URL|https://cloud.google.com/dataproc/}}
}}
'''Google Cloud Dataproc''' (Cloud Dataproc) is a cloud-based managed Spark and Hadoop service offered on [https://cloud.google.com Google Cloud Platform]. Cloud Dataproc utilizes many [[Google Cloud Platform]] technologies such as [[Google Compute Engine]] and [[Google Cloud Storage]] to offer fully managed clusters running popular&lt;ref&gt;{{cite web|title=Fortune Spark Survey|url=http://fortune.com/2015/09/25/apache-spark-survey/}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=TechCrunch - On The Growth Of Apache Spark|url=http://techcrunch.com/2015/03/19/on-the-growth-of-apache-spark/}}&lt;/ref&gt; data processing frameworks such as [[Apache Hadoop]] and [[Apache Spark]].&lt;ref&gt;{{cite web|title=Official Google Cloud Dataproc page|url=https://cloud.google.com/dataproc/}}&lt;/ref&gt;

== Design ==
Cloud Dataproc is a [[platform as a service|Platform as a service (PaaS)]] product designed to combine the Spark and Hadoop frameworks with many common [[Cloud computing#Characteristics|cloud computing patterns]]. Cloud Dataproc separate compute and storage, which is a relatively common design for many cloud Hadoop offerings. Cloud Dataproc utilizes Google Compute Engine virtual machines for compute and Google Cloud Storage for file storage. Cloud Dataproc has a set of control and integration mechanisms that coordinate the lifecycle, management, and coordination of clusters. Cloud Dataproc is integrated with the YARN application manager to make managing and using clusters easier.&lt;ref&gt;{{cite web|title=Cloud Dataproc FAQ|url=https://cloud.google.com/dataproc/faq#how_does_cloud_dataproc_work}}&lt;/ref&gt;

Cloud Dataproc includes many open source packages used for data processing, including items from the Spark and Hadoop ecosystem, and open source tools to connect these frameworks with other Google Cloud Platform products.&lt;ref&gt;{{cite web|title=Cloud Dataproc - Versions|url=https://cloud.google.com/dataproc/dataproc-versions}}&lt;/ref&gt;

== History ==
Cloud Dataproc was released as a publicly available beta service on September 23, 2015&lt;ref&gt;{{cite web|title=Cloud Dataproc Beta Blog Post|url=https://cloudplatform.googleblog.com/2015/09/Google-Cloud-Dataproc-Making-Spark-and-Hadoop-Easier-Faster-and-Cheaper.html}}&lt;/ref&gt; and entered public general availability on February 22, 2016.&lt;ref&gt;{{cite web|title=Cloud Dataproc GA blog post|url=https://cloudplatform.googleblog.com/2016/02/Google-Cloud-Dataproc-managed-Spark-and-Hadoop-service-now-GA.html}}&lt;/ref&gt;

== Similar products ==
* [https://aws.amazon.com/elasticmapreduce/ Amazon Elastic MapReduce (EMR)]
* [http://www.databricks.com Databricks]
* [https://azure.microsoft.com/en-us/services/hdinsight/ Microsoft HDInsight]
* [https://www.qubole.com/hadoop-as-a-service/ Qubole]

== See also ==
* [http://spark.apache.org Apache Spark]. 
* [http://hadoop.apache.org Apache Hadoop]
* [http://hive.apache.org Apache Hive]
* [http://pig.apache.org Apache Pig]

== External links ==
* [https://cloud.google.com/dataproc Official website]. 
* [https://cloud.google.com Google Cloud Platform site]
* [http://stackoverflow.com/questions/tagged/google-cloud-dataproc Cloud Dataproc questions on Stack Overflow]
* [http://groups.google.com/forum/#!forum/cloud-dataproc-discuss Cloud Dataproc discussion list]

== References ==
{{Reflist}}

[[Category:Cloud computing providers]]
[[Category:Cloud platforms]]
[[Category:Google software]]
[[Category:Hadoop]]
[[Category:Web services]]</text>
      <sha1>cc32lzjd3xl4n38lod24eqqbls2pls2</sha1>
    </revision>
  </page>
</mediawiki>
